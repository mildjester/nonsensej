

var data = [
  {
    url: "https://blog2.logical-dice.com/tags/aws/",
    title: "AWS",
    date: "2025-03-15T00:00:00Z",
    body: "AWS"
  },
  {
    url: "https://blog2.logical-dice.com/en/",
    title: "Ens",
    date: "2025-03-15T00:00:00Z",
    body: "Ens"
  },
  {
    url: "https://blog2.logical-dice.com/",
    title: "Logical Dice 技術ブログ",
    date: "2025-03-15T00:00:00Z",
    body: "Logical Dice 技術ブログ"
  },
  {
    url: "https://blog2.logical-dice.com/tags/",
    title: "Tags",
    date: "2025-03-15T00:00:00Z",
    body: "Tags"
  },
  {
    url: "https://blog2.logical-dice.com/en/2025/03/15/aws-cli-s3select/",
    title: "Using AWS CLI to Search Multiple Files with S3 Select",
    date: "2025-03-15T00:00:00Z",
    body: "Using AWS CLI to Search Multiple Files with S3 Select Environment aws-cli 2.7.14 Overview I needed to search through CloudFront logs stored in S3. Since Athena was not available, I used S3 Select for the search. However, S3 Select can only search one file at a time. To handle multiple files, I integrated AWS CLI into a shell script. Steps Here are the conditions: The S3 bucket name is target-bucket-name. Files are generated in the logs-dir directory with the format AABBCCDDEE123456.YYYY-MM-DD-HH.abcdefg\u0026hellip;. The AWS CLI profile is named my-aws-profile. Below is the completed script: bash # Step 1: Configuration PROFILE=\u0026#34;my-aws-profile\u0026#34; BUCKET=\u0026#34;target-bucket-name\u0026#34; PREFIX=\u0026#34;logs-dir/AABBCCDDEE123456.2024-12-11\u0026#34; # Example: Searching files output on Dec 11, 2024. QUERY=\u0026#34;SELECT * FROM s3object s\u0026#34; # Example: Output all records. Adjust with SELECT columns or WHERE clause. # Step 2: Retrieve a list of objects (files) objects=$(aws s3api list-objects --profile $PROFILE --bucket $BUCKET --prefix $PREFIX | jq) # Step 3: Process each retrieved object for object in $(echo \u0026#34;$objects\u0026#34; | jq -c \u0026#39;.Contents[]\u0026#39;); do key=$(echo \u0026#34;$object\u0026#34; | jq -r \u0026#39;.Key\u0026#39;) echo \u0026#34;$key\u0026#34; # Step 4: Output search results aws s3api select-object-content --profile $PROFILE --bucket $BUCKET --key $key --expression \u0026#34;$QUERY\u0026#34; --expression-type SQL --input-serialization \u0026#39;{\u0026#34;CSV\u0026#34;: {\u0026#34;FileHeaderInfo\u0026#34;: \u0026#34;NONE\u0026#34;, \u0026#34;FieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;}, \u0026#34;CompressionType\u0026#34;: \u0026#34;GZIP\u0026#34;}\u0026#39; --output-serialization \u0026#39;{\u0026#34;CSV\u0026#34;: {\u0026#34;FieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;}}\u0026#39; output-tmp.csv cat output-tmp.csv \u0026gt;\u0026gt; output.csv done rm -f output-tmp.csv Step 1: Configuration Set the necessary information. Be sure to configure PREFIX to filter files properly. Searching too many files unnecessarily will increase costs. For pricing details, see: https://aws.amazon.com/s3/pricing/ For information on QUERY, refer to: https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-select-sql-reference-select.html Step 2: Retrieve a List of Objects (Files) Use the AWS CLI to retrieve a list of objects in the specified S3 bucket and prefix. The jq command is used to parse the returned JSON. Note: The CLI output format is set to JSON in the profile configuration. Step 3: Process Each Retrieved Object The list of objects (files) is returned under the Contents key. Use a for loop to iterate through the objects. Step 4: Output Search Results Since the results of S3 Select are written to a file, the search results for the current object are first output to a temporary file (output-tmp.csv) and then appended to the final file (output.csv). This script outputs the search results. You can refine the query further to extract specific information as needed."
  },
  {
    url: "https://blog2.logical-dice.com/posts/2024/12/12/aws-cli-s3select/",
    title: "AWS CLIで複数ファイルに対してS3 Selectで検索する",
    date: "2024-12-12T00:00:00Z",
    body: "AWS CLIで複数ファイルに対してS3 Selectで検索する 環境 aws-cli 2.7.14 概要 S3に保存されているCloudFrontのログを検索したかったのですが Athenaは導入されていなかったのでS3 Selectを使って検索をしました。 S3 Selectは1ファイルずつしか検索できないので、 AWS CLIをシェルに組み込んで複数ファイル検索できるようにしました。 手順 条件は以下とします。 S3バケット名は「target-bucket-name」 「logs-dir」というディレクトリ内に「AABBCCDDEE123456.YYYY-MM-DD-HH.abcdefg・・・」形式でファイルが生成される。 aws cli用のprofileは「my-aws-profile」という名前で設定してある。 できあがったものはこちら。 # ①設定 PROFILE=\u0026#34;my-aws-profile\u0026#34; BUCKET=\u0026#34;target-bucket-name\u0026#34; PREFIX=\u0026#34;logs-dir/AABBCCDDEE123456.2024-12-11\u0026#34; # 左記例では2024年12月11日に出力された分を検索する。 QUERY=\u0026#34;SELECT * FROM s3object s\u0026#34; # 左記例では全て出力しています。列を絞ったりWHERE句を入れたりしてください。 # ②オブジェクト（ファイル）の一覧取得 objects=`aws s3api list-objects --profile $PROFILE --bucket $BUCKET --prefix $PREFIX | jq` # ③取得したオブジェクトの一覧に対して1件ずつ処理をする for object in $(echo \u0026#34;$objects\u0026#34; | jq -c \u0026#39;.Contents[]\u0026#39;); do key=$(echo \u0026#34;$object\u0026#34; | jq -r \u0026#39;.Key\u0026#39;) echo \u0026#34;$key\u0026#34; # ④検索結果出力 aws s3api select-object-content --profile $PROFILE --bucket $BUCKET --key $key --expression \u0026#34;$QUERY\u0026#34; --expression-type SQL --input-serialization \u0026#39;{\u0026#34;CSV\u0026#34;: {\u0026#34;FileHeaderInfo\u0026#34;: \u0026#34;NONE\u0026#34;, \u0026#34;FieldDelimiter\u0026#34;: \u0026#34;\\t\u0026#34;}, \u0026#34;CompressionType\u0026#34;: \u0026#34;GZIP\u0026#34;}\u0026#39; --output-serialization \u0026#39;{\u0026#34;CSV\u0026#34;: {\u0026#34;FieldDelimiter\u0026#34;: \u0026#34;,\u0026#34;}}\u0026#39; output-tmp.csv cat output-tmp.csv \u0026gt;\u0026gt; output.csv done rm -f output-tmp.csv ①設定 必要な情報を設定してください。 PREFIXは必要なファイルを絞れるように設定しないと。検索対象が無駄に増えるとお金がかかってしまいます。 料金については以下を参照。 https://aws.amazon.com/jp/s3/pricing/ QUERYについては以下も参照。 https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/s3-select-sql-reference-select.html ②オブジェクト（ファイル）の一覧取得 AWS CLIを利用してS3のバケットおよびPrefixが一致するオブジェクトの一覧を取得します。 取得したjsonを解読するためjqコマンドも噛ませています。 ※profile設定時にoutputをjsonにしてある。 ③取得したオブジェクトの一覧に対して1件ずつ処理をする オブジェクト（ファイル）の配列は「Contents」というキーで返ってくるので、 それを指定してforで回します。 ④検索結果出力 S3 Selectのコマンド結果はファイルに出力するしかないので、 現在のオブジェクトの検索結果は一時ファイル(output-tmp.csv)に出力してから 最終ファイル(output.csv)に追記していくようにしています。 これで検索結果が出力されます。 あとは必要な情報をさらに絞るためにクエリを修正したりしていけば欲しい情報が取れるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/",
    title: "Posts",
    date: "2024-12-12T00:00:00Z",
    body: "Posts"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2024/08/24/al2023-kernel-downgrade/",
    title: "AmazonLinux2023のカーネルをダウングレードする",
    date: "2024-08-24T00:00:00Z",
    body: "AmazonLinux2023のカーネルをダウングレードする 環境 Amazon Linux 2023.5.20240819 概要 AmazonLinux2023のカーネルをアップグレードしたところ、動作しなくなるツールが出てきてしまったためダウングレードしました。 その操作手順の記録です。 手順 まず、現在のカーネルの状態を確認します。 $uname -r 6.1.102-111.182.amzn2023.x86_64 $ sudo dnf list kernel kernel.x86_64 6.1.92-99.174 amzn2023 @amazonlinux kernel.x86_64 6.1.96-102.177.amzn2023 @amazonlinux kernel.x86_64 6.1.102-111.182.amzn2023 @amazonlinux 現状は6.1.102になっていますが、6.1.96にダウングレードしていきます。 おそらく最短の方法 まず、ダウングレードして再起動します。 「パッケージ名-バージョン」と指定をすることで特定のバージョンに戻れるようです。 $ sudo dnf downgrade kernel-6.1.96-102.177.amzn2023 $ sudo reboot 再起動後、カーネルを確認するとダウングレードできているはずです。 $ uname -r 6.1.96-102.177.amzn2023.x86_64 適用されているバージョンは戻しましたが、最新バージョンのパッケージは残っているので削除しておきます。 こちらも「パッケージ名-バージョン」で特定バージョンのremoveができます。 $ sudo dnf remove kernel-6.1.102-111.182.amzn2023 これでダウングレード完了です。 参考：実際にやった手順 思考錯誤しながらやっていたので結構遠回しをしました。 まず、ダウングレードして再起動します。 $ sudo dnf downgrade kernel $ sudo reboot 再起動後、カーネルを確認してみます。 $ uname -r 6.1.92-99.174.amzn2023.x86_64 戻りすぎてしまいました。 どうやらバージョン指定をしないと戻れるだけ戻ってしまうようです。 なので、狙いのバージョンにアップデートします。 該当バージョンを一度削除しないとバージョンアップできないので、現バージョンより新しいものは一旦削除します。 $ sudo dnf remove kernel-6.1.96-102.177.amzn2023 $ sudo dnf remove kernel-6.1.102-111.182.amzn2023 アップデートして再起動します。 この際、また新しくなり過ぎないようにreleaseverでOSのバージョンを指定します。 ※後で思ったが、kernelのバージョンを指定すれば良いだけだったしれない。 $ sudo dnf list kernel --releasever=2023.5.20240708 kernel.x86_64 6.1.92-99.174 amzn2023 @amazonlinux kernel.x86_64 6.1.96-102.177.amzn2023 @amazonlinux ↑狙いのバージョンが最新になっている事を確認。違ったらreleaseverを変える。 $ sudo dnf update kernel --releasever=2023.5.20240708 $ sudo reboot releaseverで指定するバージョンはAL2023のリリースノートなどを参照してください。 https://docs.aws.amazon.com/ja_jp/linux/al2023/release-notes/relnotes.html 再起動後、カーネルを確認したら無事狙いのバージョンになっていました。 $ uname -r 6.1.96-102.177.amzn2023.x86_64"
  },
  {
    url: "https://blog2.logical-dice.com/tags/python/",
    title: "Python",
    date: "2024-08-07T00:00:00Z",
    body: "Python"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2024/08/07/python-on-secure-windows/",
    title: "Windowsで管理者権限なしでpythonコマンドを使えるようにする",
    date: "2024-08-07T00:00:00Z",
    body: "Windowsで管理者権限なしでpythonコマンドを使えるようにする 環境 Windows 11 git bash Python 3.12.4 背景 仕事で利用しているPCは管理者権限が貰えないのですが、Pythonを使う必要があったため 管理者権限不要でPythonを使える環境を用意しました。 手順 エンベデッド Pythonのダウンロード 以下サイトにてPythonのダウンロードができるので『Windows embeddable package』をダウンロードします。 https://www.python.org/downloads/release/python-3124/ ダウンロードしたZIPファイルを任意のフォルダに解凍し、その中にあるpython312._pthを修正します。 1行コメントアウトを外すだけです。 ■修正前 # import site ■修正後 import site 続いてpythonコマンドにパスを通します。 管理者権限がなくても以下よりPathの設定が可能です。 コントロールパネルを開く ユーザーアカウントを開く さらにユーザーアカウントを開く 左メニューにある環境変数の変更を開く ユーザー環境変数内にあるPathを編集し、今回配置したフォルダとそこのScripts配下を追加する。 追加例: pythonをC:¥toolsフォルダの下に解凍した場合は以下2つを追加する。 C:¥tools¥python-3.12.4-embed-amd64 C:¥tools¥python-3.12.4-embed-amd64¥Scripts 優先度を高めるために「上へ」ボタンで上部に持っていっておくこと これでPCを再起動すればpythonコマンドが利用可能となっています。 pipコマンドのインストール 次にpipコマンドを利用できるようにします。 まず以下のファイルをpython-3.12.4-embed-amd64フォルダにダウンロードします。 https://bootstrap.pypa.io/get-pip.py 次に、以下コマンドを実行します。 trusted-hostオプションを付けないとエラーとなるのでご注意ください。 $ cd 配置したパス/python-3.12.4-embed-amd64 $ python get-pip.py --trusted-host=files.pythonhosted.org --trusted-host=pypi.org カレントディレクトリ配下にScriptsフォルダが作られ、そこにpip.exeが生成されます。 すでに前手順でPathには追加済みなので、Path追加は不要です。 これでpythonおよびpipコマンドが利用できるようになりました。 管理者権限が無いPCという事はセキュアな情報を扱うPCだと思うので、常識の範囲＆自己責任の範囲でご利用ください。 おまけ：aws cliのインストール pipが使えると、実はaws cliもインストールできます。 以下コマンドでpip.exeと同じフォルダにawsコマンドが生成されます。 $ pip install awscli トラブルシューティング ① get-pip.py実行エラー get-pip.pyをそのまま実行すると以下のようなエラーが発生します。 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\u0026#39;))\u0026#39;: /simple/pip/ Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=\u0026#39;pypi.org\u0026#39;, port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\u0026#39;))) - skipping ERROR: Could not find a version that satisfies the requirement pip (from versions: none) ERROR: No matching distribution found for pip これはSSL証明書のエラーが発生しているので、上記手順の通りtrusted-hostオプションを付けて実行しましょう。 ②pipが無いエラー pipコマンド実行時に以下エラーが発生する場合があります。 Traceback (most recent call last): File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 198, in _run_module_as_main File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 88, in _run_code File \u0026#34;C:\\tools\\python-3.12.4-embed-amd64\\Scripts\\pip.exe\\__main__.py\u0026#34;, line 4, in \u0026lt;module\u0026gt; ModuleNotFoundError: No module named \u0026#39;pip\u0026#39; 上記手順の通りpython312._pthにてsiteをimportするようにすれば解決します。 参考 Embeddable packageでPortableなPython環境を構築する | Zenn Embedded Pythonでget-pip.pyに失敗するときの対処方法 | Qiita aws-cli portable for windows? | stack overflow"
  },
  {
    url: "https://blog2.logical-dice.com/tags/scrum/",
    title: "Scrum",
    date: "2024-07-25T00:00:00Z",
    body: "Scrum"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2024/07/25/story-point-knowledge/",
    title: "スクラムのストーリーポイントの「時間で見積もらない」とは",
    date: "2024-07-25T00:00:00Z",
    body: "スクラムのストーリーポイントの「時間で見積もらない」とは 背景 普段スクラム開発でスクラムマスターとして業務しています。 開発者の方々にはストーリーポイントで見積もりをしてもらっています。 ストーリーポイントは「工数」で見積もってはならず、代わりに「難易度」や「労力」で見積もります。 ただ、この表現がフワッとしていてメンバーが困っていました。 メンバーにより解釈に違いがあるし、そもそも工数で見積もらないけどストーリーポイントを元にスプリントで実施できるチケットを判断したりしているし、 結局皆どうしても各チケットの相対的な工数を考えてしまっており、すごい悶々としていました。 そんな時にスクラムコーチの方々のスクラム研修を受ける機会があり、いい機会なので見積もりの考え方について色々ぶつけてみました。 結果として自分の中ではすごいシックリくる解釈ができたので残しておきます。 見積もりの考え方 結果から言ってしまうと「各チケットの相対的な工数」を考えるのは間違いでは無さそうです。 ただし、前提条件があるので、そこを履き違えてしまうと正しい見積もりができません。 前提条件①：自分がやった場合。ではない 見積もりをしていてポイントの根拠を聞いた時によく聞くセリフで以下のようなものがあります。 この対応はCSSでの調整が必要ですが、自分はスタイル調整が苦手なので高めに付けました。 この機能は自分は関わった事がないので、影響箇所の調査から入るので高めに付けました。 上記の考え方はダメです。 客観的にみて「どんな作業があるか」「それぞれの作業は一般的にどの程度重いか」を考えてポイントを入れる必要があります。 チーム内で「もし普段この業務を担当している並レベルの人がやった場合」などの基準を設けましょう。 また、もしタスクに対して必要な作業が分からなければ、ポイントを入れる前に有識者から作業内容をざっくり聞いておきましょう。 それを行なった上でも「作業の重さ」というのはズレが生じるので、そこだけ最後に擦り合わせできれば良いと思います。 他の人とズレてたなと思った場合は擦り合わせた結果を次の見積もりから活かせば良いです。 前提条件②：工数は具体的な数値じゃなくて、どっちの方が時間かかりそうかで考える 相対的に工数でポイントをつけるとは言ったものの、この作業にどのくらいの時間がかかりそうかを考える時に、具体的に「3日かかりそうだから3P相当」みたいな考え方はやってはいけません。 なぜかというと人は成長していくものなので今は3日かかる作業も1年後は1日でできるようになっているかもしれません。 もし具体的な数値で見積もっていると全く同じ作業をでも現在と1年後でポイントが3倍も変わってしまい、同じ作業なのにポイントが違うという矛盾が生じてしまいます。 具体的な数値ではなく相対的な感覚でいけば1年後もポイントが大きくズレることはないと思います。 前提条件③：よく分からなければポイントを入れない これは前提条件というか、見積もる際の注意点かもしれません。 前提条件①でも少し触れましたが、見積もりタスクに対して「この機能は自分は関わった事がない」という事は普通にありえると思います。 ポイント見積もり前にどんな作業があるのかなどは話し合いますが、それでも全然想像を付かなけば見積もりに参加しない勇気も必要です。 WEBのプランニングポーカーツールでも「？」があると思いますが、それです。 作業量が分からないのに無理にポイントをいれてもノイズにしかならないので、空気読んで他の人と合いそうなポイントを入れるって言うのは止めましょう。 ただし、該当機能は今後見積もりをしなくていいやと諦めるのではなく、どんなポイントが付いたのかを見ながら次からは参加できるようにする努力はしましょう。 あとがき いろいろと前提条件をつけましたが、これを一言で伝えるのは難しいと思いました。 その結果出てきた表現が「難易度」とか「労力」なのかなと思います。 また、見積もりはあくまで作業の目安をつけているだけで顧客に価値は提供していません。 より良い見積もりをするために工数をかけすぎて、結果として顧客に提供できる価値にかける工数が減ってしまっては本末転倒です。 チームによって最適な運用というのは違うと思うので、各チームで現実的な見積もり方法というのを見つけていきましょう。 その辺りを推進していくのもスクラムマスターの仕事かと思います。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2024/06/06/aws-codecommit-mfa/",
    title: "【AWS】MFA(多要素)認証を突破してCodeCommitからgit cloneする",
    date: "2024-06-06T00:00:00Z",
    body: "【AWS】MFA(多要素)認証を突破してCodeCommitからgit cloneする 環境 MacBook Pro (Sonoma14.5 M1チップ) aws-cli/2.15.14 aws-mfa 0.0.12 手順 AWSへのアクセスにMFAが必須である環境においてCodeCommitよりリポジトリをcloneする手順です。 調べても「CodeCommitでMFA不要にする手順」ばかり出てきて手間取ったので、メモを残します。 AWS CLIのインストール もしAWS CLIが未インストールである場合は以下ページ参考にインストールします。 https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/getting-started-install.html aws-mfaのインストール コマンドラインでAWS操作する際にMFA認証を楽にするツールがあります。 https://github.com/broamski/aws-mfa こちらをpipでインストールします。 $ pip install aws-mfa AWS認証情報の確認 AWSのマネジメントコンソールにログイン後、ヘッダー右端にあるアカウント名をクリックすると「セキュリテイ認証情報」というメニューが表示されるので選択します。 そこでアカウントの詳細が表示されるので「ユーザーのARN」を控えておきます。 また、もし自分のAWS アクセスキーを発行していない場合はここで発行します。 少し下へスクロールすると「アクセスキーを作成」というボタンがあるのでそこからアクセスキーが発行できます。 AWS CLI向けアクセスキー設定 AWS CLIにprofileを指定してアクセスキーを紐づけます。 『hoge』というプロファイル名で登録したい場合 『hoge』だけではなく『hoge-long-term』というプロファイルも登録します。 まず『hoge-long-term』の登録をします。 $ aws configure --profile hoge-long-term AWS Access Key ID [None]: XXXXXXXXXXXXXXXXXXXX (発行したアクセスキー) AWS Secret Access Key [None]: XXXXXXXXXXXXXXXXXXXX (発行したアクセスキーに紐づくシークレット) Default region name [None]: ap-northeast-1 (利用するリージョン 任意) Default output format [None]: json (利用するフォーマット 任意) 次に『hoge』の登録をします。 $ aws configure --profile hoge AWS Access Key ID [None]: (空のまま) AWS Secret Access Key [None]: (空のまま) Default region name [None]: ap-northeast-1 (利用するリージョン 任意) Default output format [None]: json (利用するフォーマット 任意) MFA認証登録 以下のコマンドでMFA認証を通します。 コマンド実行後にMFA code(数字6桁)を聞かれるので入力します。 $ aws-mfa --profile 『作成したプロファイル』 --device 『事前に確認したユーザーのARN』 # 例：aws-mfa --profile hoge --device arn:aws:iam::000000000000:mfa/hoge-taro これでしばらくMFA認証をしなくてもコマンドを打てるようになりました。 有効期限は~/.aws/credentialsを見ると確認できます。 git clone実行 以下のようにprofileを指定して実行することでcloneができます。 $ git clone codecommit::『リージョン』://『作成したプロファイル』@『cloneしたいリポジトリ』 # 例：git clone codecommit::ap-northeast-1://hoge@my-application-repository 参考 AWS CLIからのMFA(多要素認証)を楽にするツール(aws-mfa)を使ってみた - Qiita"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2023/11/03/aws-lambda-use-paramiko/",
    title: "【AWS】lambdaでparamikoを使ってSFTP通信する",
    date: "2023-11-03T00:00:00Z",
    body: "【AWS】lambdaでparamikoを使ってSFTP通信する 環境 MacBook Air M1チップ AWS SAM version 1.99.0 python 3.9 手順 他サーバーよりSFTPでファイルを取得する処理を作るためにparamikoを利用します。 AWS lambdaでparamikoを利用するためにレイヤーを利用します。 lambdaのレイヤーはDockerなどを使って構築する方法もありますが 簡易に作成したかったのでAWS SAMを利用してレイヤーの作成をしました。 ①AWS SAMをインストールする Macにて以下コマンドを実行してAWS SAMをインストールします。 $ brew tap aws/tap $ brew install aws-sam-cli この記事を書いている時点でインストールできたのはバージョン1.99.0でした。 $ sam --version SAM CLI, version 1.99.0 ②AWS SAM用のファイルを作成する。 以下構成でファイルを作成します。 paramiko-layerディレクトリは変えても良いです。 (current dir) ├── paramiko-layer │ └── requirements.txt └── template.yaml 各ファイルの中身は以下です。 【requirements.txt】 インストールするライブラリを記載します paramiko 【template.yaml】 ここで記載するContentUriはrequirements.txtを配置しているディレクトリにします。 またpython3.9、x86_64の部分は作成するlambdaに合わせて読み替えてください。 Resources: ParamikoLayer: Type: AWS::Serverless::LayerVersion Properties: ContentUri: \u0026#39;paramiko-layer/\u0026#39; CompatibleRuntimes: - python3.9 CompatibleArchitectures: - x86_64 Metadata: BuildMethod: python3.9 ③レイヤーをビルドする template.yamlがあるディレクトリにて以下コマンドを実行します。 ParamikoLayerは変えても良いです。 sam build ParamikoLayer これにより現在のディレクトリ配下に.aws-samディレクトリが作成され、ビルドされたファイルができあがります。 これをCLI上のコマンドでlambdaのレイヤーにデプロイすることもできますが、今回はZIPをアップする方法でデプロイしてみます。 生成されたpythonディレクトリ配下にあるファイルを以下コマンドで圧縮します。 cd .aws-sam/build/ParamikoLayer/ zip -r paramiko-layer.zip python/* 作成したparamiko-layer.zipをAWSマネジメントコンソールでアップロードします。 lambdaのレイヤーページを開き、『レイヤーの作成』をクリックします。 以下入力して『作成』をクリックする。 項目 内容 名前 任意のレイヤーの名前 説明 空で良いです。必要ならレイヤーの説明を記載してください。 アップロード ラジオで『.zipファイルをアップロード』を選択し、『アップロード』よりさきほど生成したzipを選択する。 互換性のあるアーキテクチャ template.yamlに記載したものと合わせてください。 互換性のあるランタイム template.yamlに記載したものと合わせてください。 ライセンス 空で良いです。何かしらレイヤーにライセンスをつける場合は記載してください。 あとはlambda関数で上記レイヤーを設定し、paramikoを利用するだけです。 実装サンプルとしては以下のようになります。 import paramiko def lambda_handler(event, context): sshClinet = paramiko.SSHClient() policy = paramiko.client.MissingHostKeyPolicy() sshClinet.set_missing_host_key_policy(policy) sshClinet.set_missing_host_key_policy(paramiko.AutoAddPolicy()) sshClinet.connect(\u0026#39;接続先ホスト名\u0026#39;, \u0026#39;接続先ポート\u0026#39;, \u0026#39;接続先ユーザー名\u0026#39;, \u0026#39;接続先パスワード\u0026#39;) sftpClient = sshClinet.open_sftp() sftpClient.get(\u0026#39;取得するファイルのパスとファイル名\u0026#39;, \u0026#39;接続元(lambda)の配置先パスとファイル名\u0026#39;) 〜後続処理を記載〜 参考 Lambdaで絶対にエラーならないレイヤーの作成方法 #AWS - Qiita"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2023/07/04/aws-lightsail-vpn-india/",
    title: "【AWS】LightsailでインドのVPNサーバーを構築する",
    date: "2023-07-04T00:00:00Z",
    body: "【AWS】LightsailでインドのVPNサーバーを構築する 諸事情によりインドのIPアドレスが必要になったため、インドにVPNサーバーを建てました。 手軽に済ませるため、AWS Lightsailを使って実現しました。 ■構築手順 ※AWSアカウントを持っている前提とします。 Lightsailの管理画面を開き、新規インスタンスの作成を行います。 インスタンスのリージョンはインドのムンバイ（ap-south-1a）にし、Amazon LinuxのOSのみで立ち上げます。 自分しか使わない前提だったので、料金プランは最安のものにします。 ここは利用用途に合わせて変えてください。 上記でインスタンスを作成します。 作成後しばらく待つとインスタンスが立ち上がるので、縦三点リーダより「接続」をクリックします。 Webブラウザ上でコンソールが立ち上がるので、VPNサーバーのDockerイメージを立ち上げます。 なお、以下の値は好きに書き換えてください。 common_key：接続に利用する共通鍵文字列 user：接続ユーザー password：接続パスワード $ sudo -i # yum install -y docker # systemctl start docker # systemctl enable docker # docker run -d --privileged -p 500:500/udp -p 4500:4500/udp -p 1701:1701/tcp -p 5555:5555/tcp -e PSK=\u0026#39;common_key\u0026#39; -e USERS=\u0026#39;user:password\u0026#39; siomiz/softethervpn 以上でインスタンス内の設定は完了です。 続いてネットワーク設定をします。 Lightsail管理画面より作成したインスタンスの設定画面に入り、ネットワーキングタブにて「静的IPをアタッチする」をクリックします。 色々ダイアログが表示されて質問されますが、指示通りに続行していけば固定IPが割り当てられます。 次にファイアーウォール設定で以下を解放します。 TCP 1701 TCP 5555 UDP 500 UDP 4500 また、デフォルトで全IPに空いている22ポートや80ポートも余力があれば閉じておきます。 以上でVPNサーバー構築は完了です。 Macから接続してみます。 ネットワーク設定にてL2TP over IPsecのVPNを追加します。 サーバアドレスは作成したインスタンスのIPで、認証情報はVPNサーバー構築コマンドの中で指定したものを入れます。 VPNのオプションで全てのトラフィックをVPN経由するようにトグルをONにしておきます。 上記で設定は完了なので、「OK」をクリックしてVPN接続をします。 以下ツールなどで接続IPがムンバイになっていることを確認できます。 https://rakko.tools/tools/2/ 以上で完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/vite/",
    title: "Vite",
    date: "2023-02-11T00:00:00Z",
    body: "Vite"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2023/02/11/vite-tutorial/",
    title: "Viteを使ってみる",
    date: "2023-02-11T00:00:00Z",
    body: "Viteを使ってみる Viteとはwebpackのようにjsやcssをコンパイルするツールです。 Vue.jsを作ったEvan Youさんが作ったらしく、Vue.js同様に楽に使い始められそうです。 https://ja.vitejs.dev/ 放置していたサイトがコンパイルできなくなっていたので、Viteでコンパイルするよう作り替えてみました。 大した実装はしていないので、条件的には1ページだけのサイトでjsとsassがコンパイルできれば良いサイトです。 Viteプロジェクトの作成 まず、以下コマンドでViteプロジェクトを新規作成します。 $ npm create vite@latest 続けていいか聞かれるので「y」と入力します。 Ok to proceed? (y) y プロジェクト名を入力します。 本例では「hogehoge-prj」。 ✔ Project name: … hogehoge-prj 次にフレームワークを聞かれます。 当サイトはフレームワークを利用していなかったので「Vanilla」を選択します。 ※上下キーで選択 ? Select a framework: › - Use arrow-keys. Return to submit. ❯ Vanilla Vue React Preact Lit Svelte Others 次にJavaScriptかTypeScriptか選択します。 当サイトは「JavaScript」です。 ※上下キーで選択 ? Select a variant: › - Use arrow-keys. Return to submit. ❯ JavaScript TypeScript これでプロジェクトが作成され、以下メッセージが表示されます。 Scaffolding project in /Users/goro/workspace/hogehoge-prj... Done. Now run: cd hogehoge-prj npm install npm run dev 上記メッセージ通りにコマンドを打つとデフォルト状態でページが立ち上がります。 VITE v4.1.1 ready in 170 ms ➜ Local: http://localhost:5173/ ➜ Network: use --host to expose ➜ press h to show help こんな感じのクリックしただけカウントアップするページが表示されます。 ファイルとしては以下が生成されています。 counter.js index.html javascript.svg main.js node_modules/ package-lock.json package.json public/ style.css sassのインストール Viteはそのままではsassに対応していないようなので別途インストールします。 $ npm add -D sass 既存サイトをViteに取り込む 生成したViteプロジェクトに既存のサイトを反映させます。 index.htmlの修正 まず、index.htmlにページのテンプレート（html部分）を反映します。 jsの読み込みはbodyの最後でmain.jsを読み込むように記載するのみです。 cssに関してはコンパイルするものは記載する必要ありません。後述のpublicディレクトリに置かれたcssファイルは読み込む記載をする必要があります。 以下はデフォルトのindex.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/svg+xml\u0026#34; href=\u0026#34;/vite.svg\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Vite App\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;/main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; main.jsの修正 次にmain.jsにて必要なjsファイルやsassファイル、画像ファイルをimportします。 デフォルトではindex.htmlに\u0026lt;div id=\u0026quot;app\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;があるので、そこにjsにてHTMLを流し込んでいます。 固定な部分についてはJSで流し込まずにindex.htmlに直接記載してしまっても良さそうです。 既存サイトを取り込む際は必要な処理が他JSに全て書かれていると思うので、main.jsにはimportだけ記載すれば良いです。 以下はデフォルトのmain.js import \u0026#39;./style.css\u0026#39; import javascriptLogo from \u0026#39;./javascript.svg\u0026#39; import { setupCounter } from \u0026#39;./counter.js\u0026#39; document.querySelector(\u0026#39;#app\u0026#39;).innerHTML = ` \u0026lt;div\u0026gt; \u0026lt;a href=\u0026#34;https://vitejs.dev\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/vite.svg\u0026#34; class=\u0026#34;logo\u0026#34; alt=\u0026#34;Vite logo\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://developer.mozilla.org/en-US/docs/Web/JavaScript\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;${javascriptLogo}\u0026#34; class=\u0026#34;logo vanilla\u0026#34; alt=\u0026#34;JavaScript logo\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h1\u0026gt;Hello Vite!\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;counter\u0026#34; type=\u0026#34;button\u0026#34;\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p class=\u0026#34;read-the-docs\u0026#34;\u0026gt; Click on the Vite logo to learn more \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ` setupCounter(document.querySelector(\u0026#39;#counter\u0026#39;)) ここでimportしたファイルはCacheBusting対応（キャッシュ対策でファイル名にランダムな文字列が付く）されます。 画像ファイルに関しては上記デフォルトファイル内のsrc=\u0026quot;${javascriptLogo}\u0026quot;でCacheBustingされたファイル名が読み込まれます。 その他固定ファイルの配置 生成されているpublicディレクトリ配下に置かれたファイルやディレクトリはそのままDocumentRootに置かれます。 コンパイルする必要の無いjs/cssファイルやCacheBustingをしない画像ファイルはここにおきます。 不要ファイルの削除 以下のファイルは不要なので削除してしまいます。 counter.js javascript.svg public/vite.svg これでViteを使ったサイト構築は完了です。 npm run devコマンドでローカルで動かせますし、npm run buildでコンパイルしたファイルがdistディレクトリに出力されます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2023/01/29/aws-s3-cloudfront-not-redirect/",
    title: "【AWS】S3のリダイレクトが効かない",
    date: "2023-01-29T00:00:00Z",
    body: "【AWS】S3のリダイレクトが効かない ■発生した事象 AWSにて、S3 静的ウェブサイトホスティング + Cloud Frontを利用してWebページを公開していました。 そのWebページにてリダイレクト処理が必要になったので、以下を参考にS3へリダイレクト設定を仕込みました。 AWS公式S3ドキュメント しかし、実際にブラウザからアクセスするとリダイレクトされませんでした。 Cloud Frontのキャッシュクリアも無意味で、リダイレクト設定を見直しても正しそうでした。 試しにバケットウェブサイトエンドポイントのURLを直接開くとリダイレクトされました。 なので、被疑はCloud Frontとの連携部分にありそうです。 ■原因 Cloud Frontにてオリジンドメインを指定する際に選択肢からS3を選択してしまうと、S3で設定したリダイレクトが効かなくなってしまうようでした。 リダイレクト設定を効かせるにはS3を「選択」するのではなく、バケットウェブサイトエンドポイントのドメインをそのまま入力する必要があるようです。 上記のようにすることでブラウザからアクセスしてもリダイレクトが効くようになりました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2022/05/03/ec2-not-inservice/",
    title: "【AWS】Auto ScalingのインスタンスがInServiceにならない",
    date: "2022-05-03T00:00:00Z",
    body: "【AWS】Auto ScalingのインスタンスがInServiceにならない ■発生した事象 AWSにてシステムを構築しており、EC2のオートスケール設定をしてあったのですが、突然オートスケールで立ち上がったインスタンスのライフサイクルが『InService』にならずに『Pending:Wait』から変わらない事象が発生しました。 今まではオートスケールで正常にインスタンスが立ち上がっていたのですが、ある日突然発生しました。 ■原因 原因はAutoscalingとCodeDeployの不一致でした。 システムはCodeDeployでGitHubのソースを持ってきてEC2にデプロイする構成で、Autoscalingでインスタンスを生成した際にはデプロイグループが紐づいているCodeDeployが実行されます。 AWS ユーザーガイド 事象発生時は「デプロイグループが紐づいているCodeDeploy」が存在しておらず、Autoscalingのライフサイクルフックが完了することができずにライフサイクルが『Pending:Wait』で止まっていたようでした。 基本的には自動的にAutoscalingグループとデプロイグループは紐づいているはずですが、何かしら障害や不具合があるとズレることがあります。 ライフサイクルが『InService』にならない時は、デプロイグループに設定されている「環境設定: Amazon EC2 Auto Scaling グループ」がAutoscalingグループと一致しているか確認した方が良さそうです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/06/19/laravel-create-http200/",
    title: "【Laravel】モデルをcreateしてもレスポンスをHTTP 200にする",
    date: "2021-06-19T00:00:00Z",
    body: "【Laravel】モデルをcreateしてもレスポンスをHTTP 200にする Laravelでモデルをcreateして返却すると、HTTPステータスは自動で201になります。 何か理由があってHTTP 200に戻したい場合はモデルのwasRecentlyCreatedをfalseにします。 public function store() { $hoge = HogeModel::create(); $hoge-\u0026gt;wasRecentlyCreated = false; // create時はtrueになっている return $hoge; }"
  },
  {
    url: "https://blog2.logical-dice.com/tags/laravel/",
    title: "Laravel",
    date: "2021-06-19T00:00:00Z",
    body: "Laravel"
  },
  {
    url: "https://blog2.logical-dice.com/tags/javascript/",
    title: "Javascript",
    date: "2021-06-06T00:00:00Z",
    body: "Javascript"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/06/06/lodash-debounce-vs-throttle/",
    title: "lodashのdebounceとthrottleの違い",
    date: "2021-06-06T00:00:00Z",
    body: "lodashのdebounceとthrottleの違い 参考 lodash document WEBページ上のボタンが連打された際にアクセス流量制限をするために、lodashのdebounceやthrottleを使うことがあります。 debounceもthrottleもどちらもリクエスト間の流量制限ができるのですが、具体的に何が変わるのか調べました。 先にまとめ debounceとthrottleの違いは以下の通りです。 オプションは指定せずデフォルトで呼び出す場合を想定しています。 debounce 処理呼び出し後、指定時間経過後に発火します。 指定時間内に処理が連続して呼び出された場合は最後に呼び出されてから指定時間後に発火します。 debounceメソッドの処理はsetTimeoutで実行されるので、awaitを指定しても待ちません。 throttle 処理呼び出し後、すぐに処理が走ります。 指定時間内に処理が連続して呼び出された場合は、指定時間間隔で発火します。 throttleメソッドの処理は最初に実行される1回はawaitで待ちますが、指定時間後に最初に即走る処理は待ちますが、throttleで遅延させている処理は待ちません。 おすすめ debounce(throttle)はオプションを指定することで挙動をカスタマイズできるので、必要なら自分の期待する挙動となるようにオプションを指定するのがオススメ。 調べた内容 debounceについて debounceについてはこちらに記載があります。 https://lodash.com/docs/4.17.15#debounce _.debounce(func, [wait=0], [options={}]) 各項目については以下の通り。 項目 内容 func 流量制限対象の処理(function)です。 wait 処理が呼び出されてから実行するまでの待機時間です。待機中に再度呼び出されたら、そこからさらに待機時間待ちます。 option 任意オプションをオブジェクトで指定します。 option.leading 待機前に処理を実行するかどうか。デフォルト：false。 option.maxWait 処理が連続して呼び出されたとしても、maxWaitで指定した時間が経過したら処理を走らせます。デフォルト：0（指定なし）。 option.trailing 待機後に処理を実行するかどうか。デフォルト：true。 つまり、optionをデフォルトのまま呼び出すのであれば、debounceは呼び出されてから指定時間待機してから発火します。 maxWaitの指定が無いので待機時間中に呼び出され続けたら、ずっと処理は発火しません。 throttleについて throttleについてはこちらに記載があります。 https://lodash.com/docs/4.17.15#throttle _.throttle(func, [wait=0], [options={}]) 各項目については以下の通り。 項目 内容 func 流量制限対象の処理(function)です。 wait 処理が呼び出される間隔の時間です。 option 任意オプションをオブジェクトで指定します。 option.leading 待機前に処理を実行するかどうか。デフォルト：true。 option.trailing 待機後に処理を実行するかどうか。デフォルト：true。 optionはdebounceとほぼ同じですが、leadingのデフォルトがtrueでmaxWaitの指定がありません。 throttleのソースを見ると分かりますが、throttleはdebounceのオプションを指定して呼び出しているだけです。 https://github.com/lodash/lodash/blob/4.17.15/lodash.js#L10897-L10913 デフォルトでleadingがtrueになり、waitで指定された時間がoption.maxWaitにも指定されています。 つまり、throttleはデフォルト値が違うdebounceです。 図解 ボタンクリックのイベントハンドラにdebounce/throttleをデフォルトオプションで設定したとします。 1回クリック 1回だけクリックされた時の挙動は以下の通りです。 debounceの場合はwait後、throttleの場合はwait前に処理が走ります。 debounceだとクリックしてから反応するまで、1秒未満かもしれませんが少しタイムラグが感じられるかもしれません。 連打クリック 一瞬でクリック連打された場合は以下の通りです。 debounceについては1回クリックした際と同じ挙動になりますが、throttleについてはwait後にも処理が走ります。 throttleだと2回処理が走ってしまうのは微妙ですね。 クリックし続ける wait中に一定間隔でクリックし続けた場合は以下の通りです。 debounceはクリックし続けている間は処理は走らず、最後のクリックからwaitで指定した時間だけ待って処理が走ります。 throttleはクリックし続けている間もwaitで指定した時間間隔で処理が走ります。 あまり何度も走らせたくない処理であればdebounceで、一定間隔空けば何度走っても良いのであればthrottleが向いてそうです。 思ったこと debounceもthrottleも期待する挙動とはズレている事もあるので、 期待する挙動となるようにdebounceのオプションを設定すると良いのかなと思いました。 例えばクリックした時に遅延なく処理が走って欲しくて、かつ一瞬で連打した際も一定間隔でクリックし続けた場合も1回だけ処理が走って欲しい場合、 optionのleadingがtrueでtrailingがfalseになれば良さそうです。 _.debounce(callback, wait, {leading: true, trailing: false}) もし一定間隔でクリックし続けた場合は定期的に処理が走って欲しいのであれば、debounceではなくthrottleを使えば良いです。 _.throttle(callback, wait, {leading: true, trailing: false}) // debounce(callback, wait, {leading: true, maxWait: wait, trailing: false}) と同じ などなど、optionの指定次第で可能性は広がると思いました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/04/12/laravel-jwt-clear/",
    title: "【Laravel】JTIを指定してJWTトークンを無効化する",
    date: "2021-04-12T00:00:00Z",
    body: "【Laravel】JTIを指定してJWTトークンを無効化する LaravelでAPIを構築するとJWTトークンを利用することがあると思います。 JTI(トークンの識別子)を指定してJWTトークンを無効化してみます。 実装するとこんな感じです。 use Tymon\\JWTAuth\\Providers\\Storage\\Illuminate as JWTStorage; class JwtDisableService { protected $jwtStorage; public function __construct(JWTStorage $jwtStorage) { $this-\u0026gt;jwtStorage = $jwtStorage; } public function __invoke(string $jti) { // 無効化する $this-\u0026gt;jwtStorage-\u0026gt;forever($jti, \u0026#39;forever\u0026#39;); return response()-\u0026gt;json([ \u0026#39;result\u0026#39; =\u0026gt; \u0026#39;OK\u0026#39; ]); } } 注意が必要なのは、JWTトークンの有効/無効判定はブラックリストをキャッシュで持つという点です。 JWTの有効なトークンはどこかで保持されている訳ではありません。 妥当な形式な有効期限内であるトークンは明示的に無効化されてなければ使えます。 なので、上記の方法でトークンを無効化したとしても、有効期限が切れる前にキャッシュクリアしてしまうと再度使えるようになります。 そこにだけ注意して実装する必要がありそうです。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/windows/",
    title: "Windows",
    date: "2021-03-07T00:00:00Z",
    body: "Windows"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/03/07/windows-localforward-mysql-workbench/",
    title: "WindowsのMySQL Workbenchで2段SSH踏み台で接続する",
    date: "2021-03-07T00:00:00Z",
    body: "WindowsのMySQL Workbenchで2段SSH踏み台で接続する 環境 Windows 10 MySQL Workbench 8.0 WSL2 (Ubuntu 20.04) やりたいこと 作業PCからDBに接続するまでに2段階のSSH接続を挟む環境があるとします。 この環境で作業PC(Windows)からMySQL WorkbenchでDBに接続します。 基本的なSSH操作はWSL上でコマンドを打つ前提とします。 Windows上でSSH操作をする場合は後述のSSHコンフィグのパスはWindows用に読み替えてください。 (SSHコマンドや秘密鍵パスなど) 駄目なパターン WSL上の~/.ssh/configに以下の記載をしていれば、アプリケーションサーバーへSSH接続はできます。 Host fumidai-server HostName 111.111.111.111 User my-user IdentityFile ~/.ssh/id_rsa_hoge IdentitiesOnly yes Host app-server HostName 222.222.222.222 User my-user ProxyCommand ssh -W %h:%p fumidai-server IdentityFile ~/.ssh/id_rsa_hoge ForwardAgent yes ※HostName, User, IdentityFileは適宜読み替えてください。 ここで指定しているProxyCommand設定により、 app-serverへSSHする時はfumidai-serverを経由するようになります。 しかし、MySQL WorkbenchのTCP/IP over SSHでapp-serverを指定して fumidai-server ⇒ app-server ⇒ DB という接続をしようとしても接続できません。 (MacやUbuntu等であれば接続できる。なぜかWindowsだけ不可。) 良いパターン MySQL WorkbenchのTCP/IP over SSHは利用せずに、 SSH Local ForwardでDBに繋がるようにしておいてからMySQL Workbenchを繋ぎます。 まず、~/.ssh/configは以下のようにします。 Host fumidai-server HostName 111.111.111.111 User my-user IdentityFile ~/.ssh/id_rsa_hoge IdentitiesOnly yes Host app-server-localforward HostName 222.222.222.222 User my-user ProxyCommand ssh -W %h:%p fumidai-server IdentityFile ~/.ssh/id_rsa_hoge GatewayPorts yes LocalForward 53306 hogehoge.ap-northeast-1.rds.amazonaws.com:3306 先ほどとの違いはGatewayPortsとLocalForwardの設定が増えています。 これでapp-server-localforwardへSSH接続すれば、ローカルの53306ポートが DBの3306ポートに繋がるようになります。 ※DBのホスト名は適宜読み替えてください。 MySQL Workbenchの接続設定は以下のようにします。 SSHコンフィグファイルを読み込む必要はありません。 項目 内容 Connection Method Standard（TCP/IP) Hostname 127.0.0.1 Port 53306 Username DBのユーザー Password DBのパスワード これで設定は完了です。 繋ぐ場合は以下の手順で繋ぎます。 ① SSHコマンドでapp-server-localforwardに繋いでおく。 ② MySQL WorkbenchでDBに接続する。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/02/21/larave-observer-stop/",
    title: "【Laravel】Observerが実行されない",
    date: "2021-02-21T00:00:00Z",
    body: "【Laravel】Observerが実行されない 環境 Laravel 5.8 Laravelで設定したObserverの処理が実行されない事があったので、その際の調査メモです。 事象 LaravelのObserverを使いModelのupdating時に処理が走るようにしようと思ったのですが、 updatingの処理を書いても実行されませんでした。 updatingの処理と同クラス内でupdated, saving, savedの処理を記載すると実行されたので、 Observerのクラス自体が読み込まれていない訳では無さそうでした。 原因 今回Observerを設定しようとしていたクラスには既に別のObserverが設定されていました。 それ自体は問題ないのですが、その既存のObserverのupdatingにて生成した値をreturnしていました。 Laravelのソースを読むと、同イベントのObserverを複数設定している場合、 先行のObserverで値をreturnしてしまうと後続のObserverが実行されないようでした。 https://github.com/laravel/framework/blob/5.8/src/Illuminate/Events/Dispatcher.php#L198-L203 特に理由がない限り、Observerの処理内でreturnはしないほうが良さそうです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/02/17/ubuntu-howdy/",
    title: "Howdyを使ってubuntuで顔認証する",
    date: "2021-02-17T00:00:00Z",
    body: "Howdyを使ってubuntuで顔認証する 環境 Ubuntu 20.10 Thinkpad T480 参考 https://github.com/boltgolt/howdy https://wiki.archlinux.jp/index.php/Howdy Windowsを使っている時に顔認証（Windows Hello）でログインできるのがとても便利だったのでUbuntuでも実現してみます。 UbuntuではHowdyというアプリケーションで実現できそうなので設定してみました。 赤外線カメラ情報の取得 顔認証には赤外線カメラを使います。 カメラ情報を取得するためにv4l-utilsを利用します。 v4l-utilsインストール sudo apt-get install v4l-utils カメラ情報確認 以下コマンドでカメラ情報を確認します。 赤外線カメラ(IR Camera)の項目を確認します。 v4l2-ctl --list-devices --all # 以下出力結果で見る場所 Integrated IR Camera: Integrate (usb-0000:00:14.0-5): /dev/video0　←赤外線カメラのパスはvideo0※ /dev/video1 Format Video Capture: Width/Height : 340/340　←キャプチャサイズは340×340 ※Thinkpat T480だとvideo1も列挙されてましたが、後々の設定でvideo1を指定すると動きませんでした。 Howdy導入 Howdyのインストール まず必要なソフトをインストールします。 sudo add-apt-repository ppa:boltgolt/howdy sudo apt update sudo apt install howdy 途中で以下のようにセキュア具合を聞かれます。 自分の好みや状況に合わせて選択してください。 (f:早さ重視、b:バランス型、s:セキュリティ重視) After detection, Howdy knows how certain it is that the match is correct. How certain Howdy needs to be before authenticating you can be customized. F: Fast. Allows more fuzzy matches, but speeds up the scanning process greatly. B: Balanced. Still relatively quick detection, but might not log you in when further away. S: Secure. The safest option, but will take much longer to authenticate you. You can always change this setting in the config. What profile would you like to use? [f/b/s]: ←ここでfかbかsを入力 設定ファイル更新 以下コマンドでHowdyの設定ファイルを編集します。 vimユーザーには慣れないUIですが、下部に記載されているショートカットを参照しながら対応します。 (^:Ctrl押しながら　M-:Alt押しながら) sudo howdy config 以下の箇所の設定をします。 ※機種によってはデフォルトのままでも自動で設定されるようですが、Thinkpad T480では自動で設定されませんでした。 device_path = /dev/video0 ← v4l2-ctlで確認したパス frame_width = 340 ← v4l2-ctlで確認したキャプチャサイズ frame_height = 340 dark_threshold = 100 ← 暗さ調整（後述） これで設定は完了です。 以下コマンドを実行すると顔を検知できるかテストできます。 sudo howdy test テスト画面で『DARK FRAME』と表示される場合は暗さ調整が必要です。 設定ファイルのdark_thresholdの値を大きくしてください。 顔の登録 以下コマンドで顔の登録をします。 コマンドを入力すると登録名の入力を求められるので必要に応じて入力してください。 （そのままEnterでも可） その後、顔登録が始まるのでカメラを見つめて待機します。 sudo howdy add これでAdded a new model to {ユーザー名}と表示されれば登録完了です。 pam設定 /etc/pam.d/配下のファイルにHowdyの設定を追加します。 よく使いそうなのだとloginとsudoあたりかと思います。 /etc/pam.d配下のファイルには以下行を追加します。 auth sufficient pam_python.so /lib/security/howdy/pam.py これでHowdyの設定は完了です。 顔認証でログインやsudoができるようになっていることを確認してください。 使ってみた所感、確かに顔認証でログインできるのですが ログイン後にChromeを起動したりするタイミングで結局パスワードを聞かれます。 本当にセキュアな部分は顔認証だけではダメなようでした。 トラブルシューティング Camera path is not configured エラー sudo howdy addを実行すると以下エラーが出ました。 Camera path is not configured correctly, please edit the \u0026#39;device_path\u0026#39; config value. Exception ignored in: \u0026lt;function VideoCapture.__del__ at 0x7fe09a3a05e0\u0026gt; Traceback (most recent call last): File \u0026#34;/usr/lib/security/howdy/recorders/video_capture.py\u0026#34;, line 55, in __del__ self.internal.release() AttributeError: \u0026#39;VideoCapture\u0026#39; object has no attribute \u0026#39;internal\u0026#39; デバイスを自動で見つけることができなかったようです。 sudo howdy configでdevice_pathを指定してください。 No face detected エラー sudo howdy addを実行すると以下エラーが出ました。 Please look straight into the camera No face detected, aborting カメラを真っ直ぐ見ていても発生するのですが、これはキャプチャサイズが自動検知できていないのが原因でした。 sudo howdy configでframe_width/frame_heightを指定してください。 All frames were too dark エラー sudo howdy addを実行すると以下エラーが出ました。 Please look straight into the camera All frames were too dark, please check dark_threshold in config Average darkness: [77.42311], Threshold: 50.0 部屋を明るくしたり顔にライトを当てても駄目でした。 sudo howdy configでdark_thresholdを100にしたら治りました。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/ubuntu/",
    title: "Ubuntu",
    date: "2021-02-17T00:00:00Z",
    body: "Ubuntu"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/02/14/ubuntu-mouse-keymap/",
    title: "ubuntuで多機能マウスを設定する",
    date: "2021-02-14T00:00:00Z",
    body: "ubuntuで多機能マウスを設定する 環境 Ubuntu 20.10 Elecom M-DT2DR BK(マウス) 参考 https://help.ubuntu.com/community/MouseCustomizations https://wiki.archlinux.jp/index.php/Xbindkeys Ubuntuにマルチボタンのマウス（Elecom M-DT2DR）を接続すると、何をしなくても通常のマウス操作や「進む」「戻る」ボタンは機能します。 ですが、機能ボタン（Fn 1〜3）には何もマッピングされていないので、音量操作をマッピングします。 手順 インストール まず必要なソフトをインストールします。 sudo apt install xbindkeys ボタン番号の取得 次にマウスのボタン番号を取得します。 まず以下コマンドを実行してボタン検知ができる状態にします。 xev 現れる白い四角が書かれたウィンドウにマウスカーソルを持っていき、その状態でマウスのボタンを押すとコンソールに情報が表示されます。 例えば私のマウス（M-DT2DR）のFn 1であれば以下のようにbutton 10と表示されます。 ButtonRelease event, serial 36, synthetic NO, window 0x4000001, root 0x7ed, subw 0x4000002, time 1758091, (31,39), root:(1951,76), state 0x0, button 10, same_screen YES 同様に、Fn 2はbutton 11でFn 3はbutton 12でした。 ボタンのマッピング ボタンマッピングするための設定ファイルを作成します。 設定ファイルは空ファイルから始めても良いのですが、一応デフォルト値から生成しておきます。 xbindkeys --defaults \u0026gt; ~/.xbindkeysrc 作成した設定ファイルに以下を追記します。 # Increase volume \u0026#34;pactl set-sink-volume @DEFAULT_SINK@ +1000\u0026#34; b:10 # Decrease volume \u0026#34;pactl set-sink-volume @DEFAULT_SINK@ -1000\u0026#34; b:11 # Mute volume \u0026#34;pactl set-sink-mute @DEFAULT_SINK@ toggle\u0026#34; b:12 設定ファイルを保存したら、以下コマンドで読み込みます。 xbindkeys -p これでマウスのFn 1〜3ボタンに音量操作がマッピングできました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/02/11/ubuntu-20-10-initialize/",
    title: "ubuntu 20.10の初期設定",
    date: "2021-02-11T00:00:00Z",
    body: "ubuntu 20.10の初期設定 環境 Ubuntu 20.10 ThinkPad T480 Thinkpad T480にubuntu20.10をインストールした際のメモです。 日本語入力設定 そのままでも日本語入力はできますが、入力ソースをMozcに切り替えたりしないといけないので 入力ソースのMozcの優先度を高くします。 アプリランチャーより『設定』を開き、『地域と言語』を開きます。 そこの入力ソースに「日本語」と「日本語(Mozc)」が並んでいるので 「日本語(Mozc)」が上にくるように並べ替えます。 各入力ソースの左端の6丸をドラッグすれば順番を入れ替えられます。 IMEのON/OFFをMac風にする アプリランチャーより『Mozcの設定』を開き、『一般』タグを開きます。 「キー設定の選択」の横にある「編集」ボタンをクリックするとキー設定を変更できます。 そこでHenkan/Hiragana/Katakanaあたりに『IME有効』を割り当て、 Muhenkanに『IME無効』を割り当てます。 これでMacのようにIMEのON/OFFを切り替えられるようになりました。 CapsLockをCtrlに、半角/全角をEscにする Gnome Tweak Toolを使って制御します。 まずは以下コマンドでインストールします。 sudo apt install gnome-tweak-tool ランチャーから起動できるので「ユーティリティ \u0026gt; Tweaks」を開きます。 Tweaksが開いたら『キーボードとマウス』の『追加のレイアウトオプション』をクリックします。 オプション画面が開くので『Ctrl position \u0026gt; Caps LockをCtrlとして扱う』と『日本語キーボードオプション \u0026gt; Make Zenkaku Hankaku an additional Esc』にチェックを入れます。 tiling機能をOFFにする デフォルトだとウィンドウを画面端にドラッグすると勝手に最大化します。 無くてもいい機能なので以下コマンドで無効化します。 gsettings set org.gnome.mutter edge-tiling false ステータスバーを下に移動する デフォルトだと時計や電池残量が表示されているバーがウィンドウ上にあり、アプリケーションバーはウィンドウ左にあります。 Windowsのように全てウィンドウ下に並んでいる方が好みなので、Dash to Panelを使って移動させます。 Dash to Panelのインストールは以下ページにてOn/OffスイッチをクリックしてOnにするだけです。 https://extensions.gnome.org/extension/1160/dash-to-panel/ これでステータスバーはウィンドウ下に移動します。 設定を変更したい場合はステータスバーを右クリックして「Dash to Panelの設定」を選択すればできます。 諸々インストール 基本セット sudo apt install vim curl npm PHP sudo apt install php7.4 composer php7.4-bcmath php7.4-common php7.4-gd php7.4-json php7.4-mbstring php7.4-mysql php7.4-xml php7.4-xmlrpc MySQL Workbench 公式ページからdebをダウンロードしてインストールしました。 https://dev.mysql.com/downloads/workbench/ Shutter(スクショツール) 既存画像に文字や矢印を書き込む事もできるので便利です。 sudo add-apt-repository ppa:linuxuprising/shutter sudo apt update sudo apt install shutter apacheを起動しないようにする 検証環境としてDocker等を利用するならローカルでApacheが動いていても邪魔なので止めます。 systemctl stop apache2 systemctl disable apache2 その他参考ページ ホームディレクトリの英語化 Tilixの導入 Terminator+fish環境構築 Docker環境構築 L2TPクライアント設定 デュアルブート環境で時計が狂うのを防ぐ ubuntuで多機能マウスを設定する Howdyを使ってubuntuで顔認証する"
  },
  {
    url: "https://blog2.logical-dice.com/tags/git/",
    title: "Git",
    date: "2021-02-06T00:00:00Z",
    body: "Git"
  },
  {
    url: "https://blog2.logical-dice.com/tags/php/",
    title: "PHP",
    date: "2021-02-06T00:00:00Z",
    body: "PHP"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/02/06/phpcs-git-diff/",
    title: "修正したファイルだけPHP_CodeSnifferでチェックする",
    date: "2021-02-06T00:00:00Z",
    body: "修正したファイルだけPHP_CodeSnifferでチェックする PHP_CodeSniffer (phpcs)を使ってコードのPSR2のフォーマットチェックをする際、 普段からphpcsを使っているプロジェクトで無いと警告が大量に出て見切れません。 そこで、git diffを使って、今回修正したファイルだけphpcsでチェックすることにしました。 起点ブランチを作る まず、修正の起点ブランチを用意します。 起点ブランチとは、phpcsでチェックしたい修正をし始める前の状態のブランチです。 他の人が更新していなければ既存のマージ先ブランチ(master, develop等)で良いですが、 他の人がマージ先ブランチを更新していた場合はgit diffで他の人の修正分も差分として出てしまうのでダメです。 (自分の修正分だけをgit diffで出すオプションはたぶん無いはず) その場合はgit logで自分が修正を始めた１つ前のコミットIDを確認して、 git checkout -b tempBranch 『コミットID』でブランチを作ってしまうのが簡単だと思います。 tempBranchの部分は作成する起点ブランチ名なので、なんでも良いです。 phpcsでチェックする 起点ブランチが用意できたら早速phpcsでチェックします。 チェックしたいブランチになっている状態で以下コマンドを実行します。 git diff 『起点ブランチ』 --name-only | xargs phpcs --standard=\u0026#34;PSR2\u0026#34; これで修正したファイルについてのみphpcsを実行できました。 今回かPSR2のチェックをしていますが、そこはプロジェクトで採用している基準に合わせて変更してください。 ※補足 phpcsの実行パスについては自分の環境に合わせて修正してください。 例えばcomposerでプロジェクトに入れたphpcsを利用する場合は./vendor/bin/phpcsという風になると思います。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/googleservice/",
    title: "GoogleService",
    date: "2021-01-25T00:00:00Z",
    body: "GoogleService"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/01/25/lost-tag-qr/",
    title: "Googleフォームを使って迷子札を作る",
    date: "2021-01-25T00:00:00Z",
    body: "Googleフォームを使って迷子札を作る 子供たちが自由に走り回れるようになり、ショッピングセンターやアミューズメント施設で迷子になるリスクが高くなったので迷子札の作成を検討しました。 迷子札の条件は以下のように設けました。 子供の持ち物等に付けておく 子供を見つけてくれた人が親に連絡できるようにする 自分の連絡先を札に直接書くのは嫌 連絡先が悪意ある人に漏れた時にすぐに変更できるようにしたい 捨てメアドを作るのは管理など面倒なので嫌 そこで、Googleフォームです。 以下の流れで迷子札を作ってみます。 ① Googleフォームで問い合わせフォームを作る。 ② 作成したGoogleフォームのURLをQRコード化する。 ③ QRコードが入ったアクセサリ等を作成する。 今回は迷子札想定で作成していますが、QRコードをシールにしてスマホや旅行バッグ等に貼っておけば、 紛失したとしても善良な人が拾ってくれたら連絡が来るかもしれません。 前提条件 既にGoogleアカウント(Gmail)を持っていること。 手順詳細 Googleフォーム作成 まず、Googleフォームで問い合わせフォームを作ります。 https://www.google.com/intl/ja_jp/forms/about/ フォームの内容は連絡者と連絡が取れる程度で良いと思います。 フォームの設定を変えます。 右上の歯車アイコンをクリックして、設定ポップアップを開きます。 まず全般タブでは基本的に全てのチェックを外しておきます。 プレゼンテーションタブの「確認メッセージ」で連絡をしてくれた人に表示するメッセージを変更できます。 テストフォームでは無いので、テストタブは何もしません。 次に連絡してもらった時の受け取り方を設定します。 回答タブを選択して『…』が縦に並んでいるアイコンをクリックします。 そこで『新しい回答についてのメール通知を受け取る』にチェックを入れます。 これで誰かがGoogleフォームを送信してくれたら自分宛にメールが届きます。 連絡を受けた履歴をスプレッドシートに残しておきたいなら『回答先を選択』で選択しておきます。 これでフォームは完成です。 右上の『送信』よりフォームのURLを取得します。 QRコード作成 QRコード生成サービスを使ってGoogleフォームのURLをQRコード化します。 無料で使えるところだと以下のようなサービスがあります。 https://qr.quel.jp/ アクセサリー作成 生成したQRコードは色んな使い方ができます。 シールにしてもいいし、アクセサリーにしてもいいと思います。 今回はアクセサリーを作ってみました。 利用したのは以下のサービスです。 https://originalprint.jp/ アクセサリーのデザインは自分で決められます。 こんな感じで作れます。 実際にスマホでQRコードを読み込んでGoogleフォームを開き、連絡先等を送信してみます。 自分のGmail宛にGoogleフォームが送信された旨のメールが届けば成功です。 （フォーム送信からメール着まで、だいたい1分弱のタイムラグがあります） これを子供のカバンに付けておきます。 GoogleフォームのURLが悪意ある人に漏れた場合はGoogleフォームを作り変えればOKです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/01/13/laravel-auth-user/",
    title: "LaravelのQueue内でAuth::user()が取れたり取れなかったりする",
    date: "2021-01-13T00:00:00Z",
    body: "LaravelのQueue内でAuth::user()が取れたり取れなかったりする 環境 Laravel 5.8 Laravelで『データが更新されたらアラートメールで更新者情報を送る』というイベントリスナーを作ったのですが、 その中でAuth::user()の値が環境により「更新者情報」だったり「null」だったりしました。 このイベントリスナーで登録した処理はキュー(Queue)に登録されて非同期に処理されるようにしていたので、 非同期処理では当然ながらAuth::user()でユーザー情報は取れません。 なので、nullになるのは妥当な動きでした。 では、なぜ環境によりユーザー情報が取れてしまったのかというと それは.envのQUEUE_DRIVERの値が原因でした。 QUEUE_DRIVER=database等にしているとキューの処理は非同期に処理されるのですが、 QUEUE_DRIVER=syncではキューの処理も同期的に処理されるようになります。 キューが同期的に実行されるとユーザーの情報がAuth::user()で取得できる状態でキューの処理が走るので、 ローカル環境などでQUEUE_DRIVER=syncにしているとキュー内でAuth::user()が動いてしまいます。 これも、ちゃんと.envが本番相当になっている環境でテストをすれば見つけられるので、 ローカル検証だけでリリースをするのではなく、ちゃんと本番相当の環境でもテストしましょう。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/typescript/",
    title: "Typescript",
    date: "2021-01-07T00:00:00Z",
    body: "Typescript"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2021/01/07/git-revert-revert/",
    title: "Typescript + Vue.jsで子コンポーネントのプロパティにアクセスする",
    date: "2021-01-07T00:00:00Z",
    body: "Typescript + Vue.jsで子コンポーネントのプロパティにアクセスする Typescript + Vue.jsの構成で親コンポーネントから子コンポーネントのプロパティにアクセスする場合、 普通のjavascriptと同じように実装するとエラーとなる場合があります。 例えば、以下のように記載するとします。 # template \u0026lt;HogeChild ref=\u0026#34;hogeChildRef\u0026#34; /\u0026gt; # script import HogeChild from \u0026#34;〜対象ファイルパス〜\u0026#34;; @Component({ components: { HogeChild } }) export default class HogeParent extends Vue { changeChildFlg(): void { this.$refs.hogeChildRef.someFlg = true; } } すると以下のエラーが発生することがあります。 Property \u0026#39;someFlg\u0026#39; does not exist on type \u0026#39;Vue | Element | Vue[] | Element[]\u0026#39;. Property \u0026#39;someFlg\u0026#39; does not exist on type \u0026#39;Vue\u0026#39;.Vetur(2339) この場合、Typescriptを使う場合は子コンポーネント呼び出し時にも型を指定してあげる必要があります。 子コンポーネント呼び出し部分を以下のようにすると動きます。 changeChildFlg(): void { const hogeChildRef = \u0026lt;HogeChild\u0026gt;this.$refs.hogeChildRef; hogeChildRef.someFlg = true; } どういう条件で発生するか、詳細は把握していませんが、 Typescript内にVue.jsを書くと問題ないけど、Vue.js内にTypescriptを書くとエラーになる気がします。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/vue.js/",
    title: "Vue.js",
    date: "2021-01-07T00:00:00Z",
    body: "Vue.js"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/12/28/git-revert-revert/",
    title: "gitで一旦取り消しRevertをしたら、RevertのRevertをすべし",
    date: "2020-12-28T00:00:00Z",
    body: "gitで一旦取り消しRevertをしたら、RevertのRevertをすべし いろんなプロジェクトを渡り鳥していると、「間違ってmaster/developにマージしちゃいました」案件がちょいちょいあります。 その場合、基本的にはmaster/developでgitのRevertをして対応してました。 でも、その後RevertのRevertをするのが漏れてしまうパターンがよくあります。 RevertのRevertが必要だと分かっていて漏れてしまうのは仕方ないのですが、 以前手伝っていたプロジェクトで「え？なんで必要なの？」っていう人がいたのを思い出したので記事にしてみました。 まずこんなソースがmasterブランチにあったとします。 ここで$hogeだけでなく$foooという値も返せるようにしたくなったとします。 ブランチ「feature/fooo」を切って修正し、masterにマージします。 ここで、$foooを返すメソッドを作ってなかったことに気づき、masterでrevertします。 「feature/fooo」にメソッドを追加し、masterにマージします。 クラス変数やコンストラクタの処理が残っているように見えます。 ※なお、ここでコンフリクトは起きていません。 できあがったソースがこちらです。 最後に追加したメソッドのみが追加されており、クラス変数やコンストラクタの処理は入っていません。 これだと想定した挙動になりませんね。 勘違いされる要素として、Revertは「対象コミットを取り消す」処理だと思っている人がたまにいます。 Revertはコミットを取り消すのではなく「対象コミットを打ち消すような修正コミットを新たに発行する」処理です。 Revert対象のコミットが無かったことになっている訳ではありません。 なので、Revertコミットはそれを更にRevertしないと、Revertコミットの修正が残ってしまいます。 手順をまとめると以下のようになります。 ダメなパターン 作業ブランチからmasterにマージする。 masterでリバートする。 作業ブランチで修正を続ける。 作業ブランチからmasterにマージする。 良いなパターン 作業ブランチからmasterにマージする。 masterでリバートする。 作業ブランチにmasterをマージする。(ダメなパターンで漏れてる) 作業ブランチでRevertをRevertする。(ダメなパターンで漏れてる) 作業ブランチで修正を続ける。 作業ブランチからmasterにマージする。 という訳でRevertした修正を捨てる訳でなければRevertのRevertをしましょう。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/10/25/javascript-autocomplete/",
    title: "javascriptでライブラリ無しでオートコンプリート",
    date: "2020-10-25T00:00:00Z",
    body: "javascriptでライブラリ無しでオートコンプリート javascriptを使ってイチからオートコンプリートを実装しました。 オートコンプリートは何もなければライブラリを使って実装するのが早いですが、 イチから自分で実装する機会があったのでメモを残します。 要件 PC(Chrome, IE, Safari), Android(Chrome), iPhone(safari)に対応する。 入力したキーワードを含む都道府県を選択肢として表示する。 選択肢をクリックしたら選択したこととする。 PCでは選択肢からカーソルキー\u0026amp;エンターキーでも選択できるようにする。 この4つ目が肝です。 対応方法 こちらに動くものを用意してあります。 デモ HTMLは以下のようになっているとします。 テキストボックスに文字を入力するとulタグ内に選択肢が表示されるようにします。 \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;keyword\u0026#34; placeholder=\u0026#34;都道府県名を入れてください\u0026#34; autocomplete=\u0026#34;off\u0026#34;/\u0026gt; \u0026lt;ul id=\u0026#34;outputBox\u0026#34;\u0026gt;\u0026lt;/ul\u0026gt; javascriptは以下の通りです。 // 都道府県一覧 const prefectureList = [\u0026#39;北海道\u0026#39;,\u0026#39;青森県\u0026#39;,\u0026#39;岩手県\u0026#39;,\u0026#39;宮城県\u0026#39;,\u0026#39;秋田県\u0026#39;,\u0026#39;山形県\u0026#39;,\u0026#39;福島県\u0026#39;, \u0026#39;茨城県\u0026#39;,\u0026#39;栃木県\u0026#39;,\u0026#39;群馬県\u0026#39;,\u0026#39;埼玉県\u0026#39;,\u0026#39;千葉県\u0026#39;,\u0026#39;東京都\u0026#39;,\u0026#39;神奈川県\u0026#39;, \u0026#39;新潟県\u0026#39;,\u0026#39;富山県\u0026#39;,\u0026#39;石川県\u0026#39;,\u0026#39;福井県\u0026#39;,\u0026#39;山梨県\u0026#39;,\u0026#39;長野県\u0026#39;,\u0026#39;岐阜県\u0026#39;, \u0026#39;静岡県\u0026#39;,\u0026#39;愛知県\u0026#39;,\u0026#39;三重県\u0026#39;,\u0026#39;滋賀県\u0026#39;,\u0026#39;京都府\u0026#39;,\u0026#39;大阪府\u0026#39;,\u0026#39;兵庫県\u0026#39;, \u0026#39;奈良県\u0026#39;,\u0026#39;和歌山県\u0026#39;,\u0026#39;鳥取県\u0026#39;,\u0026#39;島根県\u0026#39;,\u0026#39;岡山県\u0026#39;,\u0026#39;広島県\u0026#39;,\u0026#39;山口県\u0026#39;, \u0026#39;徳島県\u0026#39;,\u0026#39;香川県\u0026#39;,\u0026#39;愛媛県\u0026#39;,\u0026#39;高知県\u0026#39;,\u0026#39;福岡県\u0026#39;,\u0026#39;佐賀県\u0026#39;,\u0026#39;長崎県\u0026#39;, \u0026#39;熊本県\u0026#39;,\u0026#39;大分県\u0026#39;,\u0026#39;宮崎県\u0026#39;,\u0026#39;鹿児島県\u0026#39;,\u0026#39;沖縄県\u0026#39;]; // in/out要素を変数に入れておく const inputElm = document.getElementById(\u0026#39;keyword\u0026#39;); const outboxElm = document.getElementById(\u0026#39;outputBox\u0026#39;); /** * テキストボックスに文字入力時 */ inputElm.addEventListener(\u0026#39;compositionend\u0026#39;, function(e) { // タブキーは次の要素に移動させるのでスルー if (e.key == \u0026#39;Tab\u0026#39;) { return; } outboxElm.innerHTML = \u0026#39;\u0026#39;; let _this = this; setTimeout(function(){ // 入力された文字を含む都道府県を抽出する prefectureList.forEach(function(prefecture){ if (prefecture.indexOf(_this.value) != -1) { let liElm = document.createElement(\u0026#39;li\u0026#39;); liElm.setAttribute(\u0026#39;tabindex\u0026#39;, \u0026#39;0\u0026#39;); // 表示する選択肢にクリックイベントを設定する liElm.onclick = function() { // 選択肢をクリックしたらテキストボックスに反映する inputElm.value = this.innerHTML; outboxElm.innerHTML = \u0026#39;\u0026#39;; } // キーワードを含む選択肢をulに追加する let txtNode = document.createTextNode(prefecture); liElm.appendChild(txtNode); outboxElm.appendChild(liElm); } }); }, 10); }); /** * テキストボックスにフォーカスが当たっている時のキー操作 */ inputElm.addEventListener(\u0026#39;keydown\u0026#39;, function(e) { // 下キーを入力されたら選択肢の一番上にフォーカスを当てる if (e.key == \u0026#39;ArrowDown\u0026#39; || e.key == \u0026#39;Down\u0026#39;) { let firstElm = outboxElm.getElementsByTagName(\u0026#39;li\u0026#39;)[0]; if (firstElm) { firstElm.focus(); } e.preventDefault(); } }); /** * 選択肢にフォーカスが当たっている時のキー操作 or Enter */ outboxElm.addEventListener(\u0026#39;keydown\u0026#39;, function(e) { var currentElm = document.activeElement; // 下キーを入力されたら次の選択肢にフォーカスを当てる // ブラウザによりArrowDownだったりDownだったりする if (e.key == \u0026#39;ArrowDown\u0026#39; || e.key == \u0026#39;Down\u0026#39;) { if (currentElm.nextElementSibling) { currentElm.nextElementSibling.focus(); } e.preventDefault(); } // 上キーを入力されたら前の選択肢にフォーカスを当てる // ブラウザによりArrowUpだったりUpだったりする if (e.key == \u0026#39;ArrowUp\u0026#39; || e.key == \u0026#39;Up\u0026#39;) { if (currentElm.previousElementSibling) { currentElm.previousElementSibling.focus(); } e.preventDefault(); } // Enterを入力されたら現在の選択肢をクリックした時と同じ挙動をする if (e.key == \u0026#39;Enter\u0026#39;) { currentElm.click(); } }); これで基本的な挙動は実装できました。 なお、これだけではキーワードをBackspaceやDeleteで削除した際に選択肢を変更する処理は入っていません。 必要であればkeydownイベント等を利用して文字削除時の処理を入れてください。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/node/",
    title: "Node",
    date: "2020-09-15T00:00:00Z",
    body: "Node"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/09/15/webpack-swiper-env-diff/",
    title: "swiper.scssがwebpackで取り込まれない",
    date: "2020-09-15T00:00:00Z",
    body: "swiper.scssがwebpackで取り込まれない 環境 webpack 4.44.1 swiper 6.0.0 事象 webpackでswiperを取り込んでビルドしようとした時の話。 swiper.scssがなぜか開発環境では取り込めてて、本番環境では取り込めてないという事象が起きました。 つまり、↓だとswiper.scssが取り込まれたCSSが出力されるんですが node node_modules/webpack/bin/webpack.js --mode development ↓だとswiper.scssが取り込まれていないCSSが出力されます。 node node_modules/webpack/bin/webpack.js --mode production 対応 原因は結局不明だったのですが swiperのバージョンを6.0.0から6.2.0に変更したら治りました。 事象が起きているときの状態 npmインストールしていたパッケージ \u0026#34;devDependencies\u0026#34;: { \u0026#34;css-loader\u0026#34;: \u0026#34;^4.3.0\u0026#34;, \u0026#34;mini-css-extract-plugin\u0026#34;: \u0026#34;^0.11.2\u0026#34;, \u0026#34;sass\u0026#34;: \u0026#34;^1.26.10\u0026#34;, \u0026#34;sass-loader\u0026#34;: \u0026#34;^10.0.2\u0026#34;, \u0026#34;style-loader\u0026#34;: \u0026#34;^1.2.1\u0026#34;, \u0026#34;swiper\u0026#34;: \u0026#34;^6.0.0\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;^4.44.1\u0026#34;, \u0026#34;webpack-cli\u0026#34;: \u0026#34;^3.3.12\u0026#34; } webpack.config.js const path = require(\u0026#39;path\u0026#39;); const MiniCssExtractPlugin = require(\u0026#39;mini-css-extract-plugin\u0026#39;); module.exports = { entry: path.resolve(__dirname, \u0026#39;src/js/index.js\u0026#39;), output: { path: path.resolve(__dirname, \u0026#34;dist\u0026#34;), filename: \u0026#39;main.js\u0026#39;, }, module: { rules: [ { test: /\\.js$/, include: [path.resolve(__dirname, \u0026#39;src/js\u0026#39;)] }, { test: /\\.(sa|sc|c)ss$/, use: [ MiniCssExtractPlugin.loader, \u0026#39;css-loader\u0026#39;, \u0026#39;sass-loader\u0026#39; ] } ] }, plugins: [ new MiniCssExtractPlugin({ filename: \u0026#39;style.css\u0026#39;, }) ] } /src/js/index.js import \u0026#34;swiper/swiper.scss\u0026#34;; ～以下Javascriptの処理～"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/09/04/windows-terminal-install/",
    title: "Windows Terminalの導入",
    date: "2020-09-04T00:00:00Z",
    body: "Windows Terminalの導入 Windows TerminalでUbuntuを使えるようにします。 Windows Terminalのインストール Microsoft Storeで「Windows Terminal」を検索してインストールするだけです。 Windows TerminalのデフォルトをUbuntuにする Windows Terminalを起動して設定を開きます。 settings.jsonが開くので、「profiles \u0026gt; list」の中からUbuntuのguidを探します。 { \u0026#34;guid\u0026#34;: \u0026#34;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\u0026#34;, ←これ \u0026#34;hidden\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;Ubuntu-20.04\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;Windows.Terminal.Wsl\u0026#34; }, 見つけたguidを「defaultProfile」に設定します。 \u0026#34;defaultProfile\u0026#34;: \u0026#34;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\u0026#34;, ←ここに入れる 起動時のディレクトリを指定する Windows Terminalを起動した際に開く場所はWindows側のホームディレクトリになっています。 Ubuntuを使う際はUbuntu側のホームディレクトリから始まって欲しいので変更します。 settings.jsonのUbuntuの設定にstartingDirectoryを追加します。 Ubuntu内の/home/hogeuserからスタートしたい場合は以下のように設定します。 { \u0026#34;guid\u0026#34;: \u0026#34;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\u0026#34;, \u0026#34;hidden\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;Ubuntu-20.04\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;Windows.Terminal.Wsl\u0026#34;, \u0026#34;startingDirectory\u0026#34;: \u0026#34;//wsl$/Ubuntu-20.04/home/hogeuser\u0026#34; ←これを追加 }, フォントを指定する Ubuntuのシェルはfishを利用しているのですが、fishのテーマによってはフォントを変更する必要があります。 fishの導入方法についてはこちら settings.jsonのUbuntuの設定にfontFaceを追加します。 もしフォントサイズを変更したい場合はfontSizeも追加してください。 フォントを「Cascadia Code PL」に変更し、フォントサイズを11にする場合は以下のようになります。 { \u0026#34;guid\u0026#34;: \u0026#34;{07b52e3e-de2c-5db4-bd2d-ba144ed6c273}\u0026#34;, \u0026#34;hidden\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;Ubuntu-20.04\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;Windows.Terminal.Wsl\u0026#34;, \u0026#34;startingDirectory\u0026#34;: \u0026#34;//wsl$/Ubuntu-20.04/home/hogeuser\u0026#34;, \u0026#34;fontFace\u0026#34;: \u0026#34;Cascadia Code PL\u0026#34;, ←これを追加 \u0026#34;fontSize\u0026#34;: 11 ←これを追加 },"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/09/04/win10-keymap-change/",
    title: "Windows10でキー配置を変更する",
    date: "2020-09-04T00:00:00Z",
    body: "Windows10でキー配置を変更する 参考 Ctrl2Capツールで［Ctrl］と［CapsLock］キーを入れ替える（Windows編）|@IT Windowsのキー配置を変更してMac風にします。 CapsLockをCtrlにする CapsLockは使うことがないのでCtrlキーに割り当ててしまいます。 MacのCtrlキーと同じ位置にCapsLockはあるので、Macを使っていた人にも操作しやすくなると思います。 変更するには『Ctrl2Cap』を使います。 https://docs.microsoft.com/ja-jp/sysinternals/downloads/ctrl2cap インストール手順は以下の通りです。 ①ctrl2capのZIPファイルをダウンロードして解凍する。 ②PowerShellを管理者権限で起動し、①の解凍したフォルダに移動する。 ③.\\ctrl2cap.exe /installを実行する。 これで『Ctrl2cap successfully installed.』と表示されたらインストール成功です。 PCを再起動するとCapsLockがCtrlとして使えるようになります。 変換/無変換キーでIMEをON/OFFする Mac風にスペースバーの左右のキーでIMEをON/OFFできるようにします。 設定手順は以下の通りです。 ①設定を開く。 ②設定の検索で『日本語IMEの設定』を検索して開く。 ③『キーとタッチのカスタマイズ』を開く ④『各キーに好みの機能を割り当てる』をオンにする。 ⑤無変換キーを「IME-オフ」に割り当てる。 ⑥変換キーを「IME-オン」に割り当てる。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/09/04/win10-initial-setting/",
    title: "Windows10で開発PCをセッティングする。",
    date: "2020-09-04T00:00:00Z",
    body: "Windows10で開発PCをセッティングする。 Windows 10のWSL2やWindows Terminalが良い感じだったので 開発環境を整えてみました。 大きな作業は以下を参照 Windowsのキー配置変更 WSL2でUbuntuを使う Windows Terminalの導入 ウィンドウが勝手に最大化しないようにする Win10はデフォルトだとウィンドウを画面の端に持っていくと勝手に最大化されてしまいます。 邪魔になることが多い機能なので無効にしてしまいます。 『設定 \u0026gt; システム \u0026gt; マルチタスク』を開き、「ウィンドウのスナップ」をオフにすれば完了です。 Dockerのインストール 以下よりDocker Desktop for Windowsをダウンロードします。 https://hub.docker.com/editions/community/docker-ce-desktop-windows ダウンロードしてきたインストーラーを実行してインストールします。 インストール時に「Enable WSL 2 Windows Features」のチェックボックスがあるので、忘れずにチェックするようにしてください。 これでUbuntu側でもdockerコマンドが使えるようになっています。 もしWSL2より先にDockerをインストールしてしまった場合は「Settings \u0026gt; Resources \u0026gt; WSL INTEGRATION」からWLS上のDockerを有効化できます。 Visual Studio Codeのインストール 以下よりダウンロードしてインストールします。 https://azure.microsoft.com/ja-jp/products/visual-studio-code/ Ubuntu側でgitコマンドを使ってソースコードを管理している場合はWindowsからUbuntuに接続する必要があります。 そのためには『Remote - WSL』という拡張子のインストールをインストールします。 （おそらくVisual Studio Codeの初回起動時に推奨される） MySQL Workbenchのインストール MySQL Workbenchをインストールするには「Visual C++ 2019」が必要です。 インストールされていない場合は以下よりダウンロードしてインストールしておきます。 https://support.microsoft.com/ja-jp/help/2977003/the-latest-supported-visual-c-downloads 以下よりMySQL Workbenchをダウンロードしてインストールします。 https://dev.mysql.com/downloads/workbench/"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/09/04/win10-wsl2-ubuntu/",
    title: "WSL2でUbuntuを使えるようにする",
    date: "2020-09-04T00:00:00Z",
    body: "WSL2でUbuntuを使えるようにする 参考 Windows 10 用 Windows Subsystem for Linux のインストール ガイド WSL の場所を変更する|パソコン鳥のブログ Windows 10のWLS2でUbuntu20.04を使えるようにします。 Windows 10のアップデート WSL2を使うにはWindowsのバージョン 1903以降、ビルド 18362以上である必要があります。 バージョンを確認するには「Winキー＋Rキー」で『ファイル名を指定して実行』を開き、 名前にwinverを入力して「OK」をクリックします。 Windows Updateで最新化できれば良いですが、更新が来ない場合もあるので その場合は更新アシスタントを利用します。 https://www.microsoft.com/ja-jp/software-download/windows10 WSL2をインストールする 管理者権限でPowerShellを開いて、以下を実行します。 dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart ここまで実行したら一度再起動します。 再起動後、再度PowerShellを管理者権限で開き、以下コマンドを実行します。 wsl --set-default-version 2 もしカーネルの更新が必要という旨のメッセージが表示された場合は 以下を参考にカーネルの更新をしてから再実行します。 https://docs.microsoft.com/ja-jp/windows/wsl/wsl2-kernel Ubuntuのインストール Microsoft Storeで「Ubuntu」と検索し、Ubuntu20.04をインストールします。 その後、Ubuntuを起動すると初期設定が走るので、アカウント設定等をします。 デフォルトではaptでの取得元が海外になっているので、以下コマンドで日本に変更します。 sudo sed -i -e \u0026#39;s|archive.ubuntu.com|ftp.riken.go.jp/Linux|g\u0026#39; /etc/apt/sources.list ここまででWSL2 でUbuntuを使えるようになりました。 ですが、デフォルトではCドライブにUbuntu用の領域をとってしまうので、Dドライブに移したいと思います。 WSLの場所を移動させるツール『LxRunOffline』を利用します。 まず、GitHubより圧縮ファイルをダウンロードして解凍しておきます。(LxRunOffline-vX.X.X-mingw.zip) https://github.com/DDoSolitary/LxRunOffline/releases 次にWSLを配置するフォルダを作成しておきます。 本例ではD:\\wslを作ります。 PowerShellを管理者権限で開き、以下コマンドで先ほど作成したフォルダに権限を付与します。 icacls D:\\wsl /grant $env:USERNAME\u0026#34;:(OI)(CI)(F)\u0026#34; そのままPowerShellでLxRunOfflineを解凍したフォルダに移動し、以下コマンドを実行します。 net stop LxssManager .\\LxRunOffline.exe move -n Ubuntu-20.04 -d d:\\wsl\\Ubuntu-20.04 net start LxssManager これでUbuntuのインストールは完了です。 もしアンインストールする場合は以下のコマンドを実行してください。 wsl -t Ubuntu-20.04 .\\LxRunOffline.exe ui -n Ubuntu-20.04 ※補足 WSL2配置の場所を移動させなくても/mnt/d/にDドライブがマウントされているのでDドライブを利用することは可能です。 ただし/mnt/配下のWindowsディレクトリへのアクセスは時間がかかるので、WSLの配置場所を変更する方がおススメです。 Ubuntu内で起動しているサーバーにWindows側からローカルとしてアクセスする Ubuntu内のDocker等でWEBサーバーを立ち上げた際に、Windows側のブラウザからhttp://localhostでアクセスできない場合があります。 ローカルとして繋ぐために、Windows側のホームディレクト(C:\\Users\\hogeUser)直下に.wslconfigというファイルを作成し、 以下を記載しておきます。 [wsl2] localhostForwarding=True また、Windowsの高速スタートアップが有効になっていると上手くいかない事があるので無効にしておきます。 「設定＞システム＞電源とスリープ＞電源の追加設定＞電源ボタンの動作を選択する」を開き、 『高速スタートアップを有効にする（推奨）』のチェックボックスを外してください。 Ubuntuから参照するDNSサーバーを変更する WSLでは名前解決に使うDNSサーバーが自動で設定され、/etc/resolv.confに記載されます。 このファイルは修正してもデフォルトではWSL起動の度に書き換えられてしまいます。 このままだと稀にUbuntu内で名前解決ができなることがあるので、 resolv.confが自動更新されないようにして、向け先をGoogleのDNSサーバー「8.8.8.8」に変更します。 まず、自動更新を止めるために/etc/wsl.confに以下を追記します。（ファイルがなければ新規作成） [network] generateResolvConf = false ※この内容は自動生成されているresolv.confにも記載されています。 この状態でWSLを再起動すると設定が反映されます。 PCを再起動するか、Windows側のPowershellなどでwsl --shutdownを実行してください。 WSLに再度アクセスして/etc/resolv.confにDNS設定をします。 もし/etc/resolv.confが/run/～へのリンボリックリンクになっている場合はunlinkコマンドでリンク解除しておきます。 /etc/resolv.confを編集し、nameserverを以下のように編集します。 nameserver 8.8.8.8 これでWSL(Ubuntu)での名前解決に使うDNSサーバーをGoogleのDNSサーバーに変更できました。 Ubuntu側からWindows側のフォルダで権限変更できるようにする Ubuntu側では/mnt/内にWindows側のフォルダがあるので参照はできますが、 デフォルトではchmodでの権限変更ができません。 もしWindows側のフォルダ・ファイルの権限変更をしたい場合は/etc/wsl.confを作成して以下を記載します。 [automount] options = \u0026#34;metadata\u0026#34; ※補足 Ubuntu側からWindows側のファイルへアクセスするのは激重なので、 基本的にはUbuntuからWindows側のファイルを扱うことは勧めません。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/08/26/ec2-ecs-to-csv/",
    title: "EC2とECSのレコードを全てCSVに出力する",
    date: "2020-08-26T00:00:00Z",
    body: "EC2とECSのレコードを全てCSVに出力する 環境 Linux Mint 20 Ulyana aws-cli/2.0.41 jq 1.6 前回の続きです。 前回AWS Route53のレコードをCSVに出力するシェルスクリプトを作ったので、同様にEC2・ECSの情報を出力するシェルスクリプトを作ってみます。 EC2 以下のようなシェルです。 region一覧は固定で持っているので、今後増減があった場合は手動で修正が必要です。 #!/bin/bash #region一覧 regionList=( us-east-2 us-east-1 us-west-1 us-west-2 af-south-1 ap-east-1 ap-south-1 ap-northeast-3 ap-northeast-2 ap-southeast-1 ap-southeast-2 ap-northeast-1 ca-central-1 cn-north-1 cn-northwest-1 eu-central-1 eu-west-1 eu-west-2 eu-south-1 eu-west-3 eu-north-1 me-south-1 sa-east-1 ) echo \u0026#39;\u0026#34;AvailabilityZone\u0026#34;, \u0026#34;Name\u0026#34;, \u0026#34;PublicDnsName\u0026#34;,\u0026#34;PublicIpAddress\u0026#34;,\u0026#34;PrivateDnsName\u0026#34;,\u0026#34;PrivateIpAddress\u0026#34;\u0026#39; for region in ${regionList[@]}; do EC2List=`aws ec2 describe-instances --region ${region} --output json --query \u0026#39;Reservations[].Instances[]\u0026#39;` EC2ListLen=`echo $EC2List | jq length` for i in $( seq 0 $(($EC2ListLen - 1)) ); do EC2=`echo $EC2List | jq .[$i]` avZone=`echo $EC2 | jq -c -r \u0026#39;.Placement.AvailabilityZone // \u0026#34;\u0026#34;\u0026#39;` priDns=`echo $EC2 | jq -c -r \u0026#39;.PrivateDnsName // \u0026#34;\u0026#34;\u0026#39;` priIp=`echo $EC2 | jq -c -r \u0026#39;.PrivateIpAddress // \u0026#34;\u0026#34;\u0026#39;` pubDns=`echo $EC2 | jq -c -r \u0026#39;.PublicDnsName // \u0026#34;\u0026#34;\u0026#39;` pubIp=`echo $EC2 | jq -c -r \u0026#39;.PublicIpAddress // \u0026#34;\u0026#34;\u0026#39;` Tags=`echo $EC2 | jq -c -r \u0026#39;.Tags\u0026#39;` TagsLen=`echo $Tags | jq length` name=\u0026#39;None\u0026#39; for j in $( seq 0 $(($TagsLen - 1)) ); do tag=`echo $Tags | jq .[$j]` key=`echo $tag | jq -c -r \u0026#39;.Key\u0026#39;` if [ \u0026#34;$key\u0026#34; = \u0026#34;Name\u0026#34; ]; then name=`echo $tag | jq -c -r \u0026#39;.Value // \u0026#34;\u0026#34;\u0026#39;` break fi done echo \u0026#34;\\\u0026#34;${avZone}\\\u0026#34;,\\\u0026#34;${name}\\\u0026#34;,\\\u0026#34;${pubDns}\\\u0026#34;,\\\u0026#34;${pubIp}\\\u0026#34;,\\\u0026#34;${priDns}\\\u0026#34;,\\\u0026#34;${priIp}\\\u0026#34;\u0026#34; done done このシェルではEC2のドメインやIPアドレスを取得していますが、そこは必要に情報に置き換えても良いです。 ECS 以下のようなシェルです。 #!/bin/bash echo \u0026#39;\u0026#34;ClusterArns\u0026#34;, \u0026#34;IPAddress\u0026#34;\u0026#39; ClusterArnsList=`aws ecs list-clusters --output json --query \u0026#39;clusterArns\u0026#39;` ClusterArnsListLen=`echo $ClusterArnsList | jq length` for i in $( seq 0 $(($ClusterArnsListLen - 1)) ); do clusterArns=`echo $ClusterArnsList | jq .[$i]` clusterArns=${clusterArns//\\\u0026#34;/} TaskArnsList=`aws ecs list-tasks --cluster $clusterArns --output json --query \u0026#39;taskArns\u0026#39;` TaskArnsListLen=`echo $TaskArnsList | jq length` for j in $( seq 0 $(($TaskArnsListLen - 1)) ); do taskArns=`echo $TaskArnsList | jq .[$j]` taskArns=${taskArns//\\\u0026#34;/} Tasks=`aws ecs describe-tasks --cluster ${clusterArns} --tasks ${taskArns} --output json --query \u0026#39;tasks\u0026#39;` TasksLen=`echo $Tasks | jq length` for k in $( seq 0 $(($TasksLen - 1)) ); do task=`echo $Tasks | jq .[$k]` TaskDetails=`echo $task | jq -c -r \u0026#39;.attachments[].details\u0026#39;` TaskDetailsLen=`echo $TaskDetails | jq length` for l in $( seq 0 $(($TaskDetailsLen - 1)) ); do taskDetail=`echo $TaskDetails | jq .[$l]` name=`echo $taskDetail | jq -c -r \u0026#39;.name\u0026#39;` if [ \u0026#34;$name\u0026#34; = \u0026#34;privateIPv4Address\u0026#34; ]; then ipaddress=`echo $taskDetail | jq -c -r \u0026#39;.value\u0026#39;` break; fi done echo \u0026#34;\\\u0026#34;${clusterArns}\\\u0026#34;,\\\u0026#34;${ipaddress}\\\u0026#34;\u0026#34; done done done こちらもECSのIPアドレスを出力しています。必要な情報に差し替えてください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/08/21/route53-to-csv/",
    title: "Route53のレコードを全てCSVに出力する",
    date: "2020-08-21T00:00:00Z",
    body: "Route53のレコードを全てCSVに出力する 環境 Linux Mint 20 Ulyana aws-cli/2.0.41 jq 1.6 Route53に大量のホストゾーンとレコードが登録されているアカウントで、Route53に登録されている内容を一覧化する必要があったので対応しました。 ワンライナーで実現しようと思ったのですが、希望した形にできなかったのでシェルスクリプトにしました。 必要なソフトのインストール AWS CLI AWS CLIが無い場合はインストールしておきます。 https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2.html AWS CLIの初期設定はこちら https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/cli-configure-quickstart.html jq jqはjsonを整形するツールです。 Linux Mintであれば以下コマンドでインストールできます。 apt install jq Route53の内容を取得するスクリプト作成 AWS CLIでRoute53の内容をjson形式で取得し、それをjqで整形しながらCSVに出力するシェルスクリプトを作成します。 スクリプトの内容は以下の通りです。 #!/bin/bash # CSVのヘッダ出力 echo \u0026#39;\u0026#34;HostedZone\u0026#34;,\u0026#34;DomainName\u0026#34;,\u0026#34;DomainType\u0026#34;,\u0026#34;TTL\u0026#34;,\u0026#34;Value\u0026#34;\u0026#39; # ホストゾーン一覧を取得 HostedZones=`aws route53 list-hosted-zones --output json --query \u0026#39;HostedZones\u0026#39;` HostedZonesLen=`echo $HostedZones | jq length` for i in $( seq 0 $(($HostedZonesLen - 1)) ); do Zone=`echo $HostedZones | jq .[$i]` ZoneId=`echo $Zone | jq -c -r \u0026#39;.Id\u0026#39;` ZoneName=`echo $Zone | jq -c -r \u0026#39;.Name\u0026#39;` # ホストゾーン内のレコード一覧を取得 Records=`aws route53 list-resource-record-sets --hosted-zone-id $ZoneId --output json --query \u0026#39;ResourceRecordSets\u0026#39;` RecordsLen=`echo $Records | jq length` for i in $( seq 0 $(($RecordsLen - 1)) ); do record=`echo $Records | jq .[$i]` name=`echo $record | jq -c -r \u0026#39;.Name\u0026#39;` type=`echo $record | jq -c -r \u0026#39;.Type\u0026#39;` ttl=`echo $record | jq -c -r \u0026#39;.TTL\u0026#39;` data=`echo $record | jq -c -r \u0026#39;.AliasTarget.DNSName // \u0026#34;\u0026#34;\u0026#39;` # AliasTargetのDNSNameが設定されていない場合はResourceRecordsのValuesを取得する if [ -z \u0026#34;$data\u0026#34; ]; then data=`echo $record | jq -c -r \u0026#39;.ResourceRecords[].Value\u0026#39;` fi # dataにはダブルクォートが含まれる可能性があるのでエスケープしておく data=${data//\\\u0026#34;/\\\u0026#34;\\\u0026#34;} # CSVのボディ出力 echo \u0026#34;\\\u0026#34;${ZoneName}\\\u0026#34;,\\\u0026#34;${name}\\\u0026#34;,\\\u0026#34;${type}\\\u0026#34;,\\\u0026#34;${ttl}\\\u0026#34;,\\\u0026#34;${data}\\\u0026#34;\u0026#34; done done 簡単な説明 AWS CLIでホストゾーンの情報をjson形式で取得します。 aws route53 list-hosted-zones --output json --query \u0026#39;HostedZones[]\u0026#39; 取得したホストゾーン情報を1件ずつループを回します。 以下コマンドでホストゾーンのレコードをjson形式で取得します。 aws route53 list-resource-record-sets --hosted-zone-id $ZoneId --output json --query \u0026#39;ResourceRecordSets\u0026#39; そして要所要所でjqコマンドでjsonから必要な情報を抽出しています。 補足 AWS CLIのqueryオプションはjqコマンドで抽出するのと同じ効果があります。 つまり、以下のコマンドはどちらも同じ結果となります。 aws route53 list-resource-record-sets --hosted-zone-id $ZoneId --output json --query \u0026#39;ResourceRecordSets\u0026#39; aws route53 list-resource-record-sets --hosted-zone-id $ZoneId --output json | jq \u0026#39;.ResourceRecordSets\u0026#39;"
  },
  {
    url: "https://blog2.logical-dice.com/tags/linux/",
    title: "Linux",
    date: "2020-08-05T00:00:00Z",
    body: "Linux"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/08/05/replace-with-find-sed/",
    title: "改行コードを一括で置換する",
    date: "2020-08-05T00:00:00Z",
    body: "改行コードを一括で置換する 定期的に使うのでメモ。 Linuxサーバー上で対象ディレクトリ配下のファイルの文字コードを一括で置換するコマンドです。 以下コマンドで現ディレクトリ配下にあるphpファイルに対してCRLF(\\r\\n)をLF(\\n)に置換します。 sudo find . -name \u0026#34;*.php\u0026#34; -type f -exec sed -i -e \u0026#34;s/\\r//g\u0026#34; {} \\; ※sudoは必要に応じて。全対象ファイルの編集権限があるなら不要です。 一応解説 CRLFはWindows等でよく使われる改行コードで、LFやLinuxやUnix等でよく使われる改行コードです。 まず、findコマンドの構成は以下の通り。 find {検索対象ディレクトリ} -name {検索対象の名前} -type {検索対象のタイプ} -exec {検索対象に対して実施するコマンド} -type fでファイルを対象にしています。 -name \u0026quot;*.php\u0026quot;でphpファイルを指定しているのでタイプは指定する必要が内容に思えますが、 やろうと思えばhoge.phpというディレクトリも作れない訳ではないので、念の為指定しています。 そして、-execオプションでsedコマンドを指定しています。 sedコマンドの構成は以下の通り。 sed -i -e \u0026#34;s/{置換対象の文字列}/{置換後の文字列}/g\u0026#34; 対象ファイル CRLFをLFに置換すると言いましたが、このコマンドでは正確にはCR(\\r)を消しているだけです。 なので、CRLFはLFになりますし、もしCRがあれば消滅します。 (古いMacintoshではCRを改行コードとして扱っていたらしい。とりあえず現在では消滅してくれて良い。) findの-execで実行する場合、{}に検索結果のファイル名が入ります。 そして実行したいコマンドの最後は\\で閉じます。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/bookmarklet/",
    title: "Bookmarklet",
    date: "2020-07-16T00:00:00Z",
    body: "Bookmarklet"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/07/16/bookmarklet-kimetsu/",
    title: "GitHubの草を炭治郎柄にする",
    date: "2020-07-16T00:00:00Z",
    body: "GitHubの草を炭治郎柄にする 私は鬼滅の刃自体は見たことが無いのですが、それでも主人公の炭治郎の着物はよく目にします。 あの柄を見ると、ちょっとGitHubの草っぽいなと思ってました。 というわけで、GitHubの草を炭治郎の着物柄にしてみます。 Bookmarkletの説明や作り方についてはこちらを参照。 Bookmarklet(ブックマークレット)の作り方 Bookmarklet作成 まず、GitHubの草部分の色を変えるスクリプトを書いてみます。 // GitHubの草部分の要素を取得 var kusaElmList = document.getElementsByClassName(\u0026#39;day\u0026#39;) for (var i=0; i\u0026lt;kusaElmList.length; i++){ var kusa = kusaElmList[i] if (i % 2 == 0){ // 偶数マスは緑 kusa.setAttribute(\u0026#39;fill\u0026#39;,\u0026#39;#2A7\u0026#39;) }else{ // 奇数マスは黒 kusa.setAttribute(\u0026#39;fill\u0026#39;,\u0026#39;#000\u0026#39;) } } これをBookmarkletにします。 javascript:for(var t=document.getElementsByClassName(\u0026#34;day\u0026#34;),e=0;e\u0026lt;t.length;e++){var l=t[e];e%2==0?l.setAttribute(\u0026#34;fill\u0026#34;,\u0026#34;#2A7\u0026#34;):l.setAttribute(\u0026#34;fill\u0026#34;,\u0026#34;#000\u0026#34;)} 実行結果はこちら。 色はそれっぽくなりましたが、まだ隙間が気になります。 なので、隙間も埋めるよう修正します。 // GitHubの草部分の要素を取得 var kusaElmList = document.getElementsByClassName(\u0026#39;day\u0026#39;); for (var i=0; i\u0026lt;kusaElmList.length; i++){ var kusa = kusaElmList[i]; // 草の大きさと形を整える kusa.setAttribute(\u0026#39;width\u0026#39;,\u0026#39;15\u0026#39;); kusa.setAttribute(\u0026#39;height\u0026#39;,\u0026#39;15\u0026#39;); kusa.style.rx=0; kusa.style.ry=0; if (i % 2 == 0){ // 偶数マスは緑 kusa.setAttribute(\u0026#39;fill\u0026#39;,\u0026#39;#2A7\u0026#39;) }else{ // 奇数マスは黒 kusa.setAttribute(\u0026#39;fill\u0026#39;,\u0026#39;#000\u0026#39;) } } これをBookmarkletにします。 javascript:for(var t=document.getElementsByClassName(\u0026#34;day\u0026#34;),e=0;e\u0026lt;t.length;e++){var l=t[e];l.setAttribute(\u0026#34;width\u0026#34;,\u0026#34;15\u0026#34;),l.setAttribute(\u0026#34;height\u0026#34;,\u0026#34;15\u0026#34;),l.style.rx=0,e%2==(l.style.ry=0)?l.setAttribute(\u0026#34;fill\u0026#34;,\u0026#34;#2A7\u0026#34;):l.setAttribute(\u0026#34;fill\u0026#34;,\u0026#34;#000\u0026#34;)} 実行結果はこちら。 いい感じになりました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/07/15/howto-make-bookmarklet/",
    title: "Bookmarklet(ブックマークレット)の作り方",
    date: "2020-07-15T00:00:00Z",
    body: "Bookmarklet(ブックマークレット)の作り方 Bookmarklet(ブックマークレット)とは 詳細はこちら ウィキペディア 簡単に言えばブラウザのブックマークにURL代わりにjavascriptを入力して保存しておいて。 そのjavascriptを実行したい時に該当のブックマークをクリックするというものです。 たとえばブックマークにjavascript:alert('hoge');と登録しておけば、 そのブックマークをクリックしたら\u0026lt;a href=\u0026quot;javascript:alert('hoge');\u0026quot;\u0026gt;hoge\u0026lt;/a\u0026gt;をクリックした時と同じ事が起きます。 作り方は色々ありますが、uglify-jsを使って作成する方法を紹介します。 javascriptの作成 まず、適当にjsファイルを作って適当に実行したいjavascriptのコードを書きます。 今回は例としてページ内のdivタグの数を数えるスクリプトを作ります。 以下のコードをsample.jsというファイルに記載しておきます。 var divElms = document.getElementsByTagName(\u0026#39;div\u0026#39;); alert(\u0026#39;divの数は\u0026#39; + divElms.length + \u0026#39;です！\u0026#39;); uglify-jsによるjavascriptの圧縮 uglify-jsを使って作成したsample.jsを圧縮します。 まず、uglify-jsコマンドを使えるようにインストールします。 npmをインストールしていない人は先にインストールしておいてください。 $ npm install -g uglify-js # 環境に応じて必要であればsudoを付けてください 次にjavascriptの圧縮をします。 以下のコマンドでjavascriptの圧縮ができます。 ※各オプションについては公式ページ参照 $ uglifyjs -c -m --toplevel -- sample.js 実行結果として以下のように圧縮されたスクリプトが出力されます。 var e=document.getElementsByTagName(\u0026#34;div\u0026#34;);alert(\u0026#34;divの数は\u0026#34;+e.length+\u0026#34;です！\u0026#34;); この圧縮スクリプトの頭にjavascript:を付けたものがBookmarkletになります。 ブラウザへのBookmarklet登録 ブラウザのブックマークに新規ページを追加し、URL欄にjavascript:｛生成した圧縮スクリプト｝を登録します。 Chromeであれば以下のようになります。 （名前は自分で分かるものであれば何でもいいです） これで任意のページを開いている時に、登録したブックマークをクリックすると処理が走ります。 Googleで実行するとこのようになります。 これでBookmarkletの作成は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/07/12/vpc-lambda/",
    title: "AWS lambdaからDBアクセスも外部アクセスもできるようにする。",
    date: "2020-07-12T00:00:00Z",
    body: "AWS lambdaからDBアクセスも外部アクセスもできるようにする。 AWS lambdaからAWS RDSへアクセスさせるためにはRDSへアクセスできるVPCのサブネットにlambdaを追加します。 しかし、VPC内のlambdaは何もしないと外部アクセスができなくなります。 lambdaがRDSと同時に外部システムのAPIなどを利用する際に困りますので、 VPC内のlambdaから外部アクセスができるように設定します。 VPC設定 VPC内のlambdaから外部アクセスをさせる設定方法は、実はAWS公式で説明されています。 基本的にはこの手順を実行すれば良いだけです。 https://aws.amazon.com/jp/premiumsupport/knowledge-center/internet-access-lambda-function/ 手順詳細は公式ページを見ればよいので、ここではざっくりと流れを書いておきます。 ①〜②はすでにあれば既存の使い回しでも良いです。 ①（なければ）RDSにアクセスできるVPCを作る。 ②（なければ）インターネットゲートウェイを作成して①のVPCにアタッチする。 ③RDSにアクセスできるVPCにサブネットを2つ作る。(public用とprivate用) ④NATゲートウェイを新規作成し、②で作成したpublic用サブネットを紐づける。 ⑤ルートテーブルを新規作成し、public用サブネットを紐付ける。 作成したら以下のルートを追加する。 項目 設定値 宛先 0.0.0.0/0 ターゲット ③のインターネットゲートウェイ ⑥ルートテーブルをもう１つ新規作成し、private用サブネットを紐付ける。 作成したら以下のルートを追加する。 項目 設定値 宛先 0.0.0.0/0 ターゲット ④のNATゲートウェイ VPC設定はここまでです。 ここで作成したprivate用サブネットの方にlambdaを追加すればRDSと外部の両方にアクセスできるようになります。 lambda設定(Serverless Framework) 今回はServerless Frameworkを使ってlambdaを構築します。 Serverless Frameworkの基本的な構築については前回の記事を参照してください。 前回の記事で作成したserverless.ymlのproviderにvpc設定を追加します。 provider: 〜省略〜 vpc: subnetIds: - 今回作成したprivateサブネットのID(subnet-0123456abcde) 【おまけ】 上記VPC設定では触れていませんが、もしlambdaにセキュリティグループを設定させるなら以下のようにします。 provider: 〜省略〜 vpc: subnetIds: - 今回作成したprivateサブネットのID(subnet-0123456abcde) securityGroupIds: - 設定するセキュリティグループID(sg-012345abcde)"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/12/linux-mint-trackpoint/",
    title: "Linux MintでThinkPadのトラックポイントを調整する",
    date: "2020-07-06T00:00:00Z",
    body: "Linux MintでThinkPadのトラックポイントを調整する 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) ThinkPad T480 ■参考 Ubuntu17.10でtrackpointの速度を調整する トラックポイントの速度調整は『コントロールセンター＞マウスの設定』からもできるのですが、 設定できる幅に限界があり、もう少し感度を上げたかったので他の方法で調整します。 ■最適な値の探し出し 『感度』と『速度』をいじりながら最適な数値を探します。 私の場合はsensitivityは180、speedは60にすると丁度良かったです。 $ sudo -i # cd /sys/devices/platform/i8042/serio1/serio2 # vim sensitivity // 感度 # vim speed // 速さ ■設定値の永続化 さきほど修正していたファイルは再起動時に戻ってしまうので、 別ファイルにルールを記載しておきます。 # vim /etc/udev/rules.d/10-trackpoint.rules 記載内容は以下の通りです。 speedとsensitivityは最適な値を入れてください。 KERNEL==\u0026#34;serio2\u0026#34;, SUBSYSTEM==\u0026#34;serio\u0026#34;, DRIVERS==\u0026#34;psmouse\u0026#34;, ATTR{sensitivity}:=\u0026#34;180\u0026#34;, ATTR{speed}:=\u0026#34;60\u0026#34; これで設定完了です。 (おまけ)中央クリックの貼り付けを無効化する デフォルトだとThinkPadの中央クリック(マウスでいうホイールクリック)で貼り付けがされますが、 スクロール中にいきなり貼り付けが実行されたりして不便なので無効化します。 ※2020年7月追記 中央クリックでの貼付けを防ぐだけであれば設定から変更できました。 コントロールセンター \u0026gt; マウス \u0026gt; Enable middlemouse paste まずデバイス名を取得します。 以下コマンドを実行すると「TPPS/2 IBM TrackPoint」であることが分かります。 $ xinput --list | grep TrackPoint ⎜ ↳ TPPS/2 IBM TrackPoint id=14\t[slave pointer (2)] デバイスのマッピングを調べます。 以下コマンドを実行すると中央クリックはボタンマッピングの「2」であることが分かります。 $ xinput get-button-map \u0026#34;TPPS/2 IBM TrackPoint\u0026#34; 1 2 3 4 5 6 7 $ xinput --list \u0026#34;TPPS/2 IBM TrackPoint\u0026#34; | grep \u0026#34;Button labels\u0026#34; Button labels: \u0026#34;Button Left\u0026#34; \u0026#34;Button Middle\u0026#34; \u0026#34;Button Right\u0026#34; \u0026#34;Button Wheel Up\u0026#34; \u0026#34;Button Wheel Down\u0026#34; \u0026#34;Button Horiz Wheel Left\u0026#34; \u0026#34;Button Horiz Wheel Right\u0026#34; 中央クリックを無効化します。 ボタンマッピングにて無効化にしたいボタンの数字を「0」に置き換えます。 xinput set-button-map \u0026#34;TPPS/2 IBM TrackPoint\u0026#34; 1 0 3 4 5 6 7 永続化するには上記コマンドを~/.profileに追記するだけです。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/linuxmint/",
    title: "LinuxMint",
    date: "2020-07-06T00:00:00Z",
    body: "LinuxMint"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/10/linux-mint-initialize/",
    title: "Linux Mintの初期設定",
    date: "2020-07-01T00:00:00Z",
    body: "Linux Mintの初期設定 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) ThinkPad T480 ※2020年7月追記 Linux Mint 20 Ulyanaについても追記しました。 日本語入力の設定 『コントロールセンター＞入力方法』を開き、日本語の設定をします。 基本的に上図の通り書かれている手順を実施します。 言語パッケージをインストールする(ここでinstallボタン押すだけ) ウィンドウ上部のInput MethodをFctixにする 一旦ログアウトする ※2020年7月追記 Linux Mint 20では最初からFctixが使えるので、ログアウトは不要です。 『コントロールセンター＞Fctix設定』を開き、入力メソッドをMozcだけにします。 不要な入力メソッドを「-」ボタンをクリックして消すだけです。 上図のようになればOKです。 言語設定 ※2020年7月追記 Linux Mint 20では不要そうです。 『コントロールセンター＞言語』を開き、日本語の設定をします。 「言語」と「地域」を日本にする。（なっているはず） 「言語サポート」にて日本語をインストールする。 「システムロケール」の『システム全体に適用』を実行する。 Chromeなどをインストールした際に日本語になっていない場合はこの手順を再度実施します。 入力モード切り替えをMac風にする キーボードの変換キーでIME有効、無変換キーでIME無効にすると便利なので設定します。 『コントロールセンター＞Mozcの設定』を開き、「キー設定の選択」の編集をクリックします。 するとキー設定ウィンドウが表示されるので設定します。 下図ではひらがなカタカナキーもIME有効化に割り当てています。 キー設定画面の右下の「編集」から設定のインポート/エクスポートもできるので、 再インストール時用に好みの設定をエクスポートしておくと後々楽です。 半角/全角キーとCapsLockキーを使いやすくする IMEの有効/無効切り替えを上記の通り設定すると半角/全角キーは不要になるので、Escに割り当ててしまいます。 また、CapsLockも使わないのでCtrlに割り当ててしまいます。 『コントロールセンター＞キーボード』を開き、 レイアウトタブの「キーボードの型式」のオプションを開きます。 ここで「Caps Lock behavior」は『CapsLock is also a Ctrl』を選択し、 「Japanese keyboard options(または、日本語キーボードオプション)」は『Make Zenkaku Hankaku an additional Esc』を選択します。 ソフトウェアソースを変更する ソフトウェアの取得先はデフォルトだとアメリカになっているので変更します。 『コントロールセンター＞ソフトウェアソース』にてメイン・ベースをクリックすると取得先を選ぶ画面になるのですが、 そこで各取得先との接続速度が表示されるので、早い所を選べば大丈夫です。 基本的にはCDNか日本国内になると思います。 ウィンドウを画面上端にドラッグしても何も起きなくする デフォルトだとウィンドウを画面上端へドラッグすると勝手に最大化します。 これは複数ウィンドウを並べたい時に不便なので無効化します。 『コントロールセンター＞ウィンドウ』を開き、場所タブにある「Enable window tiling」のチェックを外せば無効化されます。 aptで色々インストールする 基本セット（vim派） apt install vim git npm セキュリティソフト apt install clamav clamav-daemon PHP関係（使う人は） apt install php7.2 composer php7.2-bcmath php7.2-common php7.2-gd php7.2-json php7.2-mbstring php7.2-mysql php7.2-xml php7.2-xmlrpc ※2020年7月追記 Linux Mint 20ではデフォルトでphp7.4がインストールできます。 apt install php7.4 composer php7.4-bcmath php7.4-common php7.4-gd php7.4-json php7.4-mbstring php7.4-mysql php7.4-xml php7.4-xmlrpc なお、phpインストール時にapache2も自動でインストールされる場合があり、 自動起動する設定になっている可能性があります。 なので、apache2の自動起動はOFFにしておきましょう。 sudo systemctl disable apache2 Shutter（使う人は） Shutterとは画像に簡単に文字・矢印・モザイク・アイコンなどを挿入できるツールです。 MacのSkitchのようなアプリです。（分かる人には分かる） ※2020/7/1追記 ppa等が変わっていたようなので修正 sudo add-apt-repository ppa:linuxuprising/ppa apt update apt install shutter WEBからダウンロードする WEBからダウンロードしてインストールします。 Visual Studio Code Station ※補足 Visual Studio Codeはソフトウェアマネージャ等でもインストールできますが、 そちらだとなぜか日本語入力ができないのでWEBから取得します。(2019/3/10時点) StationはAppImageという見慣れない拡張子でダウンロードされます。 これはターミナルから実行権限を与えて実行することでインストールできます。 chmod +x browserX-1.39.2-x86_64.AppImage ./browserX-1.39.2-x86_64.AppImage (後で見たときに何か分からなくなりそうなら、Station.AppImageなどにファイル名を変更しても良いです) その他参考ページ ホームディレクトリの英語化 Terminator+fish環境構築 Docker環境構築 ThinkPadのトラックポイント調整 L2TPクライアント設定"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/06/14/oreore-certification/",
    title: "EC2上でオレオレ証明書を作る",
    date: "2020-06-14T00:00:00Z",
    body: "EC2上でオレオレ証明書を作る 環境 Amazon Linux AMI 2017.03 OpenSSL 1.0.2k-fips 26 Jan 2017 最近はLet\u0026rsquo;s Encryptの登場でめっきり出番が少なくなったオレオレ証明書を作ります。 Let\u0026rsquo;s EncryptのSSL証明書の方が信頼度が高くブラウザ側で設定する必要がありません。 インターネットに公開していない環境でもDNSにTXTレコードを追加すればLet\u0026rsquo;s Encryptの証明書を発行できます。 それでもオレオレ証明書を使いたい人向けの手順です。 オレオレ証明書(自己証明書)とは SSL証明書はどこかしらの認証局(CA)に認証して貰って発行されるのですが、 自分でCAを立てて自分で認証したSSL証明書を発行してしまうというものです。 ざっくりとした流れは以下の通りです。 ①オレオレ認証局を立てる ②オレオレ認証局でオレオレ証明書を発行する ③利用ブラウザにオレオレ認証局を認めさせる 以降の手順では「*.hoge.com」というワイルドカードのSSL証明書を作る前提とします。 「*.hoge.com」の部分は作成したいドメインに置き換えてください。 作業ディレクトリ等の準備 EC2にSSHでログインし、以下の通り準備をします。 オレオレ認証局はhogeCAというディレクトリに作成するものとします。 sudo -i cd /etc/pki cp -r CA hogeCA cp tls/openssl.cnf hogeCA/. cd hogeCA touch index.txt echo \u0026#34;00\u0026#34; \u0026gt; serial また、以前は無かったと思うのですが 最近のブラウザはSAN(subjectAltName)というものを設定しておかないと正規の証明書と認めてくれないようです。 SANを設定するためのファイルを作成しておきます。 echo \u0026#34;subjectAltName=DNS:*.hoge.com\u0026#34; \u0026gt; san.ext openssl設定変更 以下の通り、今回利用するopenssl.cnfを修正します。 vim ./openssl.cnf [ CA_default ]セクション修正 作業ディレクトリを変更します。 dir = /etc/pki/hogeCA [ req_distinguished_name ]セクション修正 今回の手順で会社所在地などを複数回聞かれるので、デフォルト値を設定しておきます。 countryName_default = JP stateOrProvinceName_default = Tokyo localityName_default = Toshima-ku 0.organizationName_default = Hoge Co. Ltd. [ usr_cert ]セクション修正 SANを読み込ませるための設定です。 subjectAltName=@alt_names 認証局(CA)構築 CAの秘密鍵作成 CA用の秘密鍵を生成します。 秘密鍵に設定するパスワードを聞かれるので入力してください。 openssl genrsa -aes256 -out ./private/cakey.pem 2048 CAのCSR(証明書発行要求ファイル)作成 CAの証明書を発行するためのCSRを作成します。 先程のcakey.pemに設定したパスワードを聞かれるので入力してください。 その後、色々質問されます。基本的にそのままEnterで大丈夫ですが、「Common Name」は後で判別が付くように自分のCA名を適当に入れてください。 openssl req -new -config ./openssl.cnf -key ./private/cakey.pem -out ./cacert.csr CAの証明書作成 CAの証明書を発行します。 ここでもcakey.pemのパスワードを聞かれるので入力してください。 openssl x509 -days 825 -in ./cacert.csr -req -signkey ./private/cakey.pem -out ./cacert.pem サーバー証明書の発行 実際にサーバーのSSL証明書を発行していきます。 後で分かりやすいように、まずSSL証明書用のファイルを格納するディレクトリを切っておきます。 mkdir hoge_com SSL用の秘密鍵作成 SSL用に秘密鍵を生成します。 今回も秘密鍵に設定するパスワードを聞かれるので入力してください。 （すぐにパスワードは解除するので適当で良いです） openssl genrsa -aes256 -out ./hoge_com/server-with-password.key 2048 生成した秘密鍵からパスワード無し版の秘密鍵を作成します。 さきほどのパスワードを聞かれるので入力してください。 openssl rsa -in ./hoge_com/server-with-password.key -out ./hoge_com/server.key SSL用のCSR作成 SSL証明書を発行するためのCSRを作成します。 色々質問されます。基本的にそのままEnterで大丈夫ですが、「Common Name」は設定するドメイン(今回は*.hoge.com)を入れてください。 openssl req -new -config ./openssl.cnf -key ./hoge_com/server.key -out ./hoge_com/server.csr SSL用の証明書発行 SSL証明書を発行します。 cakey.pemのパスワードを聞かれるので、入力してください。 openssl ca -config ./openssl.cnf -in ./hoge_com/server.csr -out ./hoge_com/server.crt -days 825 -extfile san.ext 上記コマンド後、y/nの質問を2回されるのでどちらもyで回答してください。 Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y WEBサーバー設定 生成したSSL証明書をWEBサーバーに設定します。 Apacheなら以下のようになります。 SSLCertificateFile /etc/pki/hogeCA/hoge_com/server.crt SSLCertificateKeyFile /etc/pki/hogeCA/hoge_com/server.key ブラウザ設定 今回作成したSSL証明書をエラーなく表示させるため、 認証局の証明書(/etc/pki/hogeCA/cacert.pem)をブラウザにインポートしてください。 Chromeであれば、以下の手順で認証局の証明書をブラウザにインポートできます。 ①認証局の証明書をローカルにダウンロードしておきます。 ②Chromeの『設定＞プライバシーとセキュリティ＞（もっと見る）＞証明書の管理』をクリック ③認証局タブの「インポート」をクリックし、cacert.pemを選択してインポートする。 ④信頼の設定ウィンドウが開くので、「ウェブサイトの識別でこの証明書を信頼します」にチェックを入れて「OK」をクリックする。 これでオレオレ証明書のページがエラーなく開けるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/elasticsearch/",
    title: "Elasticsearch",
    date: "2020-06-11T00:00:00Z",
    body: "Elasticsearch"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/06/11/elasticsearch-topics/",
    title: "Elasticsearchの使い方メモ",
    date: "2020-06-11T00:00:00Z",
    body: "Elasticsearchの使い方メモ 数年前にElasticsearchを利用していた頃の使いかたメモが見つかったので記事にしておきます。 以下例ではElasticsearchには商品ID(product_id)も持つ商品情報が格納されているものとします。 ID指定で1件だけ検索するクエリ 単純にproduct_idを指定するだけです。 { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;123\u0026#34; } } } ID指定で複数件、順序を指定して検索するクエリ 複数件の検索をします。取得する順序もweightで指定します。 weightが大きい方が上位に取得されます。 { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;:{ \u0026#34;should\u0026#34;:[ {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;123\u0026#34;}}, {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;323\u0026#34;}}, {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;223\u0026#34;}} ] } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;filter\u0026#34;: { \u0026#34;product_id\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;123\u0026#34; } }, \u0026#34;weight\u0026#34;: 3 }, { \u0026#34;filter\u0026#34;: { \u0026#34;product_id\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;323\u0026#34; } }, \u0026#34;weight\u0026#34;: 2 }, { \u0026#34;filter\u0026#34;: { \u0026#34;product_id\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;223\u0026#34; } }, \u0026#34;weight\u0026#34;: 1 } ] } } } ※メモ shouldはORの意味合いがあります。 ID指定で複数件削除するクエリ シンプルです。 { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;:{ \u0026#34;should\u0026#34;:[ {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;123\u0026#34;}}, {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;223\u0026#34;}}, {\u0026#34;match\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;323\u0026#34;}} ] } } }"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/06/10/aws-s3-redirect/",
    title: "AWS S3を使ったリダイレクトの方法",
    date: "2020-06-10T00:00:00Z",
    body: "AWS S3を使ったリダイレクトの方法 AWS S3上に置いていた静的コンテンツを部分的に別サーバー等に移設した際など 特定のパスへのアクセスを別サーバー等にリダイレクトする設定です。 対象のS3の 「プロパティ」＞「Static website hosting」＞「このバケットを使用してウェブサイトをホストする」 にリダイレクトルールという欄があるので、以下のように設定します。 この例では①〜③のリダイレクトルール(RoutingRule)を設定しています。 \u0026lt;RoutingRules\u0026gt; \u0026lt;RoutingRule\u0026gt;　・・・① \u0026lt;Condition\u0026gt; \u0026lt;KeyPrefixEquals\u0026gt;kokojanai_dokoka.html\u0026lt;/KeyPrefixEquals\u0026gt; \u0026lt;/Condition\u0026gt; \u0026lt;Redirect\u0026gt; \u0026lt;Protocol\u0026gt;https\u0026lt;/Protocol\u0026gt; \u0026lt;HostName\u0026gt;new-server.co.jp\u0026lt;/HostName\u0026gt; \u0026lt;ReplaceKeyWith\u0026gt;kokodayo.html\u0026lt;/ReplaceKeyWith\u0026gt; \u0026lt;/Redirect\u0026gt; \u0026lt;/RoutingRule\u0026gt; \u0026lt;RoutingRule\u0026gt;　・・・② \u0026lt;Condition\u0026gt; \u0026lt;HttpErrorCodeReturnedEquals\u0026gt;403\u0026lt;/HttpErrorCodeReturnedEquals\u0026gt; \u0026lt;/Condition\u0026gt; \u0026lt;Redirect\u0026gt; \u0026lt;Protocol\u0026gt;https\u0026lt;/Protocol\u0026gt; \u0026lt;HostName\u0026gt;new-server.co.jp\u0026lt;/HostName\u0026gt; \u0026lt;/Redirect\u0026gt; \u0026lt;/RoutingRule\u0026gt; \u0026lt;RoutingRule\u0026gt;　・・・③ \u0026lt;Condition\u0026gt; \u0026lt;HttpErrorCodeReturnedEquals\u0026gt;404\u0026lt;/HttpErrorCodeReturnedEquals\u0026gt; \u0026lt;/Condition\u0026gt; \u0026lt;Redirect\u0026gt; \u0026lt;Protocol\u0026gt;https\u0026lt;/Protocol\u0026gt; \u0026lt;HostName\u0026gt;new-server.co.jp\u0026lt;/HostName\u0026gt; \u0026lt;/Redirect\u0026gt; \u0026lt;/RoutingRule\u0026gt; \u0026lt;/RoutingRules\u0026gt; ①特定パスへのアクセス 特定のパスへアクセスがあった際にパスを書き換えてリダイレクトします。 S3から新サーバー等に移設した際にパスが変わった場合などに使えます。 設定する値は以下の通りです。 設定項目 設定値 サンプル値 KeyPrefixEquals このパスにアクセスがあったらリダイレクトします。 kokojanai_dokoka.html Protocol リダイレクト先のプロトコルです https HostName リダイレクト先のホスト名です new-server.co.jp ReplaceKeyWith KeyPrefixEqualsの値をこの値で置き換えます。 kokoday.html サンプルではS3にs3-sample.co.jpというDNSを割り振って使っていたとすると https://s3-sample.co.jp/kokojanai_dokoka.htmlへのアクセスは https://new-server.co.jp/kokodayo.htmlへリダイレクトされます。 ②③ S3上に存在しないパスへのアクセス S3上に存在しないパスへアクセスがあった場合に別サーバー等へ同パスのままリダイレクトします。 S3から新サーバー等に同パスのまま移設した際に使えます。 ②と③はHttpErrorCodeReturnedEquals以外は同じです。 設定項目 設定値 サンプル値 HttpErrorCodeReturnedEquals S3の応答HTTPコードが何だったらリダイレクトするか 403 および 404※ Protocol リダイレクト先のプロトコルです https HostName リダイレクト先のホスト名です new-server.co.jp ※S3上に存在しないパスの場合404だけじゃなく403を返すパターンもあるようなので念の為両方書いています。 サンプルではS3にs3-sample.co.jpというDNSを割り振って使っていたとすると https://s3-sample.co.jp/hoge.htmlへのアクセスは https://new-server.co.jp/hoge.htmlへリダイレクトされます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/06/06/python-dict-list/",
    title: "Dictionaly型から特定の値を取り出しlist型に入れる",
    date: "2020-06-06T00:00:00Z",
    body: "Dictionaly型から特定の値を取り出しlist型に入れる pythonのテクニック(?)的なものを教わったのでメモです。 Dictionaly型から、あるキーの値を取り出して配列に取り出す方法について 特になにも考えずにforで回して取り出してました。 例えば、idとnameからなるDictionalyからnameを取り出して配列にする場合は以下のようにしていました。 # Dictionaly定義 fruits_list = [ {\u0026#39;id\u0026#39;:1, \u0026#39;name\u0026#39;:\u0026#39;apple\u0026#39;}, {\u0026#39;id\u0026#39;:2, \u0026#39;name\u0026#39;:\u0026#39;banana\u0026#39;}, {\u0026#39;id\u0026#39;:3, \u0026#39;name\u0026#39;:\u0026#39;cherry\u0026#39;}, ] # Dictionalyからlistに取り出し name_list = [] for fruits in fruits_list: name_list.append(fruits.get(\u0026#39;name\u0026#39;)) # 出力 # [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;cherry\u0026#39;] print(name_list) しかし、この取り出し処理は1行で書けるようです。 修正後、以下のようになりました。 # Dictionaly定義 fruits_list = [ {\u0026#39;id\u0026#39;:1, \u0026#39;name\u0026#39;:\u0026#39;apple\u0026#39;}, {\u0026#39;id\u0026#39;:2, \u0026#39;name\u0026#39;:\u0026#39;banana\u0026#39;}, {\u0026#39;id\u0026#39;:3, \u0026#39;name\u0026#39;:\u0026#39;cherry\u0026#39;}, ] # Dictionalyからlistに取り出し name_list = [fruits.get(\u0026#39;name\u0026#39;) for fruits in fruits_list] # 出力 # [\u0026#39;apple\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;cherry\u0026#39;] print(name_list) スマートですね！"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/06/01/ubuntu-20-04-hdmi/",
    title: "ThinkpadにHDMIケーブルを接続したままUbuntu 20.04を起動する",
    date: "2020-06-01T00:00:00Z",
    body: "ThinkpadにHDMIケーブルを接続したままUbuntu 20.04を起動する 環境 ubuntu 20.04 LTS 参考 ubuntu バグフォーラム ubuntu wiki 前回の記事にてUbuntuを20.04にアップデートしたら HDMIケーブルを挿したままPCを起動する事ができなくなった話を書きました。 Linuxカーネルを書き換えれば動くような話もあったのですが、それはやりたくなかったので放置していました。 2020/6/1現在、ubuntu公式の対応はまだされていないようなのですが、 ubuntuのバグ報告フォーラムにて 『quiet splashを無効化したらいけた！』という書き込みが5/29にされていました。 試してみたら確かに動いたので手順を記載します。 grub設定を変更する 以下のコマンドにてgrubの設定をいじります。 $ sudo gedit /etc/default/grub 設定ファイルがテキストエディタで開くので、quiet splashの行をコメントアウトします。 コメントアウト後は保存して閉じます。 〜省略〜 GRUB_DEFAULT=0 GRUB_TIMEOUT_STYLE=hidden GRUB_TIMEOUT=10 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u0026gt; /dev/null || echo Debian` #GRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet splash\u0026#34;　←この行を#でコメントアウトする GRUB_CMDLINE_LINUX=\u0026#34;\u0026#34; 〜省略〜 grub設定の反映 以下のupdate-grubコマンドでgrub設定を反映させます。 しばらく処理が走るので完了するのを待ちます。 $ sudo update-grub Sourcing file `/etc/default/grub\u0026#39; Sourcing file `/etc/default/grub.d/init-select.cfg\u0026#39; Generating grub configuration file ... Linux イメージを見つけました: /boot/vmlinuz-5.4.0-33-generic Found initrd image: /boot/initrd.img-5.4.0-33-generic Linux イメージを見つけました: /boot/vmlinuz-5.4.0-31-generic Found initrd image: /boot/initrd.img-5.4.0-31-generic Found Windows Boot Manager on /dev/nvme0n1p1@/EFI/Microsoft/Boot/bootmgfw.efi Adding boot menu entry for UEFI Firmware Settings これで設定完了です。 次回起動時からHDMIを挿したまま起動しても問題なくOSが立ち上がります。 なお、quiet splashを無効にしたのでPC起動／停止時に出ていたLenovoとubuntuの両ロゴが表示されるLoading画面は出なくなります。 代わりに現在の処理状況がテキストでババーッと流れるので、驚かないでください。 (昔ながらのLinuxの起動／停止画面と同じ感じになる)"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/05/19/serverless-framework-lambda/",
    title: "Serverless Frameworkを使ってLambda \u0026 API Gatewayをデプロイする",
    date: "2020-05-19T00:00:00Z",
    body: "Serverless Frameworkを使ってLambda \u0026 API Gatewayをデプロイする 環境 ubuntu 20.04 LTS Serverless Frameworkというサーバーレス環境を簡単に構築できるツールがあります。 https://www.serverless.com/framework/docs/ 今回はそれを使ってAWS上にLambda \u0026amp; API Gatewayを使ったサーバーレスAPIを作ってみます。 事前準備(awscliの準備) Serverless Frameworkはaws cliを使ってデプロイするので、インストールしておく必要があります。 既に作業PC等でawscliを使える状態であれば本手順は飛ばして大丈夫です。 awscliのインストール 公式ページの手順を実施します。 https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-linux.html $ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; $ unzip awscliv2.zip $ sudo ./aws/install これでawscliのインストールが出来ました。 念の為バージョンを確認しておきます。 $ aws --version aws-cli/2.0.14 Python/3.7.3 Linux/5.4.0-31-generic botocore/2.0.0dev18 アクセスキーの準備 awscliで利用するアクセスキーをAWS IAMで生成します。 手順は公式ページにあります。 https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration ざっくりいうと、以下の作業をします。 ①IAMへアクセス ②アクセスキーを発行するユーザーを選択する。 ③「認証情報」タブの「アクセスキーの作成」をクリックする。 ④表示されるアクセスキーID/シークレットアクセスキーをメモしておく。 以上です。 awscliの設定 IAMで取得したアクセスキーとリージョン情報をawscliに設定します。 $ aws configure AWS Access Key ID [None]: YOUR-ACCESS-KEY-ID AWS Secret Access Key [None]: YOUR-SECRET-ACCESS-KEY Default region name [None]: ap-northeast-1 Default output format [None]: ※output formatを空のままエンターするとjsonになる。 もし複数のAWSアカウントを使い分けている場合などであれば、以下のように実行することでアクセスキーを設定するプロファイルを指定することもできます。 以下のprofile01は好きなプロファイル名を入れてください。 $ aws configure --profile profile01 これでプロファイルを切り替えながらawscliを使えるようになります。 Serverless Frameworkの環境構築 Serverless Frameworkのインストール 公式ページの手順に従います。 https://www.serverless.com/framework/docs/getting-started/ ubuntu(Linux)では以下コマンドを実行します。 curl -o- -L https://slss.io/install | bash Serverless Frameworkのテンプレート読込 作業ディレクトリにてserverlessコマンドを実行し、いくつか質問されるので答えることでServerless Frameworkを利用するためのテンプレートを読み込めます。 下記例ではLambdaの言語はpython、サービス名はsw-testとしています。 ※slsコマンドはserverlessコマンドの短縮系です $ sls Serverless: No project detected. Do you want to create a new one? Yes Serverless: What do you want to make? AWS Python Serverless: What do you want to call this project? sw-test Project successfully created in \u0026#39;sw-test\u0026#39; folder. You can monitor, troubleshoot, and test your new service with a free Serverless account. Serverless: Would you like to enable this? No You can run the “serverless” command again if you change your mind later. Serverless: Would you like to setup a command line \u0026lt;tab\u0026gt; completion? No これでsw-testディレクトリが生成され、その中にServerless Frameworkのテンプレートが読み込まれています。 Serverless Frameworkの設定 上記で生成したテンプレートに検証/本番環境別のconfigを追加して環境別の変数を定義できるようにします。 ファイル構成はこんな感じです。 sw-test ├─conf ・・・設定ファイルディレクトリ（追加） │ ├─dev.yml ・・・検証環境の設定ファイル（追加） │ └─prd.yml ・・・本番環境の設定ファイル（追加） ├─handler.py ・・・Lambdaで実行するスクリプト(今回はテンプレートのサンプルをそのまま使います) └─serverless.yml ・・・デプロイ内容を定義したファイル ファイルの中身は以下の通りです。 【dev.yml】 とりあえずリージョンだけ。 region: us-west-2 【prd.yml】 とりあえずリージョンだけ。 region: ap-northeast-1 【serverless.yml】 service: sw-test custom: default_stage: dev stage: ${opt:stage, self:custom.default_stage} deploy時にstageを指定しなかったらdevを設定します。 config_file: dev: ${file(./conf/dev.yml)} prd: ${file(./conf/prd.yml)} config: ${self:custom.config_file.${self:custom.stage}} stageに合わせてdev.ymlかprd.ymlを読み込みます。 provider: name: aws runtime: python3.8 region: ${self:custom.config.region} dev/prd.ymlの中身を呼び出しています。設定ファイルにregion以外の指定をすれば同様の方法で呼び出せます。 functions: hello: handler: handler.hello 以下で指定するeventsが発生するとhandler.pyのhelloメソッドを呼びます events: - http: path: hello リクエストを受けるパス method: get 上記のようにfunctionsのeventsでhttpを指定しておくと、Lambdaと一緒にAPI Gatewayもデプロイされます。 ■ハマりポイント 全体的にインデントはスペース2つですが、path: helloの部分だけ- http:のハイフンの箇所からスペース4つ分下がっています。 つまりhttp:の部分からスペース2つ分下げるのが正解のようです。 (YAMLに慣れている人にとっては当たり前なのかもしれませんが、ちょっと躓きました) 上記ができていないとデプロイ時に以下のエラーがでます。 Serverless Error --------------------------------------- Missing or invalid \u0026#34;path\u0026#34; property in function \u0026#34;hello\u0026#34; for http event in serverless.yml. If you define an http event, make sure you pass a valid value for it, either as string syntax, or object syntax. Please check the indentation of your config values if you use the object syntax. Please check the docs for more options. デプロイ実行 awscliのプロファイルを分けた場合は先にプロファイルを指定します。 export AWS_PROFILE=\u0026#34;profile01\u0026#34; デプロイします。 sls deploy 特にエラーが発生しなければ、これで検証環境リージョンのLambdaとAPI Gatewayにアプリケーションが追加されています。 本番環境にリリースしたい場合は以下のようにstageを指定してdeployします。 sls deploy --stage prd 以上です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/05/17/update-ubuntu-20-04/",
    title: "Ubuntu 20.04にアップデートした話",
    date: "2020-05-17T00:00:00Z",
    body: "Ubuntu 20.04にアップデートした話 待望のUbuntu 20.04 LTSが出たので、19.10からアップデートしてみました。 アップデートと同時に発生した事象について記載します。 消えたアプリ アップデートするとインストールしてあった以下のアプリが消えたので、再インストールしました。 shutter (スクショツール) MySQL Workbench shutter こちらのページを参考にさせて頂きました。 https://www.debugpoint.com/2020/04/shutter-install-ubuntu/ 実施した手順は以下です。 sudo apt update sudo apt upgrade sudo add-apt-repository ppa:linuxuprising/shutter sudo apt install shutter MySQL Workbench コマンドでちょこちょこやってインストールする方法もあるようですが、素直に公式ページからdebをダウンロードしてインストールしました。 https://dev.mysql.com/downloads/workbench/ コマンドでインストールする方法も結局↑でダウンロードできるdebをコマンドでインストールしてるだけのようです。 HDMI周りの不具合発生 ※2020/6/1追記 対処法が分かったのでこちらの記事で記載しています。 ThinkpadにHDMIケーブルを接続したままUbuntu 20.04を起動する 私の作業PCはThinkpad T480なのですが、HDMIポートにケーブルを挿した状態で起動しようとすると、ロゴ画面で止まってしまう事象が発生しました。 Ubuntn20.04にしてからマルチディスプレイで起動する事が出来なくなった。ずっとこの画面のまま動かない。 起動後にケーブル繋いだら普通にマルチにできる。 pic.twitter.com/Ke1ZLBLXv4 \u0026mdash; djiroy (@mildjester) May 16, 2020 PCのThunderboldポートにもThunderbold→HDMIのケーブルを繋いでいるのですが、そちらは特に挿したまま起動しても問題ありませんでした。 調べてみると同様の事象を報告している方が複数いたので、おそらくubuntu 20.04の不具合かと思われます。 https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1874194 また、ubuntuにログイン後にディスプレイケーブルの抜き差しをするとメニューバーの位置がおかしくなりました。 これはHDMIポートでもThunderboltポートでも発生します。 なので、私は以下の手順でPCを起動しています。 ①HDMIケーブルを抜いてPCの電源ON。 ②ubuntuが起動し、ログイン画面が表示されたらHDMIケーブルを挿す。 ③ログインする。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/04/15/gulp-uglify-keep-name/",
    title: "gulp-uglifyで圧縮時に変数／関数名を圧縮させない",
    date: "2020-04-15T00:00:00Z",
    body: "gulp-uglifyで圧縮時に変数／関数名を圧縮させない 参考 github mishoo/UglifyJS2 意外と調べても情報が出てこなかったのでメモです。 gulp-uglifyを使ってJSファイルを圧縮すると定義している変数名や関数名も圧縮されてアルファベット1文字等になります。 圧縮したJSファイル内でのみ呼び出される変数・関数であれば良いのですが、外部からも呼び出したいというパターンもあると思います。特に関数。 変数名や関数名を保持したい場合は、gulp-uglify実行時に以下の用にオプションを指定してあげればOKです。 (gulpfile.js例) var gulp = require(\u0026#39;gulp\u0026#39;); var uglify = require(\u0026#39;gulp-uglify\u0026#39;); var uglify_option = { keep_fnames: true, // function名を圧縮させない場合に指定 mangle: false // 変数名を圧縮させない場合に指定 } gulp.task(\u0026#34;default\u0026#34;, function() { return gulp.src(\u0026#39;src/*.js\u0026#39;) .pipe(uglify(uglify_option)) .pipe(gulp.dest(\u0026#39;dest\u0026#39;)); });"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/03/20/javascript-input-event/",
    title: "javascriptの入力イベントでIME未確定時は発火させない設定方法",
    date: "2020-03-20T00:00:00Z",
    body: "javascriptの入力イベントでIME未確定時は発火させない設定方法 テキストボックスに入力中の内容をjavascriptでチェックする場合など keydownイベントを使うとIME未確定時にも発火してしまい無駄に沢山処理が走ってしまいますし、 textInputイベントは対応していないブラウザがあるし、 そんな時の対応方法です。 javascriptのイベントにはIME入力開始時の「compositionstart」とIME確定時の「compositionend」というものがあります。 これを使ってIME入力中を判定できるフラグを作ります。 具体的には以下のようにします。 \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;hoge\u0026#34; /\u0026gt; \u0026lt;script\u0026gt; // とりあえずオブジェクト化してますが、ベタ書きでも構いません var inputEventHandler = { compFlg : false, // IME入力中フラグ initialize : function(){ let targetElm = document.getElementById(\u0026#39;hoge\u0026#39;); targetElm.addEventListener(\u0026#39;keydown\u0026#39;, function() { // ブラウザによりcompositionendイベントとの発火タイミングが前後するのでsetTimeoutで少し遅らせます setTimeout(function(){ if (self.compFlg) { return; } console.log(\u0026#39;ここで何か処理を実行する\u0026#39;); }, 10) }); targetElm.addEventListener(\u0026#39;compositionstart\u0026#39;, function() { self.compFlg = true; }); targetElm.addEventListener(\u0026#39;compositionend\u0026#39;, function() { self.compFlg = false; }); } } inputEventHandler.initialize(); \u0026lt;/script\u0026gt;"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/03/15/github-actions-deploy/",
    title: "GitHub ActionsでAWS Code Deployを回す",
    date: "2020-03-15T00:00:00Z",
    body: "GitHub ActionsでAWS Code Deployを回す GitHub ActionsからAWSコマンドを簡単に叩けるようだったので試してみました。 今回CIでやることは『masterにプッシュされたらCodeDeployを走らせる』のみです。 AWS Code Deployの設定 こちらの記事で手順を紹介しています。 AWS Code DeployでEC2にデプロイする AWS IAMの設定 こちらの記事で手順を紹介しています。 今回CircleCIは使わないので、「IAM準備」の手順のみ参照してください。 CircleCIからAWS Code Deployを実行する GitHub Secretsへのキー登録 前手順で取得したIAMユーザーの『アクセスキーID』と『シークレットアクセスキー』をGitHubのSecretsに登録します。 まず、GitHubの対象リポジトリの『Settings ＞ Secrets』を開き、『Add a new secret』をクリックします。 Secretsの設定画面になるので以下2つを設定します。 Name Value AWS_ACCESS_KEY_ID IAMのアクセスキーID AWS_SECRET_ACCESS_KEY IAMのシークレットアクセスキー GitHub Actionsの設定 デプロイしたいリポジトリのGitHubページの『Actions』タブを開き、 「Set up a workflow yourself」をクリックします。 .github/workflows/main.ymlの編集画面が開くので、以下のように記載します。 name: AWS Code Deploy on: push: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: - name: Run AWS CodeDeploy with: args: deploy create-deployment --application-name \u0026#34;※APP_NAME※\u0026#34; --deployment-group-name \u0026#34;※GROUP_NAME※\u0026#34; --github-location repository=\u0026#34;※REPO※\u0026#34;,commitId=\u0026#34;${{ github.sha }}\u0026#34; env: AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_DEFAULT_REGION: ※REGION※ ※2020/11/20 修正 以前は以下を参考にmain.ymlを記載していたが、2020年11月現在使えなくなっていたので上記の通り切り替えた。 AWS Cli install action なお、以下の部分は各自の環境に合わせて読み替えてください。 読み替える文字列 設定する値 ※APP_NAME※ CodeDeployで設定したアプリケーション名 ※GROUP_NAME※ CodeDeployで設定したデプロイグループ名 ※REPO※ デプロイ対象のGitHubリポジトリ(「ユーザー名/リポジトリ名」形式) ※REGION※ CodeDeployを走らせるAWSのリージョン(例：ap-northeast-1) 記載できたらコミットします。 コミットしたら、今回のコミットに対してGitHub Actionが動くので正常終了するか確認してください。 これで設定は完了です。 masterブランチが更新される度にAWS Code Deployが走りデプロイされるようになりました。 今回はデプロイのみのCIだからか、1回1分以内で処理完了していました。 Actionsはプライベートリポジトリで月間2,000分まで無料(2020年3月現在)なので、2,000回くらいリリースできる想定。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/03/10/gas-gmail-slack/",
    title: "GASでGmailをSlackに転送する",
    date: "2020-03-10T00:00:00Z",
    body: "GASでGmailをSlackに転送する 参考 Class GmailApp(公式ドキュメント) 【GAS】Gmailを定期的に確認し新規メールをSlackに通知する 【Google Apps Script】Gmailの新着(未読)メールを取得する【コピペでOK】 元々IFTTTやZepierを使ってGmailをSlackと連携させていたのですが、 最近Gmail側のセキュリティ変更でうまく連携できなくなってきたので Google Apps Script(GAS)を使って連携させることにしました。 GASでやることは 『5分周期でメールをチェックし、未読メールがあればSlackに転送して既読にする』 です。 SlackのWebHooks URLの取得 GASに限らず、API経由でSlackに通知を出したい時に使うURLを取得します。 これを取得しておけば自作botを作ったりできます。 こちらのページから自分のSlackチームにWebHooksを追加できます。 （要Slackログイン） https://slack.com/apps/A0F7XDUAZ-incoming-webhooks https://hooks.slack.com/services/AAAAAAA/BBBBBBB/CCCCCCCという形式のURLが取得できれば完了です。 ちなみに、今回のGASの話とは関係ないですけど PHPからWebHooksを使ってメッセージ送信するライブラリを作っています。 全然誰にも使われてないですけど、参考までに載せておきます。 https://github.com/mildjester/slackwebhock Google Apps Scriptの作成 まず、自分のGASのページを開きます。 https://script.google.com/u/0/home?hl=ja そこで「新しいプロジェクト」をクリックしてください。 コード編集画面が開くので、「コード.gs」内を以下のように記載します。 最初の「初期設定」の部分は自分の環境に合わせて修正してください。 // 初期設定 const HOOK_URL = \u0026#39;https://hooks.slack.com/services/AAAAAAA/BBBBBBB/CCCCCCC\u0026#39;; // 取得したWebHooks URL const SEND_CHANNEL = \u0026#39;general\u0026#39;; // Gmailの内容を送信したいSlackチャンネル function main() { // 検索条件：受信トレイにある未読メールで１０分以内に受信したもの var after = parseInt(((new Date()).getTime() - 10 * 60 * 1000) / 1000); //10分前の時刻をUNIX時間で取得 var searchTarget = \u0026#39;in:inbox is:unread after:\u0026#39; + after; GmailApp .search(searchTarget) .forEach(function (thread) { thread.getMessages().forEach(function (message) { send(message); }); thread.markRead(); }); } function send(message) { var sendText = \u0026#39;【件名】\\n\u0026#39; + message.getSubject() + \u0026#39;\\n【本文】\\n\u0026#39; + message.getPlainBody(); var jsonData = { \u0026#34;icon_emoji\u0026#34;: \u0026#39;:mailbox:\u0026#39;, \u0026#34;channel\u0026#34; : SEND_CHANNEL, \u0026#34;username\u0026#34; : message.getFrom(), \u0026#34;text\u0026#34; : sendText }; var options = { \u0026#34;method\u0026#34; : \u0026#34;post\u0026#34;, \u0026#34;contentType\u0026#34; : \u0026#34;application/json\u0026#34;, \u0026#34;payload\u0026#34; : JSON.stringify(jsonData) }; UrlFetchApp.fetch(HOOK_URL, options); } 上記を記載したら「Ctrl + s」か上部メニューのフロッピーマークをクリックして保存します。 保存時にプロジェクト名を聞かれるので好きな名前を付けてください。 （例：Gmail2Slack） Google Apps Scriptの自動実行設定 作成したスクリプトの自動実行設定をします。 上部メニューの時計マークをクリックしてください。 作成したスクリプトのトリガー画面が開くので、「トリガーを追加」をクリックしてください。 トリガー追加画面が開くので、以下のように入力して「保存」をクリックしてください。 項目 設定値 実行する関数 main 実行するデプロイ Head イベントのソース 時間主導型 時間ベースのトリガーのタイプ 分ベースのタイマー 時間の間隔 5分おき 連携するGoogleアカウントの選択画面が開くので、Gmailを読み込むアカウントを選択します。 「このアプリは確認されていません」というアラート画面が開くので、下部の「詳細 ＞ {作成したアプリ名}に移動」をクリックします。 次に権限の許可画面が開くので「許可」をクリックします。 これで5分おきにGmailがチェックされSlackに連携されるようになりました。 その他補足 Slackに送信される内容について Slackに送信される内容は件名と本文だけにしています。 もし他の情報も取得したい場合はスクリプト内の「sendText」の内容を変更してください。 取得できる値はこちら参考。 Class GmailApp(公式ドキュメント) なお、メール本文全てをSlackに流すとかなり長くなります。 Slackにはあくまで「こんなメール来てますよ」程度の通知を送りたいだけであれば 本文は適当な文字数で切った方が良いです。 例えば150文字で切る場合はこんな感じです。 var sendText = \u0026#39;【件名】\\n\u0026#39; + message.getSubject() + \u0026#39;\\n【本文】\\n\u0026#39; + message.getPlainBody().substr(0, 150); //150文字で切る Slackで表示される名前とアイコンについて 「jsonData」に設定している「username」と「icon_emoji」を変更することで Slack通知時の名前とアイコンも変更できます。 タイマー設定時間について 今回作成したスクリプトは「5分周期で10分以内に受信した未読メールをチェックする」というものです。 5分周期で走るなら10分以内じゃなくて6分以内とかでも良いかもしれませんが、かなり余裕をもって設定しています。 もし大量にメールを受信している人の場合は処理が重くなるかもしれないので、その場合はいい感じに時間調整してください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/03/05/get-google-analytics-report/",
    title: "Google Analyticsから直近のページ閲覧数を取得する",
    date: "2020-03-05T00:00:00Z",
    body: "Google Analyticsから直近のページ閲覧数を取得する 環境 PHP 7.3 google/apiclient 2.4 参考 https://developers.google.com/analytics/devguides/reporting/core/v3/quickstart/service-php?hl=ja 商品ページにて「24時間以内に○○人が見ています」という販促メッセージを出す時に、乱数を使っているサービスもあるようですが、 それはさすがに嫌なのでGoogle Analyticsから閲覧実績を取得して表示してみます。 なお、Google Analyticsからは「昨日から今日まで」のように日付指定しかできないので、「10分以内に」のように厳密な時間を指定して閲覧者数を表示することはできません。 GCPのサービスアカウント作成 まず、以下ページにてサービスアカウントを作成します。 ※もしGoogle CloudPlatform(GCP)のプロジェクトを作成していない場合は「プロジェクトを作成」をクリックしてGCPプロジェクトを作成してから実行します。 https://console.developers.google.com/iam-admin/serviceaccounts?hl=ja 「プロジェクトの選択」よりサービスアカウントを作成するプロジェクトを選択し、 「＋サービスアカウントを作成」をクリックします。 サービスアカウントの作成画面が開くので以下の通り入力して「作成」をクリックしてください。 項目 入力内容 サービスアカウント名 独自の自分で判断できる名前 サービスアカウントID 独自のID(サービスアカウント名を入力したら勝手に入ります) サービスアカウントの説明 自分が分かる説明 次にサービスアカウントの権限設定画面が開きますが、そのまま「続行」をクリックします。 その次の画面で「ユーザーにこのサービス アカウントへのアクセス権を付与」は特に何もしません。 「キーの作成」の中の「キーを作成」をクリックし、キーのタイプはJSONを選択して「作成」をクリックします。 JSONファイルがダウンロードされますので取っておいてください。 サービスアカウントの作成画面はそのまま「完了」をクリックして終わります。 サービスアカウント一覧に戻り、作成したサービスアカウントのメールアドレスが表示されるので記録しておきます。 Google Analytics APIの有効化 以下ページにアクセスし、Google Analytics APIを有効化してください。 https://console.developers.google.com/apis/api/analytics.googleapis.com/overview Google Analyticsの設定 さきほど作成したサービスアカウントをGoogle Analyticsに追加します。 アカウントユーザーまたはプロパティユーザーにサービスアカウントのメールアドレスを追加します。 権限は「表示と分析」のみ(デフォルト)で大丈夫です。 スクリプトの作成 PHPとcomposerの環境は構築済みの前提とします。 まず、Google Analytics APIのパッケージを取得します。 composer require google/apiclient 次に以下のPHPスクリプトを作成します。 \u0026lt;?php require_once __DIR__ . \u0026#39;/vendor/autoload.php\u0026#39;; $analytics = initializeAnalytics(); $profile = getFirstProfileId($analytics); $results = getResults($analytics, $profile); printResults($results); /** * Google Clientを初期化する */ function initializeAnalytics() { $KEY_FILE_LOCATION = {{GCPのサービスアカウント作成時に取得したjsonファイルへのパス}}; // 例：$KEY_FILE_LOCATION = __DIR__ . \u0026#39;/config/hogehoge.json\u0026#39;; $client = new Google_Client(); $client-\u0026gt;setApplicationName(\u0026#34;Hello Analytics Reporting\u0026#34;); // 任意の名前に変更してよいです $client-\u0026gt;setAuthConfig($KEY_FILE_LOCATION); $client-\u0026gt;setScopes([\u0026#39;https://www.googleapis.com/auth/analytics.readonly\u0026#39;]); $analytics = new Google_Service_Analytics($client); return $analytics; } /** * プロファイルを取得する */ function getFirstProfileId($analytics) { $accounts = $analytics-\u0026gt;management_accounts-\u0026gt;listManagementAccounts(); if (count($accounts-\u0026gt;getItems()) \u0026gt; 0) { $items = $accounts-\u0026gt;getItems(); $firstAccountId = $items[0]-\u0026gt;getId(); $properties = $analytics-\u0026gt;management_webproperties -\u0026gt;listManagementWebproperties($firstAccountId); if (count($properties-\u0026gt;getItems()) \u0026gt; 0) { $items = $properties-\u0026gt;getItems(); $firstPropertyId = $items[0]-\u0026gt;getId(); $profiles = $analytics-\u0026gt;management_profiles -\u0026gt;listManagementProfiles($firstAccountId, $firstPropertyId); if (count($profiles-\u0026gt;getItems()) \u0026gt; 0) { $items = $profiles-\u0026gt;getItems(); return $items[0]-\u0026gt;getId(); } else { throw new Exception(\u0026#39;No views (profiles) found for this user.\u0026#39;); } } else { throw new Exception(\u0026#39;No properties found for this user.\u0026#39;); } } else { throw new Exception(\u0026#39;No accounts found for this user.\u0026#39;); } } /** * 取得条件を設定して取得する */ function getResults($analytics, $profileId) { $from = \u0026#39;1daysAgo\u0026#39;; // 昨日から $to = \u0026#39;today\u0026#39;; // 今日までのデータを取得します $metrics = \u0026#34;ga:pageviews\u0026#34;; // 閲覧者数を取得します $option = [ \u0026#34;dimensions\u0026#34; =\u0026gt; \u0026#39;ga:pagepath\u0026#39;, // ページパス別に閲覧数を取得します \u0026#34;sort\u0026#34; =\u0026gt; \u0026#34;-ga:pageviews\u0026#34;, // 閲覧数でソートします。マイナスを付けているので降順になります // \u0026#34;filters\u0026#34; =\u0026gt; \u0026#34;ga:pagepath%3D@/article\u0026#34;, // コメントアウトを外すと、ページパスに「/article」を含むものに絞ります ]; return $analytics-\u0026gt;data_ga-\u0026gt;get( \u0026#39;ga:\u0026#39; . $profileId, $from, $to, $metrics, $option ); } /** * 取得結果を出力する */ function printResults($results) { echo \u0026#39;page url,previews\u0026#39; . PHP_EOL; foreach ($results as $result){ echo $result[0] . \u0026#39;,\u0026#39; . $result[1] . PHP_EOL; } } このPHPを実行すればページ別閲覧者数が取得できます。 取得した結果の中から該当ページの閲覧数を抜き出してページに表示すれば完了です。 上記ソース内のfiltersでページURLを細かく絞ってしまえば現在取得したいページの閲覧数だけ取れますが、 Google Analytics APIの応答は数秒かかるので各ページ個別に閲覧数を取るのは現実的ではありません。 なので、上記のように閲覧数の多いものから順に取ってキャッシュに入れておき、基本的にはキャッシュから取得するようにした方が良さそうです。 (キャッシュは1分間に1度更新するなど) 設定する値について こちらのページでmetricsやdimensionに設定できる値がまとめられています。 閲覧者数以外を取得したい場合など、参考にしてください。 https://ga-dev-tools.appspot.com/dimensions-metrics-explorer/ その他解説はこちらの公式ページをご参照ください。 https://developers.google.com/analytics/devguides/reporting/core/v3/reference"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/02/18/lightsail-wordpress/",
    title: "lightsail上に自前でWordpressを構築する(CentOS編)",
    date: "2020-02-18T00:00:00Z",
    body: "lightsail上に自前でWordpressを構築する(CentOS編) 環境 CentOS Linux release 7.6.1810 (Core) nginx 1.1.17 php 7.4.2 MySQL 8.0.19 以前の記事でAmazon Linux上でWordpressを構築しましたが、 lightsailのAmazon Linux上だとなぜかcertbotが正常に動かなくなったので CentOSで再構築することにしました。 インスタンス作成 lightsailのインスタンスを作成します。 プラットフォーム：Linux/Unix 設計図の選択：OSのみ CentOS インスタンス設定 作成したインスタンスの設定をします。 対象インスタンスのネットワーキングを開き、静的IPをアタッチしておきます。 また、HTTPSで公開する場合はファイアウォールにてTCPの443ポートを開けておきます。 サーバーへのログイン＆初期設定 インスタンスにSSHにて接続し、以下コマンドを実行していきます。 まずrootになります。 以降のコマンドはrootで実行するものとします。 $ sudo -i タイムゾーンを日本にします。 # timedatectl set-timezone Asia/Tokyo 基本的なアプリケーションを入れておきます。 yum install git vim SELinuxの無効化 SELinuxがあると色々動かないことがあるので無効にしてしまいます。 (nginxが403になったり) ちゃんとセキュアな環境を作りたい場合はSELinuxが有効でもちゃんと諸々動く環境を作った方がいいですが、 今回はそこまででは無かったので割愛しています。 現在有効か確認します。 以下コマンドで「Enforcing」と出たら有効です。 # getenforce Enforcing 設定ファイルを修正して無効にします。 # vim /etc/selinux/config 以下部分を修正します。 SELINUX=disabled これでlightsailのコンソールからインスタンスを再起動をすると反映されます。 nginxのインストール 公式の手順を参考にします。 http://nginx.org/en/linux_packages.html#RHEL-CentOS # yum install yum-utils # vim /etc/yum.repos.d/nginx.repo 〜空のファイルが開くので以下を記載する〜 [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true [nginx-mainline] name=nginx mainline repo baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/ gpgcheck=1 enabled=0 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true 〜ここまで〜 # yum-config-manager --enable nginx-mainline # yum install nginx nginxの設定 nginxの設定ファイルを追加します。 ファイル名の.confより前は任意です。 vim /etc/nginx/conf.d/wordpress.conf 以下のように記載します。 (Wordpressおきまりのやつです) server { listen 80; listen [::]:80; server_name your-domain.jp; root /usr/share/nginx/html; index index.php; location / { try_files $uri $uri/ /index.php?$query_string; } location ~\\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; include fastcgi_params; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass unix:/var/run/php-fpm/www.sock; } } phpのインストール 以下のコマンドを実行します。 この例ではphp7.4をインストールしていますが、別のバージョンにする場合は yum install時にenableするリポジトリのremi-phpxx部分を変更してください。 # yum install epel-release # rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm # yum install --disablerepo=* --enablerepo=epel,remi,remi-safe,remi-php74 -y php-fpm php-bcmath php-cli php-common php-devel php-gd php-json php-mbstring php-mysqlnd php-pdo php-xml phpの設定 設定ファイルを開きます。 # vim /etc/php.ini 画像アップロードが難なくできるよう、以下の箇所を修正します。 upload_max_filesize = 16M post_max_size = 16M php-fpmの設定 設定ファイルを開きます。 # vim /etc/php-fpm.d/www.conf nginxで動くように以下の箇所を修正します。 user = nginx group = nginx listen = /var/run/php-fpm/www.sock listen.owner = nginx listen.group = nginx listen.mode = 0660 貧弱インスタンスを作成した場合はpm.max_〜をインスタンスのパワーに合わせて変更してください。 pm.max_children = 15 pm.max_spare_servers = 10 nginx \u0026amp; php-fpmの起動 起動 # systemctl start nginx.service # systemctl start php-fpm.service 自動起動を有効化 # systemctl enable nginx.service # systemctl enable php-fpm.service MySQLのインストール CentOSではデフォルトでmariaDBがインストールされているので削除しておきます。 # yum remove mariadb-libs # rm -rf /var/lib/mysql　※もしあれば削除しておく。1度も起動したことがなければ無いかも。 MySQLをインストールします。 # yum install https://dev.mysql.com/get/mysql80-community-release-el7-2.noarch.rpm # yum install mysql-community-server このままMySQLを起動すると以下のエラーが出ます。 Cannot allocate memory for the buffer pool これはlightsailだとSwap領域が無い事が原因です。 確認するとSwapがゼロになっていると思います。 # free -m total used free shared buff/cache available Mem: 485 74 247 8 163 358 Swap: 0 0 0 Swap領域を確保します。 以下例では1MBブロックを512個＝512MBのSwapを生成しています。 # dd if=/dev/zero of=/swapfile bs=1M count=512 # chmod 600 /swapfile # mkswap /swapfile # swapon /swapfile 確認するとSwapが生成されています。 # free -m total used free shared buff/cache available Mem: 485 73 5 8 406 359 Swap: 511 0 511 このままだとサーバー再起動時にSwapが開放されてしまうので、fstabにも記載しておきます。 # vim /etc/fstab 〜以下行を追記〜 /swapfile swap swap defaults 0 0 MySQLの起動＆自動起動設定有効 systemctl start mysqld.service systemctl enable mysqld.service MySQLのrootパスワード変更 rootの初期パスワードはログ内にあります。 以下のhogehoge部分を確認してください。 # grep \u0026#34;temporary password\u0026#34; /var/log/mysqld.log A temporary password is generated for root@localhost: hogehoge rootのパスワード設定をします。 mysql -u root -p Enter password: (確認した初期パスワードを入れる) ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;新しいパスワード\u0026#39;; let\u0026rsquo;s encrypt設定 let\u0026rsquo;s encryptで無料SSL証明書を取得します。 こちらの記事を参考にしてください。 【Let\u0026rsquo;sEncrypt】Certbotの使い方(CentOS7 + nginx) これでインフラ部分はできました。 後はWordpressの公式サイトなどを見ながらWordpressをDownload＆設定するだけです。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/wordpress/",
    title: "Wordpress",
    date: "2020-02-18T00:00:00Z",
    body: "Wordpress"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2020/02/13/etax-legal-record/",
    title: "e-taxで法定調書関係の申請をする。",
    date: "2020-02-13T00:00:00Z",
    body: "e-taxで法定調書関係の申請をする。 決算を迎え、e-taxにて法定調書関係の申請をしたので、その手順メモです。 なお、計算方法等はマネーフォワード給与にて行なっていますので割愛しています。 申請書の作成 e-taxの左メニューから作成＞申告・申請等開き、新規作成をクリックします。 次に以下を選び『次へ』をクリックします。 もし税目の選択肢に「法定調書関係」が無い場合は、e-tax起動時のインストールメニューにて「法定調書」をインストールしてください。 手続きの種類：申請・届出 税目：法定調書関係 次に以下２つにチェックを入れて『次へ』をクリックします。 給与所得の源泉徴収等の法定調書（及び同合計表） 給与所得の源泉徴収票 給与所得の源泉徴収票等の法定調書合計表 申告・申請等名を指定する画面になるので、自分で分かる程度の名前をつけて『OK』をクリックします。 （「XXXX年度_法定調書」など） 次に基本情報の入力画面になるので、以下を入力or確認して『OK』をクリックします。 提出税務署：申請する税務署（自分の会社がある自治体） 提出年月日：今の日付 法人名(カナ)：自分の会社名カナ 法人名（必須）：自分の法人名 納税地：自分の会社の住所と電話番号 代表者名（カナ）： 代表者名（必須）： 代表者：代表者の住所電話番号 金融機関：会社の銀行口座 帳票一覧が開くので、さきほど選択した「給与所得の源泉徴収票」をダブルクリックで開きます。 提出区分を聞かれるので「新規分提出」を選択し、提出対象の年度を入力して「OK」をクリックします。 すると帳票の中身が修正できるようになるので記載します。 （入力内容は私はマネーフォワード給与から帳票をDLして参照しました） 小さくて文字が見づらいですが、右クリックすると拡大メニューが出ます。 記載が終わったら「作成完了」をクリックします。 控えが必要ならここで印刷などしておきます。 帳票一覧に戻るので、「給与所得の源泉徴収票等の法定調書合計表」の方をダブルクリックで開きます。 こちらも同様に記載をして「作成完了」をクリックします。 マネーフォワード給与から帳票をDLしてきて転記する場合の注意点ですが、 給与の提出区分の部分だけ３０（書面）ではなく１４（電子）に変更する必要があります。 申請書の署名 e-tax左メニューの「署名可能一覧へ」＞「電子署名」を開きます。 作成したデータが表示されるので署名をします。 申請書の送信 e-tax左メニューの「送信可能一覧へ」＞「送信」を開きます。 先ほど作成した帳票が表示されるので、選択して『送信』をクリックします。 何か問題があればエラーが表示されるので、エラー指摘箇所を訂正して再度送信します。 これで法定調書関係の申請は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/%E6%B3%95%E4%BA%BA%E4%BA%8B%E5%8B%99/",
    title: "法人事務",
    date: "2020-02-13T00:00:00Z",
    body: "法人事務"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/12/15/lightsail-wordpress/",
    title: "lightsail上に自前でWordpressを構築する(Amazon Linux編)",
    date: "2019-12-15T00:00:00Z",
    body: "lightsail上に自前でWordpressを構築する(Amazon Linux編) 環境 Amazon Linux AMI 2018.03 (lightsail) ※2020年2月追記 lightsailのAmazon Linuxでcretbotの挙動があやしくなってきたので CentOS版も作りました。 記事はこちら lightsailではWordpressを載せたインスタンスを生成できますが、 全部載せbitnami環境があまり好きではなかったので 空のインスタンスに自分でWordpress環境を構築しました。 構築する環境は nginx + php-fpm(PHP7.2) とします。 インスタンス作成 lightsailのインスタンスを作成します。 プラットフォーム：Linux/Unix 設計図の選択：OSのみ Amazon Linux インスタンス設定 作成したインスタンスの設定をします。 対象インスタンスのネットワーキングを開き、静的IPをアタッチしておきます。 また、HTTPSで公開する場合はファイアウォールにてTCPの443ポートを開けておきます。 各種インストール＆設定 インスタンスにSSHにて接続し、以下コマンドを実行していきます。 まずrootになります。 sudo -i いろいろインストールします。 yum install -y git nginx php72-fpm php72-bcmath php72-cli php72-common php72-devel php72-gd php72-json php72-mbstring php72-mysqlnd php72-pdo php72-xml mysql-server nginxの設定 nginxの設定ファイルを追加します。 ファイル名の.confより前は任意です。 vim /etc/nginx/conf.d/wordpress.conf 以下のように記載します。 (Wordpressおきまりのやつです) server { listen 80; listen [::]:80; server_name your-domain.jp; root /usr/share/nginx/html; index index.php; location / { try_files $uri $uri/ /index.php?$query_string; } location ~\\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; include fastcgi_params; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass unix:/var/run/php-fpm/www.sock; } } phpの設定 設定ファイルを開きます。 vim /etc/php.ini 画像アップロードが難なくできるよう、以下の箇所を修正します。 upload_max_filesize = 16M post_max_size = 16M php-fpmの設定 設定ファイルを開きます。 vim /etc/php-fpm.d/www.conf nginxで動くように以下の箇所を修正します。 user = nginx group = nginx 貧弱インスタンスを作成した場合はpm.max_〜をインスタンスのパワーに合わせて変更してください。 pm.max_children = 15 pm.max_spare_servers = 10 nginx \u0026amp; php-fpmの起動 起動 service nginx start service php-fpm start 自動起動有効化 chkconfig nginx on chkconfig php-fpm on mysqlの設定 起動＆自動起動設定有効 service mysqld start chkconfig mysqld on rootのパスワード設定をします。 以下のnew-password部分に設定したいパスワードを入力してください。 /usr/libexec/mysql55/mysqladmin -u root password \u0026#39;new-password\u0026#39; let\u0026rsquo;s encrypt設定 let\u0026rsquo;s encryptで無料SSL証明書を取得します。 your-domain.jp部分にはWordpressで使うドメインを入力してください。 cd /opt git clone https://github.com/letsencrypt/letsencrypt cd letsencrypt ./letsencrypt-auto certonly --webroot -w /usr/share/nginx/html -d your-domain.jp --debug SSL証明書が取得できたらnginxの設定ファイルを再度開きます。 vim /etc/nginx/conf.d/wordpress.conf SSLについて追記します。 server { 〜省略〜 listen 443 ssl http2; listen [::]:443 ssl http2; ssl_certificate /etc/letsencrypt/live/your-domain.jp/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/your-domain.jp/privkey.pem; 〜省略〜 } これで基盤はできあがりました。 後はWordpressの公式サイトなどを見ながらWordpressをDownload＆設定するだけです。 付録：mysqlコマンド ユーザー作成 CREATE USER \u0026#39;wp-user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;new-password\u0026#39;; データベース作成 CREATE DATABASE wp-database character set utf8mb4; 指定ユーザーのパスワードを変更 UPDATE mysql.user SET password=password(\u0026#39;new-new-password\u0026#39;) where user = \u0026#39;wp-user\u0026#39;;"
  },
  {
    url: "https://blog2.logical-dice.com/tags/apache/",
    title: "Apache",
    date: "2019-11-19T00:00:00Z",
    body: "Apache"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/11/19/apache-directory-hidden/",
    title: "htaccessで本番環境だけ特定のディレクトリを隠す",
    date: "2019-11-19T00:00:00Z",
    body: "htaccessで本番環境だけ特定のディレクトリを隠す 環境 httpd 2.2.34 概要 Apache環境で本番環境・検証環境共にソースコードはgitリポジトリを丸ごとcloneしている状態で git管理下の特定のディレクトリ内のページは検証環境でのみ表示させるという方法です。 （テスト用ページ） 対応方法 本番環境のドメインが「www.honban.com」で検証環境は別ドメインだとした場合、 隠したいディレクトリに.htaccessを配置して以下を記載します。 RewriteEngine On RewriteCond %{http_host} ^www.honban.com RewriteRule ^(.*) - [R=404,L] これは『ドメインが本番環境の場合(RewriteCondの部分)は404ページを表示する(RewriteRuleの部分)』という意味になります。 これで検証環境では該当ディレクトリ内のページは表示でき、本番環境では404ページが表示されるようにできました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/11/08/ubuntu-tilix/",
    title: "Linux用ターミナル『Tilix』",
    date: "2019-11-08T00:00:00Z",
    body: "Linux用ターミナル『Tilix』 環境 Ubuntu 19.10 今までubuntu等のLinux環境ではターミナルはTerminatorを使っていたのですが、 Tilixというターミナルも良さそうだったので使ってみました。 使い勝手はTerminatorよりも良い気がします。 インストール方法 以下コマンドだけでインストール可能です。 sudo apt install tilix 設定 ウィンドウ右上の３本線マークをクリックすると「設定」メニューが出てきます。 そちらからウィンドウの色・サイズやフォントの変更などができます。 タブ分割方法 ウィンドウ左上にタブ分割するボタンがあるので、そちらをクリックするだけでタブ分割できます。 ショートカットキーもありますが、デフォルトのショートカットキーはubuntuでは別アクションにマッピングされているので 使う場合は『設定＞ショートカット』からショートカットキーを変更してください。 fish + fishermanの導入 基本的にterminatorと導入手順は同じです。 こちらを参照してください。 terminator + fish + fishermanの導入"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/11/06/ubuntu-initialize/",
    title: "ubuntu 19.10の初期設定",
    date: "2019-11-06T00:00:00Z",
    body: "ubuntu 19.10の初期設定 環境 Ubuntu 19.10 ThinkPad T480 Thinkpad T480にubuntu 19.10をインストールした際のメモです。 日本語入力設定 そのままでも日本語入力はできますが、入力ソースをMozcに切り替えたりしないといけないので 入力ソースのMozcの優先度を高くします。 アプリランチャーより『設定』を開き、『地域と言語』を開きます。 そこの入力ソースに「日本語」と「日本語(Mozc)」が並んでいるので 「日本語(Mozc)」が上にくるように並べ替えます。 各入力ソースの左端の6丸をドラッグすれば順番を入れ替えられます。 IMEのON/OFFをMac風にする アプリランチャーより『Mozcの設定』を開き、『一般』タグを開きます。 「キー設定の選択」の横にある「編集」ボタンをクリックするとキー設定を変更できます。 そこでHenkan/Hiragana/Katakanaあたりに『IME有効』を割り当て、 Muhenkanに『IME無効』を割り当てます。 これでMacのようにIMEのON/OFFを切り替えられるようになりました。 CapsLockをCtrlに、半角/全角をEscにする こちらもMac風になる設定です。 以下のコマンドを実行するとCapsLockがCtrlになり、半角/全角キーがEsc扱いになります。 setxkbmap -option ctrl:nocaps,japan:hztg_escape tiling機能をOFFにする デフォルトだとウィンドウを画面端にドラッグすると勝手に最大化します。 無くてもいい機能なので以下コマンドで無効化します。 gsettings set org.gnome.shell.overrides edge-tiling false ※2019/12/2追記 ubuntu 19.10では、上記だけではtilingが無効になっていませんでした。 以下の実行も必要そうです。(むしろ、こちらだけでよさそう) gsettings set org.gnome.mutter edge-tiling false 諸々インストール PHP sudo apt install php7.3 composer php7.3-bcmath php7.3-common php7.3-gd php7.3-json php7.3-mbstring php7.3-mysql php7.3-xml php7.3-xmlrpc MySQL Workbench sudo apt install mysql-workbench mysql-client Shutter(スクショツール) 既存画像に文字や矢印を書き込む事もできるので便利です。 sudo add-apt-repository ppa:linuxuprising/shutter sudo apt update sudo apt install shutter その他参考ページ ホームディレクトリの英語化 Tilixの導入 Terminator+fish環境構築 Docker環境構築 L2TPクライアント設定 デュアルブート環境で時計が狂うのを防ぐ"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/10/15/reverseproxy-wordpress/",
    title: "リバースプロキシ先でWordpressを動かす",
    date: "2019-10-15T00:00:00Z",
    body: "リバースプロキシ先でWordpressを動かす 環境 Apache 2.2.34 サーバーA(www.sample.com)でメインサイトが動いていて、サーバーB(sub.sample.com)でWordpressが動いている時に Apacheのリバースプロキシを使って以下のようにアクセスできるようにします。 メインサイト: https://www.sample.com/ Wordpress: https://www.sample.com/blog なお、既に他のリバースプロキシも設定されており、極力そちらへの影響はないように設定します。 他ページ(サーバーC): https://www.sample.com/media ■サーバーAの設定 Apache設定 httpd.confなどに以下のようにリバースプロキシ設定をします。 他リバースプロキシに影響しないようにLocationで括ります。 \u0026lt;Location \u0026#34;/blog\u0026#34;\u0026gt; ProxyPass http://sub.sample.com/blog ProxyPassReverse http://sub.sample.com/blog ProxyPassReverseCookieDomain sub.sample.com www.sample.com ProxyPassReverseCookiePath / /blog/ \u0026lt;/Location\u0026gt; これで/blog配下へのアクセスはサーバーB(sub.sample.com)に飛ばすようになります。 ■サーバーBの設定 Apache設定 Apacheの設定はsub.sample.comにアクセスがある想定で記載しておけば大丈夫です。 WordpressソースはDocumentRoot配下にblogディレクトリを作成して格納します。 SSL通信扱いにする これでWordpressページ自体は表示されますが、必要なJS/CSS/画像などの外部ファイルのURLが非SSLとなってしまい Chromeなどだとエラーとなってしまいます。 なので、.htaccessに以下追記して強制的にSSL通信扱いになるようにします。 SetEnv HTTPS on ※補足 非SSLでhttp://sample.com/blogにアクセスした場合はどうするのって話もありますが、 そもそも非SSLページは推奨されないので、今回は考えてません。 http://sample.com/blogにアクセスが来たらhttps://sample.com/blogにリダイレクトしましょう。 どうしても非SSLとSSLの両方で受けたければSSL接続時はリバースプロキシ時に「X-Forwarded-HTTPS: on」を設定するなど対策してください。 「外部→(HTTPS)→ELB→(HTTP)→サーバーA→(HTTP)→サーバーB」のような構成だと結構面倒です。 ホスト名を補正する ここまでの対応だと$_SERVER['HTTP_HOST']と$_SERVER['SERVER_NAME']がsub.sample.comのままになってしまいます。 ページ表示上は問題無さそうですが、管理画面でブラウザのconsoleに警告が出て気持ち悪いので修正しておきます。 wp-config.phpの先頭に以下の記載を付け加えておきます。 if (isset($_SERVER[\u0026#39;HTTP_X_FORWARDED_HOST\u0026#39;]) \u0026amp;\u0026amp; $_SERVER[\u0026#39;HTTP_X_FORWARDED_HOST\u0026#39;] === \u0026#39;www.sample.com\u0026#39;) { $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] = \u0026#39;www.sample.com\u0026#39;; $_SERVER[\u0026#39;SERVER_NAME\u0026#39;] = \u0026#39;www.sample.com\u0026#39;; } これでwww.sample.comからリバースプロキシで飛ばされてきたアクセス時はホスト名が書き換わるようになりました。 （これはもっといい方法があるかもしれませんが、とりあえずこの方法でも対応できています。）"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/10/10/linux-sudo/",
    title: "Linuxで別ユーザとしてコマンドを実行する",
    date: "2019-10-10T00:00:00Z",
    body: "Linuxで別ユーザとしてコマンドを実行する 環境 Amazon Linux AMI release 2013.03 今更ですがsudoコマンドについて整理しておきたかったので書きました。 まず基本として、rootユーザーとしてコマンドを実行する場合はsudo {コマンド}を打ちます。 $ sudo id uid=0(root) gid=0(root) 所属グループ=0(root) もしrootユーザー以外になり代わるたい場合は-uオプションで指定します。 $ sudo -u apache id uid=48(apache) gid=48(apache) 所属グループ=48(apache) もし特定のユーザーに頻繁になり代わる場合は .bashrcなどにfunction定義しておくと楽です。 (今回一番書きたかったこと) # Apacheユーザとしてコマンド実行する sudoa () { sudo -u apache $* } # Apacheユーザとしてgitコマンドを実行する agit() { sudo -u apache git $* } こうしておくとagit pullと打てば Apacheユーザとしてgit pullを打つことができます。 また、以下のような簡単なスクリプトも書けます。 hoge() { sudo -u yamada sh -c \u0026#39; NAME=`id | grep -o -E \u0026#34;\\([^)]+\\)\u0026#34; | grep -o -m 1 -E \u0026#34;[^\\()]+\u0026#34;` echo \u0026#34;hello ${NAME}!\u0026#34; \u0026#39; } 実行結果は以下のようになります。 $ hoge hello yamada!"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/10/02/bookmarklet-mizuho/",
    title: "Bookmarkletでパスワードを自動で入力させる",
    date: "2019-10-02T00:00:00Z",
    body: "Bookmarkletでパスワードを自動で入力させる みずほダイレクトで振込時の第2暗証番号6桁から数字4桁を抽出するのが面倒だったので、 Bookmarkletを使って入力すべき数字4桁を表示できるようにしました。 Bookmarkletの説明や作り方についてはこちらを参照。 Bookmarklet(ブックマークレット)の作り方 みずほダイレクトに仕込んでみる みずほダイレクトの振込時に第2暗証番号を入力する画面まで来たら 以下のjavascriptを実行すると第2暗証番号が表示されます。 var secondPass = \u0026#39;123456\u0026#39;; // ここに第2暗証番号の6桁を入れてください var orderNum = 0; var pickPw = \u0026#39;入力するパスワード: \u0026#39;; // 聞かれる第2パスワード4桁を順に処理する for (var i = 1; i\u0026lt;=4; i++) { // 第2パスワードの何番目の数字を聞かれているかチェック orderNum = Number(document.getElementById(\u0026#39;txtScndPwdDgt\u0026#39; + i).innerHTML); // 第2パスワードから対象の数字を抜き出してテキストに追加する pickPw += secondPass.substr(orderNum -1, 1) } // 入力すべき4桁の数字を表示する alert(pickPw); これをBookmarklet用に圧縮し、頭にjavascript:を付けます。 （圧縮方法はコチラを参照） javascript:for(var t,e=\u0026#34;入力するパスワード: \u0026#34;,r=1;r\u0026lt;=4;r++)t=Number(document.getElementById(\u0026#34;txtScndPwdDgt\u0026#34;+r).innerHTML),e+=\u0026#34;123456\u0026#34;.substr(t-1,1);alert(e); あとはブラウザのブックマークに新しくページを追加し、URLに上記のjavascriptを入力しておくだけです。 みずほダイレクト以外のサイトでもブラウザの開発者ツールでHTML要素を調べれば 似たような処理を作れると思います。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/09/29/ruby-sass-error/",
    title: "Linux Mintにsassをインストールする",
    date: "2019-09-29T00:00:00Z",
    body: "Linux Mintにsassをインストールする インストール方法 最終的に行き着いたインストール方法です。 インストールするだけならこちらを実行してください。 $ apt install ruby-sass ruby-dev $ sudo gem install listen --version \u0026#39;~\u0026gt; 3.0\u0026#39; 躓いた点 エラーメッセージで検索する人もいると思うので、 躓いた工程をメモしておきます。 まず、最初にsassだけインストールしました。 $ apt install ruby-sass すると、sassコマンドでwatchをする際にエラーが出ました。 $ sass --watch ./sass:./css --style compact --no-cache \u0026gt;\u0026gt;\u0026gt; Sass is watching for changes. Press Ctrl-C to stop. LoadError: cannot load such file -- listen Run \u0026#34;gem install listen --version \u0026#39;~\u0026gt; 3.0\u0026#39;\u0026#34; to get it. Use --trace for backtrace. watchをするためにはlistenが必要であるようです。 そこでlistenをインストールしようとしましたが、エラーになりました。 $ sudo gem install listen --version \u0026#39;~\u0026gt; 3.0\u0026#39; Building native extensions. This could take a while... ERROR: Error installing listen: ERROR: Failed to build gem native extension. current directory: /var/lib/gems/2.5.0/gems/ffi-1.11.1/ext/ffi_c /usr/bin/ruby2.5 -r ./siteconf20190925-17707-1yh0gf6.rb extconf.rb mkmf.rb can\u0026#39;t find header files for ruby at /usr/lib/ruby/include/ruby.h extconf failed, exit code 1 Gem files will remain installed in /var/lib/gems/2.5.0/gems/ffi-1.11.1 for inspection. Results logged to /var/lib/gems/2.5.0/extensions/x86_64-linux/2.5.0/ffi-1.11.1/gem_make.out 調べたら、どうやらruby-devが必要らしかったです。 https://qiita.com/kyrieleison/items/81c14780ad490049202c ruby-devをインストールしてからlistenをインストールしたら成功しました。 $ apt install ruby-dev $ sudo gem install listen --version \u0026#39;~\u0026gt; 3.0\u0026#39; これでsassコマンドが動くようになりました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/09/28/linux-mint-virtualbox/",
    title: "Linux Mint上でWindows10を動かす",
    date: "2019-09-28T00:00:00Z",
    body: "Linux Mint上でWindows10を動かす フロントエンドを触っているとどうしてもIEでの動作確認が必要になるので VirtualBoxを使ってWindows環境を用意します。 Windowsディスクイメージの取得 Microsoftの公式ページにてWindows 10のディスクイメージ(iso)を取得しておきます。 https://www.microsoft.com/ja-jp/software-download/windows10ISO Virtualboxのインストール 以下のコマンドでインストールします。 apt install virtualbox virtualbox-qt 上記実行後、一旦ログアウトするとメニューにvirtualboxが追加されます。 仮想マシンの作成 VirtualBoxを起動し、仮想マシンを新規作成します。 選択肢はWindows10のものを選んで行きます。 仮想マシンが作成できたら設定の「ストレージ」を開き、 空になっている光学ドライブ(CDアイコン)にダウンロードしておいたWin10のディスクイメージを設定して起動します。 あとは指示に従ってwin10のインストールを進めるだけです。 インストールが完了したら先程光学ドライブに設定したディスクイメージは除いて構いません。 その他設定メモ ゲストマシン(仮想マシン)から見るとホストマシンのIPアドレスは「10.0.2.2」になっています。 なので、ホストマシンにてDockerを動かして置いてゲストマシンでhttp://10.0.2.2にアクセスすれば動作確認ができます。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/css/",
    title: "CSS",
    date: "2019-09-14T00:00:00Z",
    body: "CSS"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/09/14/text-orientation-upright/",
    title: "CSSでtext-orientationは使ってはいけない",
    date: "2019-09-14T00:00:00Z",
    body: "CSSでtext-orientationは使ってはいけない ちょっと釣りっぽいタイトルですが ちょっと困った事があったので書いておきます。 やった事 縦書きの文書(英数字の向きは縦)を書くことがあったので 以下のようにtext-orientationを使って実装しました。 CSS div { width: 30px; height: 290px; margin: 20px auto; } p { padding: 20px; border: 2px solid #000; -webkit-writing-mode: vertical-rl; /* 縦書きにする */ -ms-writing-mode: tb-rl; /* 縦書きにする */ writing-mode: vertical-rl; /* 縦書きにする */ text-orientation: upright; /* 英数字の向きを縦にする */ } HTML \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;僕にもし子供が産まれたら\u0026lt;br/\u0026gt;世界で2番目に好きだと話そう\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Chrome等で確認すると想定通りになりました。 これで満足していたのですが、実はtext-orientationはIEでは対応しておらず 数字が横向きになってしまいました。(2019年9月現在) 対応方法 text-combine-uprightというスタイルがあるので これを使って数字部分を縦向きにします。 変更後のソースは以下のようにします。 CSS div { width: 30px; height: 290px; margin: 20px auto; } p { padding: 20px; border: 2px solid #000; -webkit-writing-mode: vertical-rl; /* 縦書きにする */ -ms-writing-mode: tb-rl; /* 縦書きにする */ writing-mode: vertical-rl; /* 縦書きにする */ } span { font-family: \u0026#34;メイリオ\u0026#34;; -webkit-text-combine: horizontal; /* 英数字の向きを縦にする */ -ms-text-combine-horizontal: all; /* 英数字の向きを縦にする */ text-combine-upright: all; /* 英数字の向きを縦にする */ } HTML \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;僕にもし子供が産まれたら\u0026lt;br/\u0026gt;世界で\u0026lt;span\u0026gt;2\u0026lt;/span\u0026gt;番目に好きだと話そう\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; これでIEでも数字が縦向きになりました。 気をつけなければならないのは、text-combine-uprightで囲った範囲は横に並びます。 つまり、上記のspanの中を複数文字にしてしまうと以下のようになります。 HTML \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;僕にもし子供が産まれたら\u0026lt;br/\u0026gt;世界で\u0026lt;span\u0026gt;123\u0026lt;/span\u0026gt;番目に好きだと話そう\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; 結果 複数の英数字を縦向きにして縦に並べたい場合は 1文字ずつtext-combine-uprightで囲まないといけないので面倒です。 早くIEにtext-orientationしてもらいたいです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/08/26/wp-media-infomation/",
    title: "【Wordpress】画像ファイルに設定されたタイトルとキャプションを削除する",
    date: "2019-08-26T00:00:00Z",
    body: "【Wordpress】画像ファイルに設定されたタイトルとキャプションを削除する 環境 WordPress 4.8.3 やりたい事 Wordpressのメディアに画像ファイルをアップロードする際 画像のタイトルは基本的には画像のファイル名となりますが アップする画像ファイルによってはタイトルやキャプションが設定されている事があります。 例えば、オリンパスのデジカメで撮った写真だと 全ての画像のタイトルとキャプションが『OLYMPUS DIGITAL CAMERA』になる事があり このままでは管理がしにくく、都度修正するのも手間です。 そこで、画像ファイルに設定されている値は無視するようにします。 (WP上ではタイトル＝ファイル名となる) 対応方法 function.php に以下の記載をするだけです。 function delete_media_title_caption( $img_meta ) { $img_meta[\u0026#39;title\u0026#39;] = \u0026#39;\u0026#39;; $img_meta[\u0026#39;caption\u0026#39;] = \u0026#39;\u0026#39;; return $img_meta; } add_filter( \u0026#39;wp_read_image_metadata\u0026#39;, \u0026#39;delete_media_title_caption\u0026#39; ); これで画像アップ時にファイルに設定されていたタイトルとキャプションは無視されます。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/docker/",
    title: "Docker",
    date: "2019-08-15T00:00:00Z",
    body: "Docker"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/08/15/ssh-portfowarding/",
    title: "SSHポートフォワーディングでローカルから外部サーバーへアクセスする",
    date: "2019-08-15T00:00:00Z",
    body: "SSHポートフォワーディングでローカルから外部サーバーへアクセスする 環境 amazonlinux:2017.03-with-sources Dockerコンテナのベースイメージです。 sshクライアント用のソフトはyumでインストールしてあるとします。 やりたい事 ローカル環境(Docker)で動かしているアプリケーションから アクセス制限のかかっているAPIサーバーやDBサーバーへアクセスしたかったので SSHポートフォワーディングを使って実現しました。 SSHポートフォワーディングを使えば、自分が特定のポートで受けた通信を特定のサーバー(SSHで入れる事が条件)を経由して別のサーバーへ受け流す事ができるようになります。 コマンドは以下のように実行します。 ssh -f -N -L {自分が待ち受けるポート}:{アクセスしたいサーバーのIPやホスト名}:{アクセスしたいサーバーのポート} -i {SSHするための秘密鍵のパス} {SSH先のユーザー}@{SSH先のサーバー} -p {SSHポート} -4 (sshのオプション説明は調べればすぐ出てくるので割愛します) 例えば以下のようにコマンドを打つと 自分が10080ポートで受けた通信はserver2経由でserver1の80ポートに流して、そのレスポンスをそのままリクエスト元に戻す という意味になります。 なお、server2はポート22でec2-userユーザーにSSH接続できるものとします。(秘密鍵認証) ssh -f -N -L 10080:server1:80 -i ~/.ssh/id_rsa ec2-user@server2 -p 22 -4 手順 ①純粋なポートフォワーディングを使う 下図のような通信イメージです。 WEBアプリのコンテナにてSSHポートフォワーディングを実行。 WEBアプリから自身の3306ポートへ通信が発生したらポートフォワーディング発動し、 ssh.server.com経由でdb.server.com:3306へアクセスする。 WEBアプリから自身の10443ポートへ通信が発生したらポートフォワーディング発動し、 ssh.server.com経由でapi.server.com:443へアクセスする。 WEBアプリが動いているDockerコンテナ内にてSSHポートフォワーディングのコマンドを実行します。 ssh -f -N -L 3306:db.server.com:3306 -L 10443:api.server.com:443 -i /path/to/secret_key user@ssh.server.com -p 22 -4 あとはアプリケーション内で以下の箇所を変更すればポートフォワーディングでアクセスできるようになります。 ・「db.server.com:3306」の通信箇所を「localhost:3306」へ通信するよう変更 ・「api.server.com:443」の通信箇所を「localhost:10443」へ通信するよう変更 ②踏み台コンテナを利用してポートフォワーディングする もし上記①のアプリケーション変更箇所がソースコードにハードコーディングされている場合など、 変更が困難な場合はこちらの手順で対応します。 下図のような通信イメージです。 踏み台コンテナにてSSHポートフォワーディングを実行。 WEBアプリから踏み台コンテナの3306ポートへ通信が発生したらポートフォワーディング発動し、 ssh.server.com経由でdb.server.com:3306へアクセスする。 WEBアプリから踏み台コンテナの443ポートへ通信が発生したらポートフォワーディング発動し、 ssh.server.com経由でapi.server.com:443へアクセスする。 踏み台専用のDockerコンテナを作成し、コンテナの実行コマンドとしてSSHポートフォワーディングのコマンドを実行するようにします。 ※プロセスがバックグラウンドに行かないように、fオプションを外しています。 ssh -N -L 0.0.0.0:3306:db.server.com:3306 -L 0.0.0.0:443:api.server.com:443 -i /path/to/secret_key user@ssh.server.com -p 22 -4 あとはWEBアプリが動いているコンテナのhostsに 『db.server.com』と『api.server.com』が踏み台コンテナのIPアドレスとなるように記載します。 踏み台コンテナのIPアドレスは以下のコマンドで調べられます。 nslookup {踏み台コンテナ名} | grep Address | tail -n +2 | cut -f2 -d \u0026#39; \u0026#39; シェルスクリプトにするとこんな感じになります。 #!/bin/bash -eu HUMIDAI_IP=`nslookup humidai | grep Address | tail -n +2 | cut -f2 -d \u0026#39; \u0026#39;` echo \u0026#34;${HUMIDAI_IP}\tdb.server.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts echo \u0026#34;${HUMIDAI_IP}\tapi.server.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts これでSSHポートフォワーディングを使った通信の設定は完了です。 補足 ポートフォワーディングを使ってアクセスしたいサーバーがDBサーバーだけであれば ①の手順のコマンドを実行後、②のようにhostsを記載してdb.server.comを127.0.0.1(自身)にしてしまえば良いです。 しかし、APIサーバーのようにWEBアプリと同じポート(443)への通信をポートフォワーディングしたい場合、 既にWEBアプリで使っているポートをSSHポートフォワーディングでも待ち受ける事はできないので、 ②の手順のような踏み台コンテナが必要となります。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/vpn/",
    title: "VPN",
    date: "2019-08-02T00:00:00Z",
    body: "VPN"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/08/02/vpn-accident/",
    title: "フリーWi-FiでVPN通信がうまくいかない",
    date: "2019-08-02T00:00:00Z",
    body: "フリーWi-FiでVPN通信がうまくいかない ※注意 起こった事象と考察をメモしているだけです。 本当の原因や最適な解決方法は分かっていません。 事象 PCを喫茶店や漫画喫茶、建物のフリーラウンジなどで公開されているフリーWi-Fiにつなぎ、 そこからVPN(L2TP/IPsec)に繋いだところ一部環境に接続できないという状況になりました。 具体的には以下のような状態でした。 [OK] VPN接続は成功している。 [OK] グローバルIPでの接続は問題なさそう。 curl inet-ip.info コマンドでVPN接続後のIPになっていることを確認。 グローバルIPで穴あけしているプライベートサイトも表示できている。 [NG] プライベートIPでの接続ができない。 VPNサーバーからプライベートIPで接続できるサーバーへSSH接続できない。 原因の推察 不特定多数の人が使うWi-Fiの場合セキュリティ的に不安があるので Wi-Fi接続者同時が通信の傍受などをしないようにルーター側で プライベートネットワークを変更するような処理を入れているのかもしれません。 対策 対策らしい対策では無いですが、フリーWi-Fiを使うのをやめるしかなさそうです。 モバイルWi-Fiを契約して持ち歩いたり、スマホのテザリングを使って接続すればVPN接続はうまくいきました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/07/29/linux-mint-vpn/",
    title: "コマンドラインからVPNの接続＆切断をする",
    date: "2019-07-29T00:00:00Z",
    body: "コマンドラインからVPNの接続＆切断をする 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) Linux MintはGUIにてVPNの設定ができ、VPNの接続＆切断もGUIから行なえます。 しかし、ターミナル操作中などコマンドからVPNを操作したいときがあるので コマンドを調べました。 VPN名確認 自分でつけた名前なので調べる必要は無いかもしれませんが 一応以下コマンドで調べられます。 (以下例だと『SampleVPN』がVPN名) $ nmcli connection | grep vpn SampleVPN 123abc12-1a2b-1234-12a1-1234567890ab vpn -- VPN接続 以下コマンドでVPN接続します。 nmcli con up id {VPN名} # 例： nmcli con up id SampleVPN VPN切断 以下コマンドでVPN切断します。 nmcli con down id {VPN名} # 例： nmcli con down id SampleVPN 短縮コマンド登録 エイリアスを登録しておくと楽です。 alias up-vpn=\u0026#34;nmcli con up id SampleVPN\u0026#34; alias down-vpn=\u0026#34;nmcli con down id SampleVPN\u0026#34;"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/07/25/lightsail-vpn/",
    title: "LightsailにOpenVPNサーバーを立てる",
    date: "2019-07-25T00:00:00Z",
    body: "LightsailにOpenVPNサーバーを立てる 環境 Amazon Linux AMI release 2018.03 openvpn 2.4.4 参考 LightsailでVPN Serverを構築する|memoTech AmazonLinuxにOpenVPNサーバを構築する|ねこの足跡R 国内のVPNサービスを契約していたのですが 同時に1デバイスしか接続できないし、回線がめっちゃ遅かったので 自前でVPNサーバーを立てることにしました。 AWSのLightsail上にOpenVPNでサーバーを立てて Linux Mint, Android, iPhoneで接続できるようにします。 インスタンス設定 新規インスタンス作成 Lightsailのページにて『インスタンスの作成』をクリックします。 https://lightsail.aws.amazon.com/ls/webapp/home/instances インスタンスイメージとして以下を選択します。 Lunux/Unix OSのみ Amazon Linux インスタンスプランはVPNの利用予定に合わせて選びます。 個人でちょっと使う程度であれば最安のプランでよいと思います。 インスタンス名を適当につけて「インスタンスを作成」をクリックします。 静的IPの設定 インスタンス一覧画面に戻るので 先程作成したインスタンスの「︙」から『管理』をクリックします。 ネットワーキングタブを開き、「静的IPの作成」をクリックします。 静的IPの設定画面になるので、適用するインスタンス（今作ったインスタンス）と 静的IPの名前を決めて「作成」をクリックします。 ポート開放 対象インスタンスの『ネットワーキング』タブを開きます。 ファイアウォールの「＋追加」をクリックし、TCPの1194ポートを開きます。 Lightsailへの接続 インスタンス一覧にて再度「︙」から『接続』をクリックします。 別ウィンドウでターミナル調のブラウザが立ち上がるので、以降の作業を実施します。 手元のターミナルからsshで繋いでも良いです。 Lightsailの時刻を日本時間に合わせる 念の為タイムゾーンを日本にしておきます。 sudo cp -p /usr/share/zoneinfo/Japan /etc/localtime sudo service rsyslog restart sudo service crond restart OpenVPN設定 OpenVPNインストール 以下コマンドを実行します。 sudo yum install openvpn 認証局の作成 以下コマンドを実行します。 wget https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.4/EasyRSA-3.0.4.tgz tar -xf EasyRSA-3.0.4.tgz sudo mv EasyRSA-3.0.4 /usr/local/EasyRSA cd /usr/local/EasyRSA/ ./easyrsa init-pki ./easyrsa build-ca 実行後に以下の質問をされるので入力してください。 入力項目 入力内容 Enter PEM pass phase: 適当なパスフレーズ Verifying - Enter PEM pass phrase: 上と同じもの Common Name (eg: your user, host, or server name) [Easy-RSA CA]: 適当なドメイン(例：xxxxx.com) DHパラメータを生成 ./easyrsa gen-dh サーバー用の証明書＆秘密鍵を生成 ./easyrsa build-server-full server nopass 実行後にパスフレーズを聞かれるので、さきほど設定したパスフレーズを入力してください。 クライアント用の証明書＆秘密鍵を生成 以下コマンドを接続する利用者分実施します。 同じ利用者でも、PCとスマホの両方から同時接続するのであれば別クライアントとして証明書＆秘密鍵を生成する必要があります。 ./easyrsa build-client-full {クライアント名} nopass #例） ./easyrsa build-client-full client1 nopass 実行後にパスフレーズを聞かれるので、さきほど設定したパスフレーズを入力してください。 クライアント名は接続者が識別できるような任意の名前にしてください。 VPNサーバーのIPアドレス＆ネットマスクの確認 以下のコマンドを実行します。 ip a この出力結果のeth0のinetを確認しておく。 （以下のxxx.xxx.xxx.xxx/xxの部分） 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9001 qdisc pfifo_fast state UP group default qlen 1000 link/ether 06:7a:be:17:01:aa brd ff:ff:ff:ff:ff:ff inet xxx.xxx.xxx.xxx/xx brd 172.26.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::47a:beff:fe17:1aa/64 scope link valid_lft forever preferred_lft forever 設定ファイル修正 生成したサーバー用ファイルをOpenVPNのディレクトリへコピーします。 sudo cp pki/ca.crt /etc/openvpn/. sudo cp pki/issued/server.crt /etc/openvpn/. sudo cp pki/private/server.key /etc/openvpn/. sudo cp pki/dh.pem /etc/openvpn/dh2048.pem ※dhだけファイル名を変更しています 次に設定ファイルを編集します。 sudo cp /usr/share/doc/openvpn-2.4.4/sample/sample-config-files/server.conf /etc/openvpn/server.conf sudo vim /etc/openvpn/server.conf 編集後の設定は以下のようになります。 push \u0026quot;route xxx.xxx.xxx.xxx yyy.yyy.yyy.yyy\u0026quot; の部分は さきほど確認したVPNサーバーのIPアドレス＆サブネットマスクにします。 $ grep -v \u0026#34;^$\u0026#34; /etc/openvpn/server.conf | grep -v \u0026#34;^#\u0026#34; | grep -v \u0026#34;^;\u0026#34; port 1194 proto tcp dev tun ca ca.crt cert server.crt key server.key # This file should be kept secret dh dh2048.pem server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34; push \u0026#34;route xxx.xxx.xxx.xxx yyy.yyy.yyy.yyy\u0026#34; push \u0026#34;dhcp-option DNS 208.67.222.222\u0026#34; push \u0026#34;dhcp-option DNS 208.67.220.220\u0026#34; keepalive 10 120 tls-auth ta.key 0 # This file is secret cipher AES-256-CBC persist-key persist-tun status /var/log/openvpn/openvpn-status.log log /var/log/openvpn/openvpn.log log-append /var/log/openvpn/openvpn.log verb 3 explicit-exit-notify 0 元のサンプルファイルとの差分は以下のようになっています $ diff /usr/share/doc/openvpn-2.4.4/sample/sample-config-files/server.conf /etc/openvpn/server.conf 35,36c35,36 \u0026lt; ;proto tcp \u0026lt; proto udp --- \u0026gt; proto tcp \u0026gt; ;proto udp 192c192,193 \u0026lt; ;push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34; --- \u0026gt; push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34; \u0026gt; push \u0026#34;route xxx.xxx.xxx.xxx yyy.yyy.yyy.yyy\u0026#34; 200,201c201,202 \u0026lt; ;push \u0026#34;dhcp-option DNS 208.67.222.222\u0026#34; \u0026lt; ;push \u0026#34;dhcp-option DNS 208.67.220.220\u0026#34; --- \u0026gt; push \u0026#34;dhcp-option DNS 208.67.222.222\u0026#34; \u0026gt; push \u0026#34;dhcp-option DNS 208.67.220.220\u0026#34; 287c288 \u0026lt; status openvpn-status.log --- \u0026gt; status /var/log/openvpn/openvpn-status.log 296,297c297,298 \u0026lt; ;log openvpn.log \u0026lt; ;log-append openvpn.log --- \u0026gt; log /var/log/openvpn/openvpn.log \u0026gt; log-append /var/log/openvpn/openvpn.log 315c316 \u0026lt; explicit-exit-notify 1 --- \u0026gt; explicit-exit-notify 0 設定したログ出力ディレクトリも作成しておきます。 sudo mkdir /var/log/openvpn sudo chmod 0600 /var/log/openvpn ta.keyの作成 sudo openvpn --genkey --secret /etc/openvpn/ta.key パケット転送を有効にする sudo sed -i -e \u0026#34;s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g\u0026#34; /etc/sysctl.conf sudo sysctl -p ルーティングを変更する sudo iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE sudo service iptables save OpenVPNを起動 sudo chkconfig openvpn on sudo service openvpn start クライアントの設定 VPNサーバーで生成したファイルのうち、クライアントの設定用に必要なファイルは以下4点です。 /usr/local/EasyRSA/pki/ca.crt /usr/local/EasyRSA/pki/issued/{クライアント名}.crt /usr/local/EasyRSA/pki/private/{クライアント名}.key /etc/openvpn/ta.key また、AndroidやiPhoneで設定するには拡張子ovpnのファイルが必要なので作成しておく。 (hogehoge.ovpn) ovpnファイルに記載する内容は以下の通りです。 証明書や鍵の情報はovpnファイルに埋め込んでしまった方が便利なので、埋め込んでしまいます。 ※xxx.xxx.xxx.xxxはVPNサーバーのグローバルIPアドレスに置き換えてください。 client dev tun proto tcp remote xxx.xxx.xxx.xxx 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server key-direction 1 cipher AES-256-CBC verb 3 \u0026lt;ca\u0026gt; 〜上記ca.crtの中身をそのまま貼り付ける〜 \u0026lt;/ca\u0026gt; \u0026lt;key\u0026gt; 〜上記{クライアント名}.keyの中身をそのまま貼り付ける〜 \u0026lt;/key\u0026gt; \u0026lt;cert\u0026gt; 〜上記{クライアント名}.crtの中身をそのまま貼り付ける〜 \u0026lt;/cert\u0026gt; \u0026lt;tls-auth\u0026gt; 〜上記ta.keyの中身をそのまま貼り付ける〜 \u0026lt;/tls-auth\u0026gt; Linux Mint 標準でネットワーク設定画面よりOpenVPNの設定ができるので そこで上記ファイルの設定をすれば接続できるようになります。 設定画面のキャプチャを貼っておきます。 Android ovpnファイルをAndroid端末にメール添付などで送付しておきます。 GooglePlayにてOpenVPNアプリをインストールします。 https://play.google.com/store/apps/details?id=net.openvpn.openvpn インストールしたOVPNアプリを開き、『OVPN Profile』を開きます。 するとovpnファイルの選択画面になるので、ovpnファイルを選択して「IMPORT」をタップします。 次の画面で「ADD」をタップします。 これで設定は完了です。 作成された接続のスイッチをONにすればVPN接続が完了します。 iPhone ovpnファイルをiPhone端末にメール添付などで送付しておきます。 AppStoreにてOpenVPNアプリをインストールします。 https://apps.apple.com/jp/app/openvpn-connect/id590379981 メーラーにて送付してあるovpnファイルをダウンロードして、そのまま開きます。 すると開き方の選択肢にインストールしたOpenVPNアプリがあるので、それを選択します。 OpenVPNアプリが開き、さきほどダウンロードしたovpnファイルが表示されるので「ADD」をタップします。 次の画面でも「ADD」をタップします。 警告ポップアップが出ますが「Allow」をタップして進んでください。 これで設定は完了です。 作成された接続のスイッチをONにすればVPN接続が完了します。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/07/16/withholding-income-tax/",
    title: "e-taxで源泉所得税を申請する。",
    date: "2019-07-16T00:00:00Z",
    body: "e-taxで源泉所得税を申請する。 e-taxにて源泉所得税の申請をしたので、その手順メモです。 税務署に行って紙で源泉所得税の申請ができる程度の基礎知識はある前提とします。 申請書の作成 e-taxの左メニューから作成＞申告・申請等開き、新規作成をクリックします。 次に以下を選び『次へ』をクリックします。 手続きの種類：申請・届出 税目：源泉所得税 次に以下をチェックを入れて『次へ』をクリックします。 徴収高計算書関係 給与所得・退職所得等の所得税徴収高計算書（※） 給与所得・退職所得等の所得税徴収高計算書（※） ※所得税の納付を毎月してる会社は「一般」、半年分まとめて納税してる会社は「納期特例分」です 申告・申請等名を指定する画面になるので、自分で分かる程度の名前をつけて『OK』をクリックします。 （「XXXX年度〇期_源泉所得税」など） 次に基本情報の入力画面になるので、以下を入力or確認して『OK』をクリックします。 提出税務署：申請する税務署（自分の会社がある自治体） 提出年月日：今の日付 法人名(カナ)：自分の会社名カナ 法人名（必須）：自分の法人名 納税地：自分の会社の住所と電話番号 代表者名（カナ）： 代表者名（必須）： 代表者：代表者の住所電話番号 金融機関：会社の銀行口座 帳票一覧が開くので、さきほど選択した帳票をダブルクリックで開きます。 すると帳票の中身が修正できるようになるので記載します。 小さくて文字が見づらいですが、右クリックすると拡大メニューが出ます。 基本的に「いつの期間に」「何人に」「いくら支給して」「いくら源泉徴収したか」を書くだけです。 （右端の「納期等の区分」(名前は変わるかも)にも「いつの期間に」を記載しなければならないので要注意） 公式の説明はこちら。 http://www.e-tax.nta.go.jp/tetsuzuki/tetsuzuki4_4.htm 記載が終わったら「作成完了」をクリックします。 控えが必要ならここで印刷などしておきます。 申請書の送信 e-tax左メニューの「送信可能一覧へ」＞「送信」を開きます。 先ほど作成した帳票が表示されるので、選択して『送信』をクリックします。 何か問題があればエラーが表示されるので、エラー指摘箇所を訂正して再度送信します。 源泉所得税の支払い 送信が完了するとメッセージボックスに受付完了のメッセージが届き、 そのメッセージの下のボタンから払込ができます。 オンラインで払う場合 クレジットカード払いは手数料が取られるので インターネットバンキングの方が良いです。 これで源泉所得税の申請は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/07/02/exiftool-modify-alldate/",
    title: "コマンドで画像の撮影日等を変更する",
    date: "2019-07-02T00:00:00Z",
    body: "コマンドで画像の撮影日等を変更する 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) exiftool 10.80 写真をスキャナを使って取り込むと画像ファイルに撮影日が反映されないので exiftoolというEXIF情報を扱えるツールを使って撮影日を変更します。 ※この手順ではEXIF情報を無理やり書き込んでいるだけなので、 ファイルフォーマットがEXIF形式(?)に変わるわけではなさそうです。 まずexiftoolがインストールされていない場合はインストールします。 apt install exiftool 現状のEXIF情報を参照するには以下のようにコマンドを実行します。 下記コマンドだとカレントディレクトリにあるpngファイル全てのEXIF情報を参照できます。 exiftool -s ./*png (./*pngの部分は画像ファイルが置いてあるパスに読み替えてください) 続いて撮影日付等を書き換えます。 以下のコマンドで撮影日時等を「2019/02/23 00:00:00」に変更しています。 exiftool -overwrite_original -alldates=\u0026#39;2019:02:23 00:00:00+09:00\u0026#39; ./*png exiftool -overwrite_original -TimeStamp=\u0026#39;2019:02:23 00:00:00+09:00\u0026#39; ./*png ※overwrite_originalを指定する事で上書きするようにしているのですが もし書換え前のデータを残したい場合はこのオプションを外してください。 次にファイルの更新日も修正してしまいます。 ファイルの更新日はどうでも良い人は飛ばして良いです。 exiftool -overwrite_original -FileModifyDate=\u0026#39;2019:02:23 00:00:00+09:00\u0026#39; ./*png 再度EXIF情報を確認すると、日付データが更新されている事が分かります。 exiftool -s ./*png"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/04/17/linuxmint-access-point/",
    title: "Linux Mintを無線LANのアクセスポイントにする",
    date: "2019-04-17T00:00:00Z",
    body: "Linux Mintを無線LANのアクセスポイントにする 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) VPN経由でないと表示できないページをPC, Android, iOSで同時に見たかったのですが 自分の契約しているVPNが1度に1デバイスしか接続できなかったので PC(Linux Mint)でVPN接続し、その接続を無線LANで共有するようにしました。 Macでいうところの「ネットワークの共有」です。 参考 Linuxで簡単に無線Lanアクセスポイントを作る だいたい参考ページ通りですが、うまく動かない箇所があったので少しだけコマンドを修正しています。 環境構築 必要なソフトウェアのインストールします。 apt install util-linux procps hostapd iproute2 iw アクセスポイントを作成するスクリプトの取得します。 wget https://raw.githubusercontent.com/oblique/create_ap/master/create_ap 実行権限を付与してパスが通っている場所に移動させます。 sudo chmod +x create_ap sudo mv create_ap /usr/local/bin/. ネットワークの共有 事前にネットワークインターフェース(NIC)を調べます。 ip a すごいざっくりいうと eで始まるNICは有線LANで、wで始まるNICは無線LANです。 また、VPNの場合はVPNの種類(pppなど)が付いていると思います。 loやbrで始まるのは内部的に使っているものなので基本的に無視して良いです。 分からなければ『コントロールセンター＞ネットワーク接続』にて利用したいNICの詳細を開き、 デバイスの「XX:XX:XX:XX:XX:XX」が一致するNICを選べば良いです。 このNICの中から『外界⇔PC』通信用のNICと『PC⇔スマホ』通信用のNICを決めます。 注意点としては上記2つのNICは物理的に別のハードウェアでなければならないという事です。 例えば、PCに無線LANのNICが1つしか無い場合は『PC⇔スマホ』のNICにそれを使うので、 『外界⇔PC』のNICは有線LANを使ったりしなければなりません。 利用するNICが決まったら以下の用にコマンドを打ちます。 sudo create_ap {スマホとのNIC} {外界とのNIC} {SSID} {SSIDの接続パスワード} たとえば、スマホとのNICが無線LANwlp3s0で、外界とのNICがVPNppp0の場合は以下のコマンドでアクセスポイントを開始できます。 なお、以下コマンドではSSIDはMyAPでパスワードはMyPasswordとなります。 sudo create_ap wlp3s0 ppp0 MyAP MyPassword 上記コマンド実行後、ターミナルにはアクセスポイントとしての稼働ログが流れます。 アクセスポイントを閉じたい場合はCtrl + cを押します。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/04/10/javascript-event/",
    title: "IME有効時の入力イベントを調べる",
    date: "2019-04-10T00:00:00Z",
    body: "IME有効時の入力イベントを調べる PC/Android/iOSでIME有効時に入力イベントが結構違ったので調べました。 調べた環境は以下の通りです。 OS ブラウザ PC (Linux Mint 19.1 MATE) Chrome Ver.73.0 Android 8.0.0 (Galaxy S8) Chrome Ver.73.0 iOS 12.2 (iPhone 6) Safari 12.1 テキストボックスに文字を入力した際のイベントを調べました。 調べたイベントはkeydown, keyup, keypress, textInput, inputです。 （Androidはブラウザ以外にもキーボードアプリも影響あるのかな？） IME有効 入力中（未確定） ひらがなで文字を1文字ずつ入力している時のイベントです。 イベント PC Android iOS 取れる値 keydown ○ ○ 今回入力前の文字列 keyup ○ ○ ○ ※OSにより違う keypress ○ 今回入力前の文字列 textInput input ○ ○ 今回入力後の文字列 ※keyupで取れる値は以下の通りでした。 PC: 前回確定済みの文字列 スマホ: 今回入力後の文字列 Androidは2文字目以降はなぜか上記イベントが2周走りました。 しかも2周目はkeydownでも今回入力後の文字列が取れてます。 IME有効 確定時 入力した文字を確定した時のイベントです。 PCであればEnterキーを押したタイミングの事ですね。 なお、スマホでは半角英数を入力していても確定処理はあり、 挙動は以下と同じでした。 イベント PC Android iOS 取れる値 keydown ○ ○ ※OSにより違う keyup ○ 今回確定後の文字列 keypress textInput ○ ○ ○ ※OSにより違う input ○ ○ ○ 今回確定後の文字列 ※keydownで取れる値は以下の通りでした。 PC: 前回確定済みの文字列 Android: 今回確定後の文字列 ※textInputで取れる値は以下の通りでした。 PC, iOS: 前回確定済みの文字列 Android: 今回確定後の文字列 iOSはなぜかinputイベントが3回走ります。 しかも、うち1回は前回確定済みの文字列がとれ、 2回は今回確定後の文字列が取れていました。 IME無効 入力時 IMEを無効にして半角英数字などを入力した時のイベントです。 スマホではIMEという概念がないので、IME有効時で書いた結果と同じです。 ただし、Androidのイベントが2周走る件は発生しません。 イベント PC Android iOS 取れる値 keydown ○ ○ ○ 入力する前の文字列 keyup ○ ○ ○ 入力した後の文字列 keypress ○ ○ 入力する前の文字列 textInput ○ 入力する前の文字列 input ○ ○ ○ 入力した後の文字列 文字削除時 BackspaceやDeleteで文字を削除した際のイベントです。 これはOSによる差分はなさそうです。 イベント PC Android iOS 取れる値 keydown ○ ○ ○ 削除する前の文字列 keyup ○ ○ ○ 削除した後の文字列 keypress textInput input ○ ○ ○ 削除した後の文字列 文字列切り取り(カット)時のイベント PCだと右クリック、スマホだと長押しして文字列の切り取りをした時のイベントです。 こちらもOSによる差分はなさそうです。 なお、PCでCtrl+xで文字を切り取った場合は 文字削除時の挙動と同じでした。 (たぶんキー入力による文字削除という点で同じなのかと) イベント PC Android iOS 取れる値 keydown keyup keypress textInput input ○ ○ ○ 削除した後の文字列 文字列貼り付け(ペースト)時のイベント PCだと右クリック、スマホだと長押しして 文字列を貼り付けした時のイベントです。 こちらもOSによる差分はなさそうです。 イベント PC Android iOS 取れる値 keydown keyup keypress textInput ○ ○ ○ 貼り付けする前の文字列 input ○ ○ ○ 貼り付けした後の文字列 なお、PCでCtrl+vで貼り付けした場合は以下でした。 イベント PC 取れる値 keydown ○ 貼り付けする前の文字列 keyup ○ 貼り付けした後の文字列 keypress textInput ○ 貼り付けする前の文字列 input ○ 貼り付けした後の文字列 総評 レスポンシブなサイトを作るのであれば、OSにより差分のあるイベントはフックに使わない方が安全かもしれません。 使うのであれば差分をしっかり把握した上で使わないと事故りそうです。 取れる値が各イベント操作前の状態だったり操作後の状態だったりしますが このへんはsetTimeoutをうまく使えば全て操作後の状態を取れる気がします。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/13/linux-mint-l2tp/",
    title: "VPNクライアント設定(L2TP over IPsec)",
    date: "2019-03-13T00:00:00Z",
    body: "VPNクライアント設定(L2TP over IPsec) 環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) ubuntu 19.10 ■参考 ask ubuntu Linux MintでのVPNクライアント設定に少し苦戦したのでメモです。 ubuntuでもほぼ同手順で対応可能でした。 必要なアプリケーションのインストール 以下のコマンドでインストールします。 (ubuntuの場合はaptの前にsudoを付けてください) $ apt update $ apt install network-manager-l2tp-gnome strongswan libstrongswan-extra-plugins libcharon-extra-plugins ネットワーク設定 まず、『コントロールセンター＞ネットワーク接続』を開きます。 (ubuntuの場合は『設定＞ネットワーク』) 現在のネットワーク一覧が表示されるので、「＋」マーク(追加ボタン)をクリックします。 接続の種類を聞かれるのでVPNの中にある『Layer 2 Tunneling Protocol(L2TP)』を選択して『作成』をクリックします。 L2TPの設定画面が開くので、VPNタブにて以下を設定します。 項目 設定値 ゲートウェイ VPNサーバーのIPアドレスorホスト名 ユーザー名 VPN接続する際のユーザー名 パスワード 上記ユーザーのパスワード※ NTドメイン 空欄のまま ※補足 パスワードは入力欄右端の？アイコンをクリックしてポリシーを選択すれば入力できます。 パスワードを接続の都度入力する場合はここでは空欄で良いです。 次に『IPsec Settings』をクリックしてIPsecの設定をします。 以下を設定して『OK』をクリックします。 「Phase1 Algorithms」以降は『Advanced』をクリックしたら表示されます。 項目 設定値 Enable IPsec tunnel to L2TP host チェックする Gateway ID 空欄のまま Pre-shared key VPNサーバーの共有キーフレーズ Phase1 Algorithms VPNサーバーの暗号方式※ Phase2 Algorithms VPNサーバーの暗号方式サブ（あれば）※ Enforce UDP encapsulation 未チェックのまま ※補足 Algorithmsに何を指定するかはVPNサーバー側に確認してください。 私の場合は以下を設定したら繋がりました。 Phase1: 3des-sha1-modp1024! Phase2: 3des-sha1! このAlgorithms設定がうまく行ってなくて繋がらない人が多いようです。 VPN接続できない場合はこの設定をよく見直しましょう。 これでネットワーク設定は完了です。 他の記事を見るとPPPの設定もしている人が多いですが、必須ではなさそうです。 VPN接続 タスクバーのネットワークアイコンをクリックすると「VPN接続」のメニューがあります。 そこから作成したVPN接続を選択し、VPN接続 or 切断します。 接続に成功したらネットワークアイコンに鍵マークがつきます。 トラブルシューティング うまく繋がらない場合は以下のコマンドでログを確認することができます。 journalctl -f"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/10/linux-mint-terminator-fish/",
    title: "terminator + fish + fishermanの導入",
    date: "2019-03-10T00:00:00Z",
    body: "terminator + fish + fishermanの導入 ■環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) ubuntu 19.10 ※以降のaptコマンドはUbuntuの場合はsudoを付けてください。 terminatorインストール terminatorをインストールします。 apt install terminator (※2019年11月追記) Terminatorもいいですが、Tilixというターミナルも良いです。 詳しくは↓こちら↓ Linux用ターミナル『Tilix』 fishインストール fishをインストールします。 apt install fish インストール後、反映するため一度ログアウトします。 なお、Terminatorはデフォルトのウィンドウサイズが微妙なので変更した方が良いです。 Terminatorのデフォルトサイズを変更する fishをデフォルトシェルに設定 $ grep fish /etc/shells /usr/bin/fish ←表示されなかったら`which fish`の結果を/etc/shellsに追記する $ chsh -s /usr/bin/fish fishermanのインストール fishermanというfishのプラグイン管理ツールがあります。 fishermanのgithubベージにインストールコマンドが載っています。 https://github.com/fisherman/fisherman $ curl https://git.io/fisher --create-dirs -sLo ~/.config/fish/functions/fisher.fish $ fisher -v fisher version 3.1.1 ~/.config/fish/functions/fisher.fish ※fisher -vでバージョンが出ない場合は、一回ターミナルを開き直してください。 テーマの変更 fishはテーマを変更することで見栄えが変えられます。 どのようなテーマがあるかは、以下ページが参考になります。 https://github.com/oh-my-fish/oh-my-fish/blob/master/docs/Themes.md 私は「bobthefish」というテーマを利用しており、以下手順でインストールしています。 $ fisher install oh-my-fish/theme-bobthefish # fisherのバージョンによってはfisher installではなくfisher add bobthefishではPowerLineのフォントが必要なので、そちらもインストールします。 2020/09/02追記 windows terminalでは下記のコマンドでインストールしたフォントが上手く使えませんでした。 Cascadia Codeというフォントをインストールしてください。 https://docs.microsoft.com/ja-jp/windows/terminal/cascadia-code ダウンロードしたttfファイルを右クリックからインストールすると使えるようになります。 $ git clone https://github.com/powerline/fonts.git $ fonts/install.sh $ rm -rf fonts 上記コマンド実施後、ターミナルアプリの設定でインストールしたフォントを指定してください。 terminatorであればターミナル上で右クリックをして「設定」を開き、 「プロファイル＞一般＞Font」にてフォントを「○○○ for Powerline」のどれかに変更してください。 フォントを変更できない場合は「システムの固定幅フォントを使う」のチェックを外してください。 個人的には「Meslo LG M for Powerline」が好きです。 その他プラグイン fishermanを使って他にも様々なプラグインをインストールできます。 ■peco pecoをインストールします。 (ubuntuの場合はaptの前にsudoを付けてください) apt install peco fisher install oh-my-fish/plugin-peco インストール後、fishのコンフィグファイル(~/.config/fish/config.fish)へ以下のように記載します。 fishのコンフィグファイルが存在しない場合は新規作成してください。 function fish_user_key_bindings bind \\cr \u0026#39;peco_select_history (commandline -b)\u0026#39; # peco end 【使い方】 「Ctrl + R」でコマンド履歴検索画面が開きます。 検索画面では直近のコマンドから選択することもできますし、文字列を入力すれば該当の文字列を含むコマンド履歴に絞ることもできます。 ■ghq まずgolangをインストールします。 ※2020年7月追記 Linux Mint 20ではapt install golangだけで大丈夫です。 ※2020年3月追記 デフォルトでインストールできるgolangのバージョンはLinux Mintは1.10,Ubuntuは1.12なのですが(2020年3月現在) ver.1.12ではghqインストール時にエラーが出てしまうので新バージョンのgolangを取得できるようにします。 GitHubのgolangのwikiを見て作業します。 https://github.com/golang/go/wiki/Ubuntu 2020年3月現在の手順は以下の通りでした。 sudo add-apt-repository ppa:longsleep/golang-backports sudo apt update sudo apt install golang-go インストールが完了したらgoの設定をします。 echo \u0026#39;set -x GOPATH $HOME/go\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish echo \u0026#39;set -x PATH $GOPATH/bin $PATH\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish mkdir -p ~/go/bin source ~/.config/fish/config.fish 続いてghqをインストールします。 ghqのfisherman用リポジトリは元々yoshiori/fish-peco_select_ghq_repositoryなのですが そちらだとgitリポジトリ遷移時にシェルが多重で開くようになってしまったので シェルが多重で開かない版を自分のリポジトリ(mildjester/fish-peco_select_ghq_repository)に用意してあります。 ※yoshioriさんのソースに不具合があるのではなく、あるべき形になったらLinux系のシェル(?)と相性が悪くなっただけだと思います。 go get github.com/motemen/ghq fisher install mildjester/fish-peco_select_ghq_repository インストール後、fishのコンフィグファイル(~/.config/fish/config.fish)へ以下のように記載します。 上記のpecoの設定に追記する前提としています。 function fish_user_key_bindings bind \\cr \u0026#39;peco_select_history (commandline -b)\u0026#39; # peco bind \\c] peco_select_ghq_repository # ghq end 【使い方】 ghq get {gitリポジトリURL} コマンドで新規gitリポジトリをローカルに取得します。 (git cloneをghq getに置き換えるだけ) 「Ctrl + ]」でghqで管理しているgitリポジトリを選択してディレクトリへ移動できます。 デフォルトではghq getで取得したリポジトリは~/ghq配下に格納されるのですが ~/.gitconfigに以下を記載しておくと任意の階層に格納するよう変更できます。 [ghq] root = 好きなディレクトリ"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/10/linux-mint-terminator-size/",
    title: "Terminatorのデフォルトサイズを変更する",
    date: "2019-03-10T00:00:00Z",
    body: "Terminatorのデフォルトサイズを変更する ■環境 Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit) terminator 1.91 Linux環境でよく使われているターミナルソフトの「Terminator」ですが、 設定から起動時の縦横サイズを指定できなかったので無理やり設定しました。 設定方法 Terminatorのmanページを見ると以下のオプションがあることが分かります。 https://linux.die.net/man/1/terminator --geometry=GEOMETRY Specifies the preferred size and position of Terminator\u0026#39;s window; see X(7). x(7)のmanページを見るとgeometryの指定方法が記載されています。 https://linux.die.net/man/7/x -geometry WIDTHxHEIGHT+XOFF+YOFF (where WIDTH, HEIGHT, XOFF, and YOFF are numbers) これを利用して、Terminatorを起動するショートカット（ランチャー）にオプションを付与してしまい、 サイズと位置を指定するようにします。 設定例 例えばLinux Mintの場合はパネルにTerminatorを追加して右クリックからプロパティを開き、 コマンドの部分を以下のように変更してしまいます。 terminator --geometry=1400x800+200+200 これでパネルからTerminatorを起動すると 「横幅1400、高さ800、位置は上から200、左から200の場所」 という状態でウィンドウが開きます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/07/google-optimize-interactive/",
    title: "Google Optimizeでセッション情報が必要なページを触る",
    date: "2019-03-07T00:00:00Z",
    body: "Google Optimizeでセッション情報が必要なページを触る Google Optimizeは無料でABテストなどが行なえるツールです。 ログイン後のページなど、セッション情報が必要なページをGoogle Optimizeのエディタページに指定すると、 セッション情報がないのでエラーページや404ページが表示されてしまいます。 そんな時、エディタページの『インタラクティブモード』を使うと解決します。 手順 Google Optimizeの普通の使い方はできる前提とします。 手順①：エディタページの設定 まず、エディタページはOptimizeで触りたいページに至る前のセッション情報無しで開けるページを指定しておきます。 Optimizeで触りたいページそのものを指定しても良いですが、エディタを開いた時にまずエラーページになるので非効率だと思います。 手順②：Optimize対象のページへの移動 パターンを通常通り作成し、エディタの右上にある四角にカーソルがついているアイコンをクリックします。 するとインタラクティブモードになりサイトの操作ができるようになるので、Optimizeで開きたいページまで移動します。 該当ページを開いたらインタラクティブモードの『終了』をクリックします。 ※画面遷移時に勝手にインタラクティブモードが終了している場合もあります 手順③：エディットする セッション情報が不要なページと同様にページの編集をします。 この手順は特別な事はありません。 これでOptimizeの設定は完了です。 プレビュー時はエディタページに指定したURLからスタートするので、Optimizeで編集したページまで移動する必要があります。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/circleci/",
    title: "CircleCI",
    date: "2019-03-04T00:00:00Z",
    body: "CircleCI"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/03/04/aws-code-deploy-circleci/",
    title: "CircleCIからAWS Code Deployを実行する",
    date: "2019-03-04T00:00:00Z",
    body: "CircleCIからAWS Code Deployを実行する 前記事にてCodeDeployの設定は完了したので、それをCircleCIから実行します。 公式の手順ではS3に一度ソースコードを置いてからEC2に展開しているのですが、 できればS3は使いたくなかったのでこちらの記事を参考にしました。 CircleCI+AWS-CodedeployでStaging環境などに自動デプロイさせる IAM準備 CircleCI用のIAMユーザーを作成します。 まず適当なユーザー名をつけ、アクセスの種類は『プログラムによるアクセス』にチェックをつけ次のステップへ進みます。 既存ポリシーの『AWSCodeDeployFullAccess』を付与して次のステップへ進みます。 タグは不要なので、そのまま次のステップに進んで大丈夫です。 ユーザーの作成をします。 作成したユーザーの「アクセスキーID」と「シークレットアクセスキー」をメモして終了です。 ※シークレットアクセスキーはこのタイミングでしか確認できないので忘れないよう注意 CircleCIの設定ファイル作成 リポジトリ内に.circleci/config.ymlを作成し、以下のように記載します。 version: 2 jobs: deploy: docker: - image: cdssnc/aws-cli steps: - run: name: \u0026#34;Set AWS region\u0026#34; command: aws configure set region ap-northeast-1 - run: name: \u0026#34;Run AWS CodeDeploy\u0026#34; command: aws deploy create-deployment --application-name ${DEPLOY_APPLICATION} --deployment-group-name ${DEPLOY_GROUP} --github-location repository=\u0026#34;${DEPLOY_REPOSITORY}\u0026#34;,commitId=\u0026#34;${CIRCLE_SHA1}\u0026#34; workflows: version: 2 build_deploy: jobs: - deploy: filters: branches: only: master AWS CodeDeployをキックすることしか記載していないので、他にも処理が必要な場合は追記してください。 なお、以下の値は後ほどCircleCIの環境変数にて設定するので変数にしています。 変数にせずに直接config.ymlに書き込んでも良いです。 変数名 内容 DEPLOY_APPLICATION 前記事にて設定したCodeDeployのアプリケーション名 DEPLOY_GROUP 前記事にて設定したCodeDeployのデプロイグループ DEPLOY_REPOSITORY デプロイ対象のGitHubリポジトリ(「ユーザー名/リポジトリ名」形式) 作成できたらGitHubのmasterブランチにプッシュしておきます。 CircleCIの設定 まずCircleCIにさきほどのconfig.ymlに設定した環境変数を設定します。 CircleCIの基本的な設定はこちらを参照してください。 アカウント作成〜基本設定 環境変数の設定方法 次に作成したAWSのIAMユーザーのキーを設定します。 CircleCIのプロジェクトの設定画面を開き（プロジェクト画面で歯車ボタンをクリック） 『PERMISSIONS \u0026gt; AWS Permissions』を開くと設定できます。 これで設定は完了です。 masterブランチにマージorプッシュされるとCircleCIとCodeDeployが走って自動デプロイされます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/02/28/aws-code-deploy/",
    title: "AWS Code DeployでEC2にデプロイする",
    date: "2019-02-28T00:00:00Z",
    body: "AWS Code DeployでEC2にデプロイする 運用していたEC2へのソースコードを反映するのにCodeDeployを導入してみたので、その作業メモです。 とてもシンプルな状態で試したので、環境は以下の通りです。 EC2インスタンスは１台のみ ELBの利用なし ソースコードはGitHubより取得する デプロイはソースコードをサーバー上に置くのみ デプロイの実行はAWSのコンソールから手動で実行する。 ※CircleCI連携についてはコチラ IAM準備 EC2用とCodeDeploy用のロールを準備します。 まずEC2用のロールを作成します。 IAMロール作成画面にてAWSサービス ＞ EC2を選び、次のステップへ進みます。 ポリシーはAmazonEC2RoleforAWSCodeDeployを指定して、次のステップへ進みます。 タグは未指定でも良いので、必要なければ次のステップに進みます。 適当なロール名を決めて設定し、『ロールの作成』をクリックします。 次にCodeDeploy用のロールを作成します。 IAMロール作成画面にてAWSサービス ＞ CodeDeployを選びます。 画面が下へスクロールしてユースケースの選択が表示されます。 今回は純粋なEC2上のボリュームにデプロイするだけなので、CodeDeployを指定して次のステップへ進みます。 ポリシーはAWSCodeDeployRoleが表示されるので、そのまま次のステップへ進みます。 適当なロール名を決めて設定し、『ロールの作成』をクリックします。 EC2準備 まず、EC2インスタンスに管理コンソールからロール付与します。 管理コンソールにて対象インスタンスを開いて以下をクリックします。 アクション ＞ インスタンスの設定 ＞ IAMロールの割り当て/置換 そこで先ほど作成したEC2用のロールを設定します。 次にコンソールにて対象インスタンスにログインし、CodeDeployのエージェントをインストールします。 ダウンロードするインストーラーはrubyが無いと動かないので、インストールされていない場合はインストールしておきます。 $ sudo yum install ruby # rubyがインストールされてない場合 $ sudo yum install aws-cli # aws-cliがインストールされていない場合 $ aws s3 cp s3://aws-codedeploy-ap-northeast-1/latest/install . --region ap-northeast-1 $ chmod +x ./install $ sudo ./install auto インストールが完了したら念の為動作している事を確認する。 $ sudo service codedeploy-agent status The AWS CodeDeploy agent is running as PID 12345 もし上記のロール割り当てより先にCodeDeployエージェントを起動してしまった場合は、 ロール割り当て後にCodeDeployエージェントを再起動すれば大丈夫です。 $ sudo service codedeploy-agent restart リポジトリ準備 デプロイ対象のgitリポジトリのTOP階層にappspec.ymlというファイルを作成し、以下のように記載します。 version: 0.0 os: linux files: - source: ./ destination: /usr/share/nginx/html これでリポジトリ内のsourceで指定した階層(上例ではリポジトリ全て)をdestinationにデプロイするようになります。 sourceとdestinationの組み合わせは複数記載することもでき、デプロイ前後に処理を追加する事も可能です。 詳しくはAWS公式ページを参照してください。 これをGitHubにプッシュしておきます。 CodeDeployの設定 まず、アプリケーションの作成をクリックします。 アプリケーション名を決めて入力し、コンピューティングプラットフォームは『EC2/オンプレミス』を選択して 『アプリケーションの作成』をクリックします。 次にアプリケーションに登録するデプロイグループを設定します。 （デプロイグループ：デプロイする際に対象となるEC2インスタンス群などをグルーピングしておく。） グルーピングと言いながら、今回はEC2インスタンス１つだけを登録します。 まず『デプロイグループの作成』をクリックします。 作成するデプロイグループ名を決めて入力します。 また、サービスロールは先ほど作成したCodeDeploy用のロールを指定します。 下にスクロールすると『環境設定』という設定項目があるので、作成済みのEC2インスタンスの名前を指定します。 デプロイ設定は『CodeDeployDefault.AllAtOnce』にしておき、『デプロイグループの作成』をクリックします。 （今回はデプロイ対象のインスタンスは１台だけなので、どれを選んでもあまり意味はありません） これでデプロイグループの作成が完了です。 CodeDeployの実行 実際にサーバーにソースコードを反映するにはデプロイを作成します。 デプロイの作成はデプロイグループの画面より実施できます。 （以下はデプロイグループ作成完了時の画面） デプロイ作成画面に入ったら、まずデプロイグループに先ほど作成したデプロイグループを指定します。 次にリビジョンタイプでGitHubを選びます。 するとGitHub情報を入力する欄が出てくるので、GitHubトークン名にgithubアカウント名を入力して 『GitHubに接続』をクリックします。 GitHub側で承認画面が表示される場合は承認します。 続いてリポジトリ名とサーバーに反映したい時点のコミットIDを入力します。 リポジトリ名は『アカウント名/リポジトリ名』形式で、コミットIDは省略していないフルのIDです。 次に追加のデプロイ動作設定にて『コンテンツの上書き』を選択し、 『デプロイの作成』をクリックすればデプロイが走ります。 ※コンテンツの上書きを指定しないと既存コンテンツがある場合にコケます。 デプロイが途中でコケてしまう場合はEC2上で以下のエージェントのログを確認してください。 /var/log/aws/codedeploy-agent/codedeploy-agent.log"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/02/15/apache-tuning/",
    title: "Apacheにサーバーのメモリを食い尽くさせない",
    date: "2019-02-15T00:00:00Z",
    body: "Apacheにサーバーのメモリを食い尽くさせない 環境 Amazon Linux AMI 2018.03 Apache/2.4.34 (Amazon) Apache(prefork)がサーバーのメモリを食い尽くしていたのでチューニングしました。 その際に行なった調査や計算をメモとして残します。 なお、preforkからeventに変更できる場合や、nginxに移行できるのであれば そちらの方がいいかもしれません。 今回はあくまでpreforkをチューニングする前提での手順です。 サーバー物理メモリ確認 まず、サーバーの物理メモリを調べます # cat /proc/meminfo | head -n 1 MemTotal: 2004484 kB このサーバーの搭載メモリは2GBのようです。 現在のメモリ使用状況確認 まずサーバー全体の使用メモリを確認します。 以下のワンライナーで出します。(単位はMB) # ps aux | awk \u0026#39;{print $6}\u0026#39; | awk \u0026#39;{sum=sum+($1/1024);cnt++;} END{print \u0026#34;sum=\u0026#34;sum}\u0026#39; sum=919.656 次にhttpdの子プロセスの使用メモリを確認します。 以下のワンライナーで出します。(単位はMB) # ps aux | grep httpd | grep apache | awk \u0026#39;{print $6}\u0026#39; | awk \u0026#39;{sum=sum+($1/1024);cnt++;} END{print \u0026#34;sum=\u0026#34;sum \u0026#34; cnt=\u0026#34;cnt \u0026#34; ave=\u0026#34;sum/cnt;}\u0026#39; sum=804.273 cnt=20 ave=40.2137 出力結果は以下のようにしています。 項目 意味 sum 現時点のhttpd子プロセスが使っている総メモリ cnt httpd子プロセスの数 ave httpd子プロセス１つが利用している平均メモリ ※補足 サーバー全体のメモリ使用量は前述の/proc/meminfoやfreeコマンドからでも出せるのですが それだとpsコマンドの結果と少しずれてしまうのでpsコマンドに統一しています。 計算 ここまでで分かった事を整理します。 サーバーの搭載メモリ = 約2GB　・・・① 現時点の総使用メモリ = 約920MB　・・・② うち、Apache子プロセスの使用メモリ = 約800MB　・・・③ Apache子プロセス以外の使用メモリ = 約120MB　・・・④(②−③) Apache子プロセス１つあたりが使うメモリ = 約40MB　・・・⑤ ここからざっくり計算していきます。 Apache子プロセス以外用に残すメモリ = 500MB　・・・⑥(④を元に超ざっくり＆余裕を持って) Apache子プロセスで使うメモリ = 1500MB程度　・・・⑦(①−⑥) 生成できるApacheプロセス数 = 35個程度　・・・⑧(⑦÷⑤) Apacheの設定にてこれを設定します。 \u0026lt;IfModule mpm_prefork_module\u0026gt; StartServers 5 MinSpareServers 5 MaxSpareServers 10 ServerLimit 35 #先ほど計算した結果に変更 MaxClients 35 #先ほど計算した結果に変更 MaxConnectionsPerChild 0 \u0026lt;/IfModule\u0026gt; 既存のApacheの設定ファイルに上記が設定されている箇所が存在しない場合は httpd.confに追記するか/etc/httpd/conf.d/配下に新設定ファイルを作成して記載するのでも良いです。 他にも基本的にhttpd子プロセスは常時20個くらい動いていて欲しかったりして、 最終的に以下の設定にしました。 \u0026lt;IfModule mpm_prefork_module\u0026gt; StartServers 20 #httpd起動時にプロセスを20個動かす MinSpareServers 5 MaxSpareServers 20 #リクエスト待ちプロセスは最大20個とする ServerLimit 35 MaxClients 35 MaxConnectionsPerChild 5000 #ある程度処理したら子プロセスには産まれ変わってもらう \u0026lt;/IfModule\u0026gt;"
  },
  {
    url: "https://blog2.logical-dice.com/tags/mac/",
    title: "Mac",
    date: "2019-02-14T00:00:00Z",
    body: "Mac"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/02/14/mac-php-72/",
    title: "MacにPHP7.2をインストールする",
    date: "2019-02-14T00:00:00Z",
    body: "MacにPHP7.2をインストールする 環境 macOS Mojave 10.14.2 fish 2.7.1 Homebrew 2.0.1 PHP 7.2.15 Homebrewを使ってMacにPHP7.2をインストールします。 Homebrewがインストールされていない場合は以下を参照してください。 【Mac】Homebrewインストール まず、Homebrew上でPHP7.2を探します。 $brew search php72 ==\u0026gt; Formulae php@7.2 パッケージが分かったのでインストールします。 $ brew install php@7.2 〜たくさん省略〜 If you need to have php@7.2 first in your PATH run: echo \u0026#39;set -g fish_user_paths \u0026#34;/usr/local/opt/php@7.2/bin\u0026#34; $fish_user_paths\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish echo \u0026#39;set -g fish_user_paths \u0026#34;/usr/local/opt/php@7.2/sbin\u0026#34; $fish_user_paths\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish 〜少し省略〜 インストール完了の直前くらいに上記のような設定追加をするような指示が出力されます。 私はシェルにfishを使っているので上記のようなコマンドでしたが、bashやzshを使っている場合は違うコマンドになると思います。 指示通りに設定をします。 $ echo \u0026#39;set -g fish_user_paths \u0026#34;/usr/local/opt/php@7.2/bin\u0026#34; $fish_user_paths\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish $ echo \u0026#39;set -g fish_user_paths \u0026#34;/usr/local/opt/php@7.2/sbin\u0026#34; $fish_user_paths\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish これで端末を再起動するか設定ファイルを再読み込みするとPHPのバージョンが7.2になっています。 $ php -v PHP 7.2.15 (cli) (built: Feb 7 2019 20:10:03) ( NTS ) Copyright (c) 1997-2018 The PHP Group Zend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.2.15, Copyright (c) 1999-2018, by Zend Technologies"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/02/13/wp-tinymcetemplate-s3/",
    title: "【Wordpress】TinyMCEテンプレートでS3の画像を使う",
    date: "2019-02-13T00:00:00Z",
    body: "【Wordpress】TinyMCEテンプレートでS3の画像を使う 環境 Wordpress 4.8.3 TinyMCEテンプレート 4.8.1 WP Offload Media Lite 2.0 事象 WP Offload Media Liteを使ってWordpressの画像を全てS3にて管理している場合、 TinyMCE テンプレートにて作成したテンプレート内に画像をアップして登録しても 記事編集にてテンプレートを呼び出すと画像が見れなくなってしまいます。 ※WEBサーバー上の画像は削除する設定とします 原因 WP Offload Media Liteでは画像ファイル全てについて『S3上のパス』と『ローカルにあった場合のパス』の両方を紐づけて持っています。 そして、WP Offload Media Liteは以下のようにデータ内画像パスを置換します。 データ登録時 POSTデータ内の画像パスに『S3上のパス』が存在していた場合は『ローカルにあった場合のパス』に置換してDBに登録する。 (S3→ローカル置換) データ呼出時 DBから呼出したデータ内の画像パスに『ローカルにあった場合のパス』が存在していた場合は『S3上のパス』に置換して画面に表示する。 (ローカル→S3置換) しかし、TinyMCE テンプレートに関してはテンプレート登録時の「S3→ローカル置換」は走るのですが、 テンプレート呼出し時の「ローカル→S3置換」は走りません。 よってテンプレート呼出し時は画像パスが『ローカルにあった場合のパス』のままとなり、 ローカルにそんな画像はないので画像が表示されません。 対応 function.phpにデータ登録時のフィルターを追加し、 TinyMCE テンプレートの登録時に「ローカル→S3置換」も走るようにします。 つまり、少々無駄ですがTinyMCE テンプレートの登録時は「S3→ローカル→S3置換」が走るようになります。 function replace_s3_image_on_tinymcetemplates($content){ if ($content[\u0026#39;post_type\u0026#39;] !== \u0026#39;tinymcetemplates\u0026#39; || !class_exists(\u0026#39;Amazon_S3_And_CloudFront\u0026#39;) || !class_exists(\u0026#39;AS3CF_Local_To_S3\u0026#39;)) { return $content; } $s3PluginPath = str_replace(\u0026#39;themes/\u0026#39;.get_template(), \u0026#39;plugins/amazon-s3-and-cloudfront/wordpress-s3.php\u0026#39;, __DIR__); $as3cf = new Amazon_S3_And_CloudFront($s3PluginPath); $localToS3 = new AS3CF_Local_To_S3($as3cf); $content[\u0026#39;post_content\u0026#39;] = $localToS3-\u0026gt;filter_post($content[\u0026#39;post_content\u0026#39;]); return $content; } add_filter(\u0026#39;wp_insert_post_data\u0026#39;,\u0026#39;replace_s3_image_on_tinymcetemplates\u0026#39;); 結構力づくなので、もっと良い方法があれば誰か教えてください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/02/01/git-cmd-cheatsheet/",
    title: "gitコマンドチートシート(応用編)",
    date: "2019-02-01T00:00:00Z",
    body: "gitコマンドチートシート(応用編) gitコマンドの応用的な使い方です。 これができればgit中級者になれるかもしれません。 削除されたリモートブランチの情報を削除する gitはプルをする度にリモートに存在するブランチの情報を取得します。 （git branch -aで見れるremote/xxxxxがそれです) 普通にgit pullを実行するとリモートブランチの取得はしますが リモートで削除されたブランチについては何もしてくれないので ローカルにリモート情報として残り続けてしまいます。 リモートで削除されたブランチ情報をローカルから削除したい場合は プル実行時に以下のオプションを付与します。 git pull --prune 強制プル(force pull) ブランチの状態を強制的にリモートの状態に合わせます。 ローカルのブランチがよく分からない状態になって捨てたい時や 誰かがリモートにforce pushした物を取り込む時に使えます。 (複数人が見てるブランチでforce pushするのは外道ですが…) git fetch git reset --hard origin/{ブランチ名} ※リモートはoriginの前提 直前のコミットに追加修正する ちょっと修正が漏れていた時などに直前のコミットに追加修正を混ぜ込む方法です。 これを使うとコミット履歴を増やさずに済みます。 git add . git commit --fixup=HEAD git rebase -i --autosquash HEAD~2 (このあとのrebase画面はそのまま保存して閉じる) ★注意★ 直前のコミットがプッシュ済みであった場合 上記コマンド後にforce pushが必要になります。 他の人と共有しているブランチにてforce pushをすると 他の人がpullできなくなる恐れがあります。 自分だけが触っているブランチで行いましょう。 マージ時にコミットログを1つにまとめる 細かくコミットをしながら作業を進めたい時などに 作業ブランチを作り、そこで遠慮なくコミットを沢山し、 その作業ブランチを元ブランチに戻す時にコミットログを1つに集約できます。 git merge --squash {マージ対象ブランチ} git commit Commitコメントに元々のCommitログがデフォルトで記載されていますが そこは削除してしまっても構いません。 (残しておいても良いです) 現在のブランチとmasterブランチの差分を確認する githubでプルリク送る時に見れるアレですね。 コマンドからも確認することができます。 bオプションをつけることで空白の差分は無視しています。 git diff -b origin/master ※補足 git diffの空白無視について、bオプションとwオプションがあり 以下のような違いがあるようです。 オプション 説明 「a=␣b」=\u0026gt;「a=␣ ␣b」 「a=b」=\u0026gt;「a=␣b」 b スペースの数を無視する 無視する 無視しない w 空白は全て無視する 無視する 無視する ※上記表の「␣」は半角スペースとする 指定ファイルの指定コミットでの変更箇所を確認する こちらもgithubでコミットログ見た時にみれるアレです。 git diff -w {コミットID}...HEAD {ファイル名} 例： git diff -w 11b2233...HEAD app/Controller/HogeController.php また、あまり使わないかもしれませんが、もし差分行だけでなくファイル全てを表示したい場合は Uオプション(差分の前後何行を表示するかを指定する)で10000行など大きい数字を指定すれば良いです。 git diff -w -U10000 {コミットID}...HEAD {ファイル名} 少し詳細なコミット履歴を表示する statオプションをつけることで、どのコミットでどのファイルを変更したのかが分かります。 また、graphオプションをつけるとブランチのマージ履歴もグラフィカルに確認できます。 git log --stat --graph gitコマンドのエイリアスを作る ~/.gitconfigに以下のような記載をすることでエイリアスが登録できます。 [alias] co = checkout br = branch logs = log --stat --graph こうしておくと、git coがgit checkoutと同様になり git logsがgit log --stat --graphと同様になったりします。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/31/php-strtotime-trap/",
    title: "【PHP】strtotimeの罠",
    date: "2019-01-31T00:00:00Z",
    body: "【PHP】strtotimeの罠 今日相談を受けた事象が面白かったので記事にします。 事象 既存のPHPで来月の年月を取得する処理があり リリース時は正常に動いていたのに、今日(1/31)見たら年月がズレていたようです。 ソースコードを見せて貰ったら以下のようになっていました。 echo date(\u0026#39;Ym\u0026#39;, strtotime(date(\u0026#39;Ym-1\u0026#39;) . \u0026#39; + 1 month\u0026#39;)); 原因 まず、date('Ym-1') がおかしいですね。 date('Ym01')とかdate('Y-m-01')とかにするべきですね。 では、なぜリリース時に正常に動いていたのか。 ここでやっとタイトルの「strtotimeの罠」が関わってきます。 strtotime(date(\u0026#39;Ym-1\u0026#39;) . \u0026#39; + 1 month\u0026#39;) これは、2019/1/31であれば strtotime(\u0026#39;201901-1 + 1 month\u0026#39;) と、なります。 ここでstrtotime様はこう考えます。 201901-1？なんやそれ、知らんわ とりあえず今日の日付から+1monthしとこ 今日は1/31だから1ヶ月後は2/31・・・ってそんな日無いわ！ よっしゃ！繰り上げで3/3にしとこ！ エラーになどしません。 PHPらしいですね。 これにより、1/31に実行すると3月を返してしまい、1ヶ月ズレてしまいます。 もしリリース日やテスト日が月の上旬〜中旬であれば strtotime様の月繰り上げが発生しないので 出力が年月だけだと不具合に気づきません。 この不具合はテストやってもなかなか出てこないと思うので 実装段階で気を付けないといけないですね。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/26/javascript-window-load/",
    title: "window.onloadが動かない",
    date: "2019-01-26T00:00:00Z",
    body: "window.onloadが動かない 事象 javascriptのwindow.onloadを利用して処理を書いても 動作しない時がある。 原因 window.onloadはページを1回表示する際に1度しか実行されない。 もし複数箇所にてwindow.onloadを記載していた場合は そのうちの1つしか実行されない。 jsファイルを複数に分けていても同様なので 意識してないところでwindow.onloadが使われていて陥る可能性が高い。 解決方法 別の記載に変更する // 別の記載例 window.addEventListener(\u0026#39;load\u0026#39;, function(){ console.log(\u0026#39;これとか\u0026#39;) }) // jQueryが使えるなら $(window).on(\u0026#39;load\u0026#39;, function(){ console.log(\u0026#39;これとか\u0026#39;) })"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/25/linux-mint-docker/",
    title: "Docker環境の構築手順",
    date: "2019-01-25T00:00:00Z",
    body: "Docker環境の構築手順 ■環境 Linux Mint 19.1 ubuntu 19.10 Linux Mint 18.3についてはコチラにて記載していましたが 19.1ではaptにて簡単にインストールできるようになっていたので再記載します。 インストール手順 以下のコマンドを実行します。 (ubuntuの場合はaptの前にsudoを付けてください) apt install docker.io docker-compose sudo gpasswd -a $USER docker service docker restart これで再起動すればdocker, docker-composeコマンドが使えるようになります。 （ログアウトだけではdockerグループへの参加が上手く反映されないことがある。要再起動）"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/21/certboy-amazon-linux-2/",
    title: "【Let'sEncrypt】Amazon Linux2でCertbotを使う",
    date: "2019-01-21T00:00:00Z",
    body: "【Let'sEncrypt】Amazon Linux2でCertbotを使う 事象 Amazon Linux 2で従来の方法でcertbotをインストールすると コマンド時に以下のエラーが出て使えませんでした。 Sorry, I don\u0026#39;t know how to bootstrap Certbot on your operating system! You will need to install OS dependencies, configure virtualenv, and run pip install manually. Please see https://letsencrypt.readthedocs.org/en/latest/contributing.html#prerequisites for more info. 従来の方法はこちら 解決方法 以下の方法でcertbotをインストールすれば正常に使えました。 curl -O http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install epel-release-latest-7.noarch.rpm sudo yum install certbot 参考 serverfault"
  },
  {
    url: "https://blog2.logical-dice.com/tags/pico/",
    title: "Pico",
    date: "2019-01-17T00:00:00Z",
    body: "Pico"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/17/wordpress-to-pico/",
    title: "WordpressからPicoへデータ移行",
    date: "2019-01-17T00:00:00Z",
    body: "WordpressからPicoへデータ移行 まず、事前にpicoのブログ化が完了しているものとします CMSのpicoをブログ化 かなりの力技です 綺麗な方法ではないのでご了承ください Wordpressからのイメージ取得 まずWordpressから画像ファイル一式をダウンロードしてきます。 Wordpressの/wp-content/uploads/配下のファイルを 全てpico内にダウンロードしてきます。 今回の例ではpico直下にimagesディレクトリを作成し、 そこにダウンロードするとします。 階層としてはpicohttps://blog2.logical-dice.com/images/2019/01/hogehoge.pngのようになります。 Wordpressからの記事取得 WordpressのDBから記事情報を取得します。 MySQL Workbenchにて以下のSQLを実行し、 結果をjsonで出力しておきます SELECT p.ID, p.post_date, u.user_nicename, p.post_title, p.post_content, t.tags FROM wp_posts p LEFT JOIN wp_users u ON u.ID = p.post_author LEFT JOIN ( SELECT tr.object_id AS post_id, group_concat(t.name) AS tags FROM wp_term_relationships tr LEFT JOIN wp_term_taxonomy tt ON tt.term_taxonomy_id = tr.term_taxonomy_id LEFT JOIN wp_terms t ON t.term_id = tt.term_id WHERE tt.taxonomy = \u0026#39;post_tag\u0026#39; GROUP BY tr.object_id ) t ON t.post_id = p.ID WHERE post_status = \u0026#39;publish\u0026#39;; 次に出力したjsonからpico用の記事mdファイルを生成します 以下のようなPHPスクリプトを実行すれば取得したJSONファイルから mdファイルを生成できます \u0026lt;?php // 出力したjsonは「tmp_insert.json」だとします $jsonStr = file_get_contents(\u0026#39;./tmp_insert.json\u0026#39;); $json = json_decode($jsonStr); foreach ($json as $post) { // Descriptionはページの本文から先頭を切り出す $description = strip_tags($post-\u0026gt;post_content); $description = str_replace(\u0026#34;\\n\u0026#34;, \u0026#39;\u0026#39;, $description); $description = mb_substr($description, 0, 64); $description .= \u0026#39;...\u0026#39;; // 画像ファイルのパスを修正 $body = str_replace(\u0026#39;https://your-wp-site.com/wp-content/uploads/\u0026#39;, \u0026#39;https://blog2.logical-dice.com/images/\u0026#39;, $post-\u0026gt;post_content); // 記事へのパスを修正 $body = str_replace(\u0026#39;https://your-wp-site.com/?p=\u0026#39;, \u0026#39;/posts/\u0026#39;, $body); // Markdownなので行末にスペース2つが入るようにする $body = str_replace([\u0026#34;\\r\\n\u0026#34;, \u0026#34;\\r\u0026#34;, \u0026#34;\\n\u0026#34;], \u0026#39; \u0026#39;.PHP_EOL, $body); $content = \u0026lt;\u0026lt;\u0026lt; EOF --- title: $post-\u0026gt;post_title summary: $description tags: - $post-\u0026gt;tags date: $post-\u0026gt;post_date draft: false --- $body EOF; file_put_contents($post-\u0026gt;ID.\u0026#39;.md\u0026#39;, $content); } あとは必要に応じて生成されたmdファイルを微調整するだけです。 また、pico/content/tags配下にWordpressにて利用していたタグを追加してください。 ※補足 環境によってはmdファイル内に記載しているとエラーとなってしまうHTMLタグがあるようです。 エラーとなったHTMLタグは削除するかMarkdown記法に置換しておきましょう。 Wordpress時のURLからpicoのURLにリダイレクトする 最後にWordpress時の記事URLを新URLにリダイレクトさせれば完了です。 (必須ではないですが、Google検索などから旧記事へ来てしまう人のため) Apacheの場合は以下のようになります。 設定ファイルかhtaccessに記載してください。 \u0026lt;IfModule mod_rewrite.c\u0026gt; RewriteEngine On # 旧ブログからのリダイレクト RewriteCond %{QUERY_STRING} p=([0-9]*)$ RewriteRule .* /posts/%1? [R=301,L] \u0026lt;/IfModule\u0026gt; Nginxの場合は設定ファイルに以下を記載します。 if ($args ~ \u0026#34;p=(.*)\u0026#34;) { set $articleId $1; rewrite ^.*$ /posts/wp/$articleId? permanent; } これでhttps://your-site.com/?p=1234へアクセスがあると https://your-site.com/posts/1234へリダイレクトしてくれます。 記事の階層を変える場合は適宜読み替えてください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/16/pico-blog/",
    title: "picoでブログ構築",
    date: "2019-01-16T00:00:00Z",
    body: "picoでブログ構築 picoの基本的な構築はこちらを参照してください。 PHP製のCMS「pico」の紹介 picoを使ってブログを構築する方法です。 公式ページにpicoをブログ化する手順が書いてあります。 公式の手順 公式の方法はWEBページ内にブログ領域を作る方法のようなので そもそもメインコンテンツがブログである場合は以下のように修正します。 なお、できあがったものをテンプレートとして公開しています。 簡単にブログ解説したいだけの人はこちらをご利用ください。 mildjester/pico-blog プラグインの追加 プラグインはpico/pluginsの中にダウンロードしておきます。 ページネーション https://github.com/rewdy/Pico-Pagination/releases ダウンロードした圧縮ファイルの中にpagination.phpがあるので それを/pico/pluginsの中に配置します。 そしてconfig.ymlにプラグイン用の設定を追記しておきます。 ## # Plugin: Pico-Pagination # pagination_limit: 10 # 一覧ページにて１ページに表示する記事数 pagination_next_text: 次へ\u0026gt;\u0026gt; # 次へリンクのテキスト pagination_prev_text: \u0026lt;\u0026lt;戻る # 戻るリンクのテキスト タグ https://github.com/PontusHorn/Pico-Tags PicoTags.phpを/pico/pluginsの中に配置します。 サイト内検索 https://github.com/mwgg/Pico-Search 40-PicoSearch.phpを/pico/pluginsの中に配置します。 このプラグインではページのタイトルをキーにして検索する事はできるようですが。 記事本文をキーにして検索することはできないようでした。 サイトマップ https://github.com/DaveKin/Pico_Sitemap/ PicoXMLSitemap.phpを/pico/pluginsの中に配置します。 テーマの編集 /pico/themes配下のテーマを編集します。 デフォルトの/pico/themes/defaultをそのまま編集しても良いですし defaultをコピーして自分用のテーマを作っても良いです。 別テーマを作成する場合はconfig.ymlのtheme: defaultの部分も作成したテーマのディレクトリ名に変更してください。 なお、公式ページなどからテーマをダウンロードすることも可能です。 ただしpico 1.x用のテーマだからか、そのままではうまく動かなかったりするので要注意です。 公式ページのテーマ一覧 index.twigの分離 デフォルトではindex.twigにHTMLの全てが記載されていますが、 テンプレートを増やした際にメンテナンス性が悪くなるので ヘッダー部分とフッター部分を分離してしまいます。 まずindex.twigと同じディレクトリに_header.twigを作成し、 index.twigの１行目から\u0026lt;div id=\u0026quot;main\u0026quot;\u0026gt;の上までをコピーします。 次に、同様に_footer.twigを作成して\u0026lt;div id=\u0026quot;main\u0026quot;\u0026gt;の閉じタグの下から 最終行までをコピーします。 そしてindex.twigのヘッダーとフッターにコピーした部分は削除し、 代わりに{% include '_header.twig' %} と {% include '_footer.twig' %} に置き換えます。 できあがったindex.twigはこんな感じです。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {% if meta.title %}\u0026lt;h2\u0026gt;{{ meta.title }}\u0026lt;/h2\u0026gt;{% endif %} \u0026lt;p class=\u0026#34;meta\u0026#34;\u0026gt;{{ meta.date_formatted }}\u0026lt;/p\u0026gt; {{ content }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} ※どこまでを共通化するかは個人の趣向もあると思うので 共通化範囲は増減あっても構いません。 ヘッダーにサイト内検索フォームを追加する ヘッダーの好きな位置に以下のソースコードを埋め込みます。 これがサイト内検索のフォームになります。 \u0026lt;form id=\u0026#34;search_form\u0026#34; action=\u0026#34;/search\u0026#34; method=\u0026#34;get\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;search\u0026#34; id=\u0026#34;search_input\u0026#34; name=\u0026#34;q\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;検索\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; ページ別テンプレートの作成 index.twigと同じディレクトリに以下のようなtwigファイルを作成します index.twig 全ての記事一覧を表示します。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {% for page in paged_pages|sort_by(\u0026#34;time\u0026#34;)|reverse %} {% if page.id starts with \u0026#34;posts/\u0026#34; and not page.hidden %} \u0026lt;div class=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;p class=\u0026#34;date\u0026#34;\u0026gt;{{ page.date_formatted }}\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;excerpt\u0026#34;\u0026gt;{{ page.description }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {% endif %} {% endfor %} {{ pagination_links }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} article.twig 記事ページを表示します。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {% if meta.title %}\u0026lt;h2\u0026gt;{{ meta.title }}\u0026lt;/h2\u0026gt;{% endif %} \u0026lt;p class=\u0026#34;meta\u0026#34;\u0026gt;\u0026lt;b\u0026gt; 投稿日：{{ meta.date_formatted }} \u0026lt;span class=\u0026#34;article_tags\u0026#34;\u0026gt; タグ： {% for tag in meta.tags %} \u0026lt;a href=\u0026#34;/tags/{{ tag }}\u0026#34; \u0026gt;{{ tag }}\u0026lt;/a\u0026gt; {% endfor %} \u0026lt;/span\u0026gt; \u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; {{ content }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} static.twig 固定ページを表示します。 ※ページタイトルと日付を非表示にしています。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {{ content }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} tags.twig タグ一覧を表示します {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {% if meta.title %}\u0026lt;h2\u0026gt;{{ meta.title }}\u0026lt;/h2\u0026gt;{% endif %} \u0026lt;ul\u0026gt; {% for page in pages %} {% if page.id starts with \u0026#34;tags/\u0026#34; and not page.hidden %} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {% endif %} {% endfor %} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} tag-index.twig タグを絞った記事一覧を表示します。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; {% if meta.title %}\u0026lt;h2\u0026gt;{{ meta.title }}\u0026lt;/h2\u0026gt;{% endif %} {% for page in pages|sort_by(\u0026#34;time\u0026#34;)|reverse %} {% if page.id starts with \u0026#34;posts/\u0026#34; and not page.hidden %} \u0026lt;div class=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;p class=\u0026#34;date\u0026#34;\u0026gt;{{ page.date_formatted }}\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;excerpt\u0026#34;\u0026gt;{{ page.description }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {% endif %} {% endfor %} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} search.twig サイト内検索結果を表示します。 {% include \u0026#39;_header.twig\u0026#39; %} \u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;SearchResults\u0026#34;\u0026gt; {% if meta.purpose == \u0026#34;search_results\u0026#34; %} \u0026lt;p class=\u0026#34;meta\u0026#34;\u0026gt;「{{ search_term }}」の検索結果（{{ search_num_results }} 件）\u0026lt;/p\u0026gt; {% for page in search_results %} {% if page.id starts with \u0026#34;posts/\u0026#34; and not page.hidden %} \u0026lt;h3\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;p class=\u0026#34;date\u0026#34;\u0026gt;{{ page.date_formatted }}\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;excerpt\u0026#34;\u0026gt;{{ page.description }}\u0026lt;/p\u0026gt; {% endif %} {% endfor %} {% endif %} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {% include \u0026#39;_footer.twig\u0026#39; %} スタイル(CSS)の調整 記事ページのタグ一覧用のスタイルも追加しておきます。 テーマディレクトリのcss/style.cssに以下を追加します。 /* 記事ページのタグ一覧 */ span.article_tags { margin-left: 40px; font-size: 0.9em; } span.article_tags a { border-radius: 5px; /* CSS3草案 */ -webkit-border-radius: 5px; /* Safari,Google Chrome用 */ -moz-border-radius: 5px; /* Firefox用 */ background-color: #707070; color: #ffffff; padding: 1px 7px; } これでテーマの準備は完了です。 mdファイルの作成 ここから実際のページのコンテンツとなるmdファイルを作成していきます。 pico/content/index.md 全記事一覧ページです。 --- title: ブログ summary: これはブログですよ date: 2010-12-31 draft: false --- pico/content/tags.md タグ一覧ページです。 --- title: タグ一覧 summary: タグの一覧ページですよ date: 2010-12-25 draft: false --- pico/content/tags/sampleTag.md タグの記事一覧ページです。 本例ではタグがsampleTagとなっている記事の一覧を取得します。 --- title: sampleTag summary: sampleTagの一覧です Filter: sampleTag --- pico/content/posts/sampleArticles.md 記事ページです。 Tagsには上記のpico/content/tags/に作成したタグを指定します。 --- title: 記事ですよ summary: これは記事ページです tags: - sampleTag date: 2015-08-08 15:35:56 draft: false --- 記事の本文ですよー pico/content/search.md サイト内検索結果ページです。 --- Purpose: search_results --- pico/content/hogehoge.md 固定ページです。 必要に応じて作成してください。 特に固定ページが不要であれば作成しなくて良いです。 --- title: 何かのページ summary: 何か固定ページだよ date: 2001-01-01 draft: false --- 固定ページですよー これで基本的に作成すべきmdは揃いました。 もしGoogle等にインデックスさせたくない(検索させたくない)場合は YAMLヘッダーに以下を追加してください。 Robots: noindex,nofollow これでpicoのブログ化は完了です。 Wordpressからデータ移行する場合はWordpressからPicoへデータ移行を参照してください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/2019/01/15/pico-introduction/",
    title: "PHP製のCMS「pico」の紹介",
    date: "2019-01-15T00:00:00Z",
    body: "PHP製のCMS「pico」の紹介 picoとは PHP製のCMSで、DBなど利用せずファイルだけでWEBページが作成できます。 公式サイト ページにはMarkdownを使えるので、Markdownでメモなどを取ることが多い人には 使いやすいCMSなのではないかと思います。 Wordpressと比べてどうか ページの書きやすさ Markdownに慣れている人にとっては書きやすいと思いますが 慣れていないライターさんなどを雇っている場合などは向かないと思います。 また、表現がMarkdownの域に絞られるので、直接HTMLを書けたWordpressの方が 描画は自由にできるかもしれません。 picoでもMarkdown内にHTML書けました 構築のしやすさ 基礎構築だけならソースコードを持ってきて終わりなので楽です。 ただし、プラグインを追加するにはテーマファイルに手を入れる必要があったりするので 単純に「インストール」をクリックするだけのWordpressに比べればそこが手間です。 あと、日本語の情報が少ないのも構築する上ではネックになるかもしれません。 管理のしやすさ 『管理画面』が無いので設定は全てソースコード上で行ないます。 なので、非エンジニアが運用するには難しいかと思います。 ですが、DBがないのでバックアップ管理などは楽です。 GitHubのprivateリポジトリが無料で作れるようになったので 全てそこで管理してしまえば良いと思います。 また、管理画面が無いのでセキュリティ的にも強いと思います。 機能面 Wordpressでデフォルトで用意されているタグやカテゴリがpicoには無いので 若干の不便さはあります。 一応プラグインで追加はできますが、やや煩雑です。 また、プラグインやテーマの数でもWordpressの方が勝ります。 picoの導入 プロジェクト作成 まず、composerでプロジェクトを作成します。 ※composerが分からない人はこちら composer create-project picocms/pico-composer pico 生成されたpicoディレクトリをそのままWEBサーバー上に配置すれば もう既にデフォルトのページが表示されます デフォルトページにはコンテンツの配置方法などが記載されているので 確認しておくと良いと思います。 URLリライト設定 picoはURLリライトを利用します。 (必須ではないが無いとURLの見た目がカッコ悪いし、SEO的にも良くなさそう) URLリライトについてはpico内の.htaccessに記載されています。 .htaccessはApacheでなければ作動しないので、Nginxの場合は別途設定が必要です。 公式のリライト設定手順 設定ファイル作成 pico/config/config.yml.templateをコピー or リネームして pico/config/config.ymlを作成してください。 とりあえず初期段階で書き換えるのは以下くらいかと思います。 設定項目 設定内容 site_title サイトのタイトルに書き換え timezone 日本ならAsia/Tokyo ※注意 timezoneはデフォルトがUTCなのでJSTに変更すればいいと思わせつつ ここはAsia/Tokyoでないといけません。 config.ymlを配置した時点でデフォルトページは表示されなくなります 後はpico/content配下にmdファイルを作成していけばページが生成されます 主に以下のようにページを作成します pico/content/index.md いわゆるインデックスページ https://your-pico-site.com/ にアクセスした時に表示される pico/content/404.md いわゆる404ページ （作らなくてもデフォルトのページが表示される。カスタマイズしたいなら作る） pico/content/hogehoge.md (hogehogeは任意) https://your-pico-site.com/hogehoge にアクセスした時に表示される デフォルトテーマでは、右上のページ一覧にも表示される pico/content/fooooo/hogehoge.md (foooooとhogehogeは任意) https://your-pico-site.com/fooooo/hogehoge にアクセスした時に表示される デフォルトテーマでは、右上のページ一覧には表示されない 各ページのmdファイル構成は『YAMLヘッダー＋コンテンツ』のようにします 例えば以下のようになります --- title: サンプルページ summary: これはサンプルページですよ date: 2019-01-07 draft: false --- これはサンプルページですよー YAMLヘッダーについての詳細は公式ページを参照してください。 公式ページのYAMLヘッダー説明 上記のTemplateには現在のテーマに用意してあるTwigファイル名を指定します (デフォルトテーマはpico/themes/default) 例えば、上記例であれば利用するテンプレートはindex.twigです。 ブログ化する場合はpicoでブログ構築に続きます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1829/",
    title: "Linux Mint 19.1 MATEをUSBメモリからインストール",
    date: "2018-12-20T16:25:54Z",
    body: "Linux Mint 19.1 MATEをUSBメモリからインストール PCを再構築する必要があったので、Linux Mintの最新バージョンを入れて見ました インストール準備をした作業環境はMacで、USBメモリにイメージを焼いてインストールしています OSイメージのダウンロード 今回選んだのは『Linux Mint 19.1 \u0026ldquo;Tessa\u0026rdquo; - MATE (64-bit)』です こちらからダウンロードできます（とりあえず近そうなTaiwanなどからDL） https://linuxmint.com/edition.php?id=263 インストールメディア(USB)の作成 まずMacにUSBメモリを刺し、ターミナルでdiskutil listを実行します これでUSBメモリのdisk番号を調べます（NAMEやSIZEを見る） 私の場合は /dev/disk2 がUSBメモリでした $ diskutil list /dev/disk0 (internal): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme 121.3 GB disk0 1: EFI EFI 314.6 MB disk0s1 2: Apple_APFS Container disk1 121.0 GB disk0s2 /dev/disk1 (synthesized): #: TYPE NAME SIZE IDENTIFIER 0: APFS Container Scheme - +121.0 GB disk1 Physical Store disk0s2 1: APFS Volume Macintosh HD 70.2 GB disk1s1 2: APFS Volume Preboot 69.6 MB disk1s2 3: APFS Volume Recovery 1.6 GB disk1s3 4: APFS Volume VM 1.1 GB disk1s4 /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *2.0 GB disk2 1: Microsoft Basic Data MINT19_1 2.0 GB disk2s1 対象ディスクをフォーマットします 以下の HOGEHOGE の部分はフォーマット後のUSBメモリの名前ですので 任意のものを入れてください $ diskutil eraseDisk MS-DOS HOGEHOGE /dev/disk2 フォーマットが終わったらアンマウントし、isoイメージを焼きます （ddコマンドで結構待ちます） $ diskutil unmountDisk /dev/disk2 Unmount of all volumes on disk2 was successful $ cd ~/Downloads #isoイメージをDLした場所に読み替えてください $ sudo dd if=linuxmint-19.1-mate-64bit.iso of=/dev/rdisk2 bs=32m dd: /dev/rdisk2: short write on character device dd: /dev/rdisk2: Input/output error 60+1 records in 60+1 records out 2029518848 bytes transferred in 907.854818 secs (2235510 bytes/sec) ※注意 ofで指定しているdiskをdisk2ではなくrdisk2にしています（頭にrを付けている） こうすると処理が早くなるようです また、bs(バッファサイズ)のベストな値は環境により変わります よく分からなければ32mで良いと思います ddコマンドが完了したらDiskを取り出してインストールメディアの作成完了です OSインストール Linux MintをインストールするPCにUSBメモリを刺して起動し、BIOS設定を開きます。 BIOS設定の起動方法はメーカーにより違うと思うので各自確認してください。 BIOS設定のブート順序設定にて1番目にUSBメモリを指定してBIOS設定を閉じます。 BIOS設定画面が閉じるとPC再度起動し、Linux Mintを起動することができます。 ここで起動しているLinux MintはOSイメージの中にある仮の環境なので、 デスクトップにあるインストーラーを実行してHDDやSSDにOSインストールします。 これでOSインストールは完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1772/",
    title: "Wordpressの画像をS3+CloudFrontで配信する",
    date: "2018-12-11T19:30:54Z",
    body: "Wordpressの画像をS3+CloudFrontで配信する 環境 WordPress 4.8.3 AWS設定手順 IAM設定 まず、WordpressとAWSの連携に使うIAMユーザーを準備します 今回の手順ではバケット側のポリシーで権限付与するので IAM側の権限設定は適当で良いです 準備したIAMユーザーの以下を控えておいてください ・ARN ・アクセスキーID ・シークレットアクセスキー S3設定 次にS3にてWordpress用のバケットを用意し バケットポリシーに以下を設定します { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowBucketAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::準備したIAMのARN\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::対象のバケット/*\u0026#34;, \u0026#34;arn:aws:s3:::対象のバケット\u0026#34; ] } ] } CloudFront設定 ユーザーにS3へ直接画像を取りに来させる事も可能なのですが 転送料金が結構高くなるのでCloudFront経由でS3に繋がせるようにします まずloudFrontの設定画面にて「Create Distribution」をクリックします 次にWebの「Get Started」をクリックします。 「Origin Domain Name」にCloud Front経由にしたいS3バケットを指定します テキストボックスに見えますが、クリックすると選択肢が表示されます 「Origin Domain Name」を選択すると、「Origin ID」にも勝手に値が入ります 上記入力したら下までスクロールして「Create Distribution」をクリックします CloudFrontの一覧に戻りますので 先ほど作成したDistributionのStatusが『Deployed』になるのを待ちます これでCloudFront経由でS3へアクセスできるようになりました アクセスできるドメインは上図の「Domain Name」を参照してください 独自ドメインでアクセスしたい場合は上部メニューの 「Distribution Settings」より編集画面を開き 以下のように設定してください Route 53設定 CloudFrontを独自ドメインで使う場合はDNS設定もしておきます Wordpress設定 まずは普通にWordpressを構築します （ここは手順割愛） プラグイン：Amazon Web Services プラグインの新規追加で『Amazon Web Services』を入れます 次にwp-config.phpにIAMのアクセスキーを設定します （下記「AAA…」と「BBB…」は該当のものに書き換えてください） define( \u0026#39;DBI_AWS_ACCESS_KEY_ID\u0026#39;, \u0026#39;AAAAAAAAAAAAAAAAA\u0026#39; ); define( \u0026#39;DBI_AWS_SECRET_ACCESS_KEY\u0026#39;, \u0026#39;BBBBBBBBBBBBBBB\u0026#39; ); これでWordpressとAWSの連携ができるようになります プラグイン：WP Offload Media Lite 次にプラグインの新規追加で『WP Offload Media Lite』を入れます インストール＆有効化後、設定メニューに「Offload Media」が追加されるので 利用するバケットを指定します 左下の『Browse existing buckets』から選択するのが楽だと思います バケット選択後、いくつか設定項目が出てきます まず、デフォルトでは画像URLがS3直アクセスになってしまうので 「Custom Domain」をCloudFront経由のURLに変更します （xxxxx.cloudfront.net または 指定した独自ドメイン） サーバー側に画像ファイルを残したくない場合は 『Remove Files From Server』をONにしておきます。 ※注意 上図Warningにも出てますが、もしかしたらプラグインによってはサーバー内に 画像ファイルを残しておかないと正常に動かないものもあるかもしれません その場合はOFFにするかプラグイン側をどうにかするかしてください 既存の画像ファイルをS3に移行したい場合 こちらを参考にしました https://php-java.com/archives/2228 まず既存画像ファイルをS3にアップロードします 階層は「wp-content/uploads/2018/・・・」のようにします 次にSQLでDBを更新します SQL内の以下部分は各環境に合わせて書き替えてください webserver_hostname：WEBサーバーのホスト名 cloudfront_hostname：CloudFrontのホスト名 以下、S3のリージョンは東京である前提にしています 東京でない場合はS3のホスト名も該当のものに書き換えてください まず記事内の画像URL差し替え UPDATE wp_posts SET post_content = REPLACE(post_content, \u0026#34;https://webserver_hostname/wp-content/uploads/\u0026#34;, \u0026#34;https://cloudfront_hostname/wp-content/uploads/\u0026#34;); 次にアイキャッチの画像URL差し替え UPDATE wp_posts SET guid = REPLACE(guid, \u0026#34;https://webserver_hostname/wp-content/uploads/\u0026#34;, \u0026#34;https://cloudfront_hostname/wp-content/uploads/\u0026#34;); 次にmeta情報に各画像ファイルのS3の情報を追加します （UPDATEではなくINSERT） 以下のSELECT文にてINSERT文を生成できるので 生成したINSERT文を実行すれば完了です SQL内の s3_bucket_name の部分は対象バケット名に書き換えてください また、S3バケットが東京リージョンでない場合はap-northeast-1の部分も書き換えてください SELECT CONCAT(\u0026#39;insert into wp_postmeta(post_id, meta_key, meta_value) values(\u0026#39; , post_id, \u0026#39;, \\\u0026#39;amazonS3_info\\\u0026#39;, \\\u0026#39;a:4:{s:8:\\\u0026#34;provider\\\u0026#34;;s:3:\\\u0026#34;aws\\\u0026#34;;s:6:\\\u0026#34;region\\\u0026#34;;s:14:\\\u0026#34;ap-northeast-1\\\u0026#34;;s:6:\\\u0026#34;bucket\\\u0026#34;;s:14:\\\u0026#34;s3_bucket_name\\\u0026#34;;s:3:\\\u0026#34;key\\\u0026#34;;s:\u0026#39;,length(meta_value) + 19,\u0026#39;:\\\u0026#34;wp-content/uploads/\u0026#39;, meta_value, \u0026#39;\\\u0026#34;;}\\\u0026#39;);\u0026#39; ) AS record from wp_postmeta where meta_key = \u0026#39;_wp_attached_file\u0026#39;; Wordpressのバージョン等によってはINSERT文を少し変えないといけないかもしれません INSERTすべき文を正確に知るためには一度Wordpress上で新規画像を登録して wp_postmetaテーブルに追加されたレコードを確認するのが一番だと思います"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1755/",
    title: "Google Tag Managerで発火しない時",
    date: "2018-11-26T23:38:42Z",
    body: "Google Tag Managerで発火しない時 Google Tag　Managerを使ってトリガーを設定したはずが 思い通りにGoogle Analytics上で集計できない時の調査方法です。 まず、Google Tag Managerの管理画面にて「プレビューモード」にします。 公開ボタンの横にある『プレビュー』をクリックするだけです。 プレビューモードになったら、同じブラウザでテストしたいページを開きます。 すると、Google Tag Managerのコンソールが表示されます。 画面上クリックをするとクリックイベントが発生するのが分かります。 ここでコンソール内の『Variables』または『Data Layer』のを開くと どのようなトリガーが発火したのかが分かります。 これで確認したクリック時に発火したトリガー（IDやClass）が Google Tag Managerで設定したトリガーと一致しているか確認します。 ありそうな間違い Google Tag Managerのイベントは一番内側の要素がトリガーになります。 javascriptのonClickとは違うので注意が必要です。 例えば以下のソースコードの場合、sample.pngをクリックした時に id=\u0026ldquo;fooo\u0026rdquo;のクリックイベントは発火しません。 \u0026lt;div id=\u0026#34;wrapper\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;fooo\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;./sample.png\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1741/",
    title: "Google Tag Managerの基本設定",
    date: "2018-11-26T22:32:40Z",
    body: "Google Tag Managerの基本設定 Google AnalyticsをGoogle Tag Managerに切り替えた際のメモです。 前提条件としてGoogle Analyticsのアカウントはあることとします。 アカウント作成 まずGoogle Tag Managerのページにてアカウントを作成します https://tagmanager.google.com アカウント名は任意の物を入力し、国は日本を選択して『続行』をクリックします。 するとコンテナの作成になるので 任意のコンテナ名と利用場所（WEBページかスマホアプリかなど）を選択します。 今回はWEBを選択したとします。 規約に同意するとHTMLに埋め込むタグが表示されます。 \u0026lt;head\u0026gt;内に埋めるタグと\u0026lt;body\u0026gt;に埋めるタグが生成されるので それぞれ対応する場所に埋め込みます。 これでGoogle Tag Managerを使う準備ができました。 イベントトラッキングの設定 クリックをトラッキングする設定をしてみます。 Google Tag Managerの管理画面を開くと「新しいタグ」のリンクがあるのでクリックします。 タグとは検知する項目みたいなものです。 タグの設定画面が開くので、タグの名前や種類を指定します。 とりあえず、簡単に下図のような設定をします。 「Google アナリティクス設定」という項目があるので、「新しい変数」を選択します。 するとトラッキングIDを入力する欄が出てくるので、Google AnalyticsのトラッキングIDを入力してください。 すべて入力したら「保存」をクリックして閉じます。 次に下の方にスクロールするとトリガーの設定ができます トリガーとは、イベントを検知する条件です。 プラスマークをクリックすると新規トリガーを作成できます。 トリガーの名前や内容を設定します。 基本的には下図のような設定でよいです。 クリック対象の要素にIDが付いている場合は「Click ID」で良いですが もしIDを指定していなくてClassなどしか設定されていない場合は 「Click　Classes」を選択してください。 全て完了したら『保存』をクリックしてトリガー設定画面を閉じ、 タグ設定画面でも『保存』をクリックして閉じます。 最後にGoogle Tag ManagerのTOP画面にて「公開」をクリックすれば完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/chrome/",
    title: "Chrome",
    date: "2018-11-19T10:29:41Z",
    body: "Chrome"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1735/",
    title: "Chromeの開発者ツールを新タブでも維持する",
    date: "2018-11-19T10:29:41Z",
    body: "Chromeの開発者ツールを新タブでも維持する Chromeの開発者ツールはデフォルトでは新タブには反映されないのですが スマホモードでページの確認をしている時などは新タブもスマホモードのまま開きたい事が多いです。 以下の設定をすれば新タブも開発者ツールを維持したまま開く事ができます。 ①開発者ツールのsettingsをクリックする。 ②下の方にスクロールしていき、Auto-open DevTools for popupsにチェックを入れる。 これだけです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1715/",
    title: "gitコマンドで複数のGitHubアカウントを使い分ける",
    date: "2018-11-14T20:37:00Z",
    body: "gitコマンドで複数のGitHubアカウントを使い分ける 環境 Mac OS (Linuxでも動く気がします） ※ 2019/3/29追記 GitHubの利用規約にて1個人で複数の無料アカウントを運用する事は禁止されているようでした。 https://help.github.com/en/posts/github-terms-of-service#b-account-terms もし複数の無料アカウントを利用している場合はアカウントの統合などを検討した方が良いです。 無料アカウントと有料アカウントを使い分ける場合などは本手順を参考にして貰えればと思います。 概要 GitHubではアカウントに秘密鍵が紐づいており git コマンドを実行した際に指定した秘密鍵で利用アカウントを判定しています。 （秘密鍵未指定の場合はデフォルトの秘密鍵） フリーランスをしていると自分のGitHubアカウントとは別で 取引先様のGitHubアカウントを作成する必要があったりするのですが １台のPCで複数GitHubアカウントを使っていると秘密鍵の切り替えが面倒です。 私も最初はこちらの記事のように.ssh/configに別ホストとして設定していたのですが これだとcloneする時などにいちいちホスト名を書き換えたりしなければいけなかったりして それも少し面倒でした。（特にリポジトリが沢山あるプロジェクトだったので） https://qiita.com/yamataku29/items/4744c9c70ad793c83b82 というわけで リポジトリ所有者（ユーザーもしくはOrganizations）と秘密鍵の紐づけを設定しておけば 自動でgitコマンド実行時に秘密鍵を使い分けてくれるスクリプトを作りました。 こちらです https://github.com/mildjester/gits 使い方 ①cloneでもZIPダウンロードでも良いので上記リポジトリをダウンロードしてきます。 ②config_templateを同ディレクトリにconfigという名前でコピーします。 ③コピーして生成したconfigの以下変数を設定します ■defaultKey デフォルトの秘密鍵へのパス。基本的にテンプレートのままでいいはず。 ■specialKeys デフォルト以外の秘密鍵を使うリポジトリ所有者と秘密鍵の紐付け一覧。 形式は『リポジトリ所有者::秘密鍵のパス』なので 例えば「所有者AAA」のリポジトリに使う秘密鍵は/.ssh/id_rsa_1、 「所有者BBB」のリポジトリに使う秘密鍵は/.ssh/id_rsa_2とする場合は 以下のようになります。 specialKeys=( \u0026#39;AAA::~/.ssh/id_rsa_1\u0026#39; \u0026#39;BBB::~/.ssh/id_rsa_2\u0026#39; ) ④~/.bashrcに以下のエイリアスを追記しておきます。 通常使っているシェルがbash以外（zshやfishなど）の場合は、そちらの設定ファイルへ追記してください。 # gitコマンドを置き換える alias git=\u0026#34;/path-to-gits-repository/gits.sh\u0026#34; ※path-to-gits-repositoryはリポジトリをcloneまたはダウンロードしてきたディレクトリを指定してください これで何も考えずにgitコマンドを打っても、裏で勝手に秘密鍵を切り替えてくれるようになります。 備考：ghqを使っている場合 私はgit cloneよりもghq getをよく使うので、そのスクリプトも用意しました。 基本的な設定は上記と同じで、エイリアスの設定だけ追加します。 alias ghq_get=\u0026#34;~/git/github.com/mildjester/gits/ghq_get.sh\u0026#34; こちらは完全にコマンドを置き換える訳ではないのですが 以下のコマンドでリポジトリを取得する事ができるようになります。 (ghqとgetの間がスペースではなくアンダーバーになるだけ) ghq_get git@github.com:AAAA/hogehoge.git"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1627/",
    title: "同じForm内に複数のreCAPTCHA認証ボタンを設置する",
    date: "2018-10-26T13:27:25Z",
    body: "同じForm内に複数のreCAPTCHA認証ボタンを設置する WEBページのreCAPTCHA対応しているform内に複数のsubmitボタンを設置する方法です。 前提としてPOSTデータより g-recaptcha-response を取り出して認証しているとします。 単純にsubmitボタンを複数設置した場合、うまく動きません。 例えば、以下のようなフォームを作った場合 \u0026lt;script\u0026gt; function mySubmit(){ document.getElementById(\u0026#34;myForm\u0026#34;).submit(); } \u0026lt;/script\u0026gt; \u0026lt;form id=\u0026#34;myForm\u0026#34; action=\u0026#34;/hoge\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;各自のサイトキー\u0026#34; data-callback=\u0026#34;mySubmit\u0026#34;\u0026gt;送信1\u0026lt;/button\u0026gt; 〜いろいろ〜 \u0026lt;button class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;各自のサイトキー\u0026#34; data-callback=\u0026#34;mySubmit\u0026#34;\u0026gt;送信2\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 送信1ボタンのすぐ上に、このようなreCAPTCHA用の要素が生成されます。 \u0026lt;div\u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-badge\u0026#34; 〜略〜 \u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-logo\u0026#34;\u0026gt; \u0026lt;iframe src=\u0026#34;https://www.google.com/recaptcha/〜略〜 \u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-error\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;textarea id=\u0026#34;g-recaptcha-response\u0026#34; name=\u0026#34;g-recaptcha-response\u0026#34; 〜略〜 \u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; そして、送信2ボタンのすぐ上にも同様に要素が生成されます。 送信1ボタンとの違いはiframe内とtextareaのidです。 textareaのidはボタンの数が増える毎に連番が付与されるようです。 \u0026lt;div\u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-badge\u0026#34; 〜略〜 \u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-logo\u0026#34;\u0026gt; \u0026lt;iframe src=\u0026#34;https://www.google.com/recaptcha/〜略〜 \u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;grecaptcha-error\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;textarea id=\u0026#34;g-recaptcha-response-1\u0026#34; name=\u0026#34;g-recaptcha-response\u0026#34; 〜略〜 \u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; このままではPOSTデータに同名のパラメータが存在してしまうので正常に動きません。 そこで、クリックしなかったボタンに紐づくreCAPTCHA要素は削除してしまいます。 具体的には以下のようなコードになります。 \u0026lt;script\u0026gt; function mySubmit(delId){ document.getElementById(delId).parentNode.remove() document.getElementById(\u0026#34;myForm\u0026#34;).submit(); } function mySubmit1() { mySubmit(\u0026#34;g-recaptcha-response-1\u0026#34;) } function mySubmit2() { mySubmit(\u0026#34;g-recaptcha-response\u0026#34;) } \u0026lt;/script\u0026gt; \u0026lt;form id=\u0026#34;myForm\u0026#34; action=\u0026#34;/hoge\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;各自のサイトキー\u0026#34; data-callback=\u0026#34;mySubmit1\u0026#34;\u0026gt;送信1\u0026lt;/button\u0026gt; 〜いろいろ〜 \u0026lt;button class=\u0026#34;g-recaptcha\u0026#34; data-sitekey=\u0026#34;各自のサイトキー\u0026#34; data-callback=\u0026#34;mySubmit2\u0026#34;\u0026gt;送信2\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; これでPOSTデータにreCAPTCHAのパラメータが重複することはなくなるので 正常に認証することができるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1573/",
    title: "CircleCIの通知をSlackへ飛ばす",
    date: "2018-10-14T22:01:37Z",
    body: "CircleCIの通知をSlackへ飛ばす 参考サイト CircleCIの結果をSlackに通知する 以下のCircle CIの基本設定は完了している前提とします。 Circle CIでプルリク時にUnitテストが走るようにする 上記手順を実施しただけではCircle CIの通知はメールで届くので それをSlackに届くよう変更します。 Slackの設定 まず、SlackにCircle CIアプリをInstallします。 https://my.slack.com/apps/A0F7VRE7N-circleci Installをクリックすると、通知するチャンネルを選択する画面が表示されます。 好きなチャンネルを指定してください。 次の画面でCircle CIの設定画面にて何をすれば良いか説明してくれます。 Circle CIに設定するWebhookのURL（通知を飛ばすためのURL）が表示されているので確認しておいてください。 また、その説明の下に通知時の名前やアイコンを変更できる欄があるので、変更したければ変更してください。 Circle CIの設定 該当プロジェクトの画面にて設定(歯車アイコン)をクリックし、設定画面を開きます。 左メニューより『NOTIFICATIONS \u0026gt; Chat Notifications』を開き、 『Webhook URL』へ先ほどSlackの画面にて確認したWebhook URLを入力して『Save』をクリックします。 ここで『\u0026amp; Test Hook』をクリックすると設定したチャンネルにテスト通知が届きます。 ちゃんと設定できたか確認するために実行しておくと良いと思います。 上記のように「Hello from CircleCI」が届けば、設定完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1538/",
    title: "CircleCIで自動デプロイする",
    date: "2018-10-13T23:20:06Z",
    body: "CircleCIで自動デプロイする 参考サイト Circle CIでwebサイトを自動デプロイ GitHubのmasterブランチへマージされたらサーバーにデプロイされるようにする方法です。 ソースコードはPHPを前提としています。 こちらのCircleCIを動かす基本設定は完了している前提とします。 Circle CIでプルリク時にUnitテストが走るようにする CI用ユーザ準備 まず、CircleCIからデプロイするサーバーへSSH接続する為の鍵ペアを作成します。 既存の鍵ペアを使っても動きますが、念のためCI用は別にしておいた方が良いと思います。 ssh-keygen -t rsa -b 4096 -C \u0026#34;ci@hogehoge.com\u0026#34; -f ~/.ssh/ci-key 上記コマンドを実行するとパスフレーズを２回聞かれますが、どちらも空のままエンターで良いです。 鍵生成が完了したらホームディレクトリの.ssh配下にci-key(秘密鍵)とci-key.pub(公開鍵)が生成されます。 ci-key.pubは後工程で使いますので、中身を確認しておいてください。 ※上記コマンドで鍵ファイル名をci-keyと指定していますが、これは任意の名前で良いです。 次に、デプロイするサーバー上でデプロイする為のユーザーを作成し、 さきほど作成した鍵ペアでSSH接続できるようにします。 こちらも既存ユーザーを使っても動きますが、念のためCI用は別にしておいた方が良いと思います。 adduser -g nginx ciuser su - ciuser mkdir .ssh vim .ssh/authorized_keys // ci-key.pubの内容を貼り付け CircleCI管理画面設定 次にCircleCIのWEBコンソールで設定をします。 該当プロジェクトの画面にて設定(歯車アイコン)をクリックし、設定画面を開きます。 SSH Keyの設定 左メニューより『PERMISSIONS \u0026gt; SSH Permissions』を開き、『Add SSH Key』をクリックします。 ポップアップが開くので、以下を設定します。 Hostname : デプロイするサーバーのホスト名 Private Key : 最初に作ったci-keyの中身全て 完了したら、今登録した鍵情報のFingerprintが表示されるので確認しておきます。 （後工程のCircleCI設定ファイル編集時に使います） 環境変数の設定 後工程で編集するCircleCI設定ファイルで環境変数を使うことができるので設定します。 この作業は別に必須ではないです。 設定ファイルを使いまわしたい場合は、環境により変動する部分を環境変数にしておいてもいいかもしれません。 左メニューより『BUILD SETTINGS \u0026gt; Environment Variables』を開き、『Add Variable』をクリックします。 ポップアップが開くので設定したい変数名と値を登録します。 例えばUSER_NAMEという変数にciuserという値を設定する場合は下図のようになります。 CircleCI設定ファイル編集 前回作った.circleci/config.ymlに以下の行を追記していきます。 以下例では前手順の環境変数を多数利用していますが、config.ymlに値を直接書いても構いません。 まず、デプロイするサーバーへ接続するための鍵情報(fingerprint)を登録します。 環境変数には以下が設定されているとします。 KEY_FINGERPRINT : さきほど確認したSSH鍵のfingerprint - add_ssh_keys: fingerprints: - \u0026#34;${KEY_FINGERPRINT}\u0026#34; # 環境変数を使わない場合はこのようにベタがきすればよいです # - \u0026#34;a1:b2:c3:・・・:x8:y9:z0\u0026#34; 次にSSH接続がコケないように設定しておきます。 環境変数には以下が設定されているとします。 SSH_PORT : デプロイするサーバーのSSHポート HOST_NAME : デプロイするサーバーのホスト名 - run: name: Start ssh-keyscan command: | ssh-keyscan -p ${SSH_PORT} ${HOST_NAME} \u0026gt;\u0026gt; ~/.ssh/known_hosts 次にデプロイします。 環境変数は上記までに出てきたものに加えて以下が設定されているとします。 USER_NAME : デプロイするユーザー名(ciuser) APP_PATH : デプロイするパス(/var/www/htmlなど) ※${CIRCLE_BRANCH}にはCircleCIが自動で現在処理しているブランチを入れてくれます。 - deploy: name: Start master deploy command: | if [ \u0026#34;${CIRCLE_BRANCH}\u0026#34; == \u0026#34;master\u0026#34; ]; then rsync -av --delete --rsh=\u0026#34;ssh -p ${SSH_PORT}\u0026#34; --exclude=\u0026#39;.git\u0026#39; --exclude=\u0026#39;.circleci\u0026#39; ./ ${USER_NAME}@${HOST_NAME}:${APP_PATH} fi できあがったconfig.ymlはこのようになります。 # PHP CircleCI 2.0 configuration file # # Check https://circleci.com/docs/2.0/language-php/ for more details # version: 2 jobs: build: docker: # specify the version you desire here - image: circleci/php:7.1-fpm-node-browsers # Specify service dependencies here if necessary # CircleCI maintains a library of pre-built images # documented at https://circleci.com/docs/2.0/circleci-images/ # - image: circleci/mysql:9.4 working_directory: ~/repo steps: - checkout # Download and cache dependencies - restore_cache: keys: - v1-dependencies-{{ checksum \u0026#34;composer.json\u0026#34; }} # fallback to using the latest cache if no exact match is found - v1-dependencies- - run: composer install -n --prefer-dist - save_cache: paths: - ./vendor key: v1-dependencies-{{ checksum \u0026#34;composer.json\u0026#34; }} # run tests! - run: vendor/phpunit/phpunit/phpunit tests/ - add_ssh_keys: fingerprints: - \u0026#34;${KEY_FINGERPRINT}\u0026#34; - run: name: Start ssh-keyscan command: | ssh-keyscan -p ${SSH_PORT} ${HOST_NAME} \u0026gt;\u0026gt; ~/.ssh/known_hosts - deploy: name: Start master deploy command: | if [ \u0026#34;${CIRCLE_BRANCH}\u0026#34; == \u0026#34;master\u0026#34; ]; then rsync -av --delete --rsh=\u0026#34;ssh -p ${SSH_PORT}\u0026#34; --exclude=\u0026#39;.git\u0026#39; --exclude=\u0026#39;.circleci\u0026#39; ./ ${USER_NAME}@${HOST_NAME}:${APP_PATH} fi ※補足 SSHポートをデフォルト(22)のままにしているのであれば config.yml内のsshコマンドにてSSHポートを指定する必要はありません。 これで、masterにマージしたらデプロイが走るようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1521/",
    title: "akamaiのレスポンスヘッダーをChromeで確認する",
    date: "2018-09-26T12:02:33Z",
    body: "akamaiのレスポンスヘッダーをChromeで確認する 背景 今お手伝いをしている会社さんではakamaiを使ってページをキャッシュさせてます akamaiについてはコチラ https://www.akamai.com/jp/ja/ WEBページの開発をしていると、今自分が表示しているページが akamaiのキャッシュなのかどうなのか知りたい時があります Google Chromeの開発者ツールを使えば一般的なレスポンスヘッダーは見れますが akamaiのレスポンスヘッダーは特殊なので見えません curlコマンドにオプションをつけて叩く方法もありますが ログイン必須の画面をチェックするにはCookieの設定もしたりしなければいけません https://qiita.com/rasaka/items/fc96cd3b8f73a02477cb 解決策 そんなとき、Chromeの拡張機能『CDN Headers \u0026amp; Cookies』を使えば 簡単にakamaiのレスポンスヘッダーを見る事ができます https://chrome.google.com/webstore/detail/cdn-headers-cookies/obldlamadkihjlkdjblncejeblbogmnb 使い方は簡単です。 ①ストアからChromeに追加する ②チェックしたい対象ページを開く ③Chromeツールバーにいる黄色いやつをクリックする ④Response Headersタグを開きステータスを確認する ステータスの意味などはakamaiのコミュニティにて説明してくれていました https://community.akamai.com/customers/s/article/Akamairxdxn3?language=en_US"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1500/",
    title: "WindowsでDocker Toolbox＆docker-composeを動かす",
    date: "2018-09-21T17:55:45Z",
    body: "WindowsでDocker Toolbox＆docker-composeを動かす 仕事でWindows利用者にDocker環境を構築してもらう事があったのですが 結構苦労したので手順を残します。 ■□ 注意 ■□■□■□■□■□■□■□■□■□■□■□■□■□■□■□ MacやLinuxでdocker-composeを普通に使える前提で手順を書きます。 Dockerやdocker-compose自体の説明はあまり書きません □■□■□■□■□■□■□■□■□■□■□■□■□■□■□■□■□■□ もし、Windows10 64bit(Home以外)を使っている人であればこの記事の作業は不要です。 Docker ToolboxではなくDocker for Windowsをインストールしてください。 （たぶん、Docker for Windowsの方が楽に構築できると思われます） Docker Toolboxのインストール まずDocker Toolboxをダウンロードします。 以下のページにてDocker Toolboxをダウンロードしてください。 https://docs.docker.com/toolbox/overview/#ready-to-get-started ダウンロードしてきたEXEファイルを実行するとインストーラーが走りますので、 そのまま次へ次へと進みます。 インストールが完了すると『Docker Quickstart Terminal』というショートカットが デスクトップに作られるので実行します。 しばらく待つとDockerが立ち上がります。(クジラのAAが表示されます) docker-compose.ymlの作成 docker-compose.ymlを書きます。 基本的な書き方はMacやLinuxと同じなので割愛します。 気をつけなければいけないのはvolumesの書き方です。 相対パスで記載する場合は以下のように他OS同様の記載ができます。 volumes: ./php.ini:/etc/php.ini 次に絶対パスで記載する場合ですが、「C:¥」の部分は「/c/」と書きます。 例えば「C:¥develop¥app」にあるディレクトリを共有する場合は以下のような記載になります。 volumes: /c/develop/app:/var/www/html 後に出てくるVirtualboxの共有フォルダ設定次第ではこの通りにしない事も可能かもしれませんが、 できるだけシンプルに構築するため、このようにします。 もしMySQLのコンテナを作成する場合は データの永続化のためのホスト側ディレクトリを指定することはできません。 つまり、この書き方だとNGで services: mysql: 〜中略〜 volumes: - ./mysql/data:/var/lib/mysql この書き方ならOKです services: mysql: 〜中略〜 volumes: - mydb:/var/lib/mysql 〜中略〜 volumes: mydb VirtualBoxの設定 Docker Toolboxでコンテナを動かす場合に一番引っかかる部分かもしれません。 ポートフォワーディングの設定 docker-compose.ymlにてポートフォワーディングを8080:80と設定しただけでは ローカルPCにて http://localhost:8080にアクセスしてもDockerコンテナに リクエストは飛びません。 なぜかというと、WindowsでDocker Toolboxを使ってコンテナを動かした場合は 『Windows上でVirtualBoxにて仮想環境が作られ、その中でDockerコンテナが動く』 という仕組みになっているからです。 つまり、docker-compose.ymlに記載したポートフォワーディングは VirtualBox =\u0026gt; Dockerコンテナ でしかないため Windows =\u0026gt; VirtualBoxのポートフォワーディングは別で設定する必要があります。 まず、VirtualBoxを開き、『設定』をクリックします。 次に『ネットワーク \u0026gt; 高度 \u0026gt; ポートフォワーディング』と、クリックしていきます。 ポートフォワーディングルールが表示されるので、追加ボタンをクリックしてルールを追加していきます。 ここで設定しているのはWindows =\u0026gt; VirtualBoxのポートフォワーディングなので、docker-compose.ymlに記載したポートフォワーディングが8080:80だとしたら ここで設定するのは8080:8080です。 全体で見ると『Windows(8080) =\u0026gt; VirtualBox(8080) =\u0026gt; Docker(80)』と ポートフォワーディングされていくイメージです。 共有フォルダの設定 ホストPCのどのフォルダをDockerと共有するかを設定します。 Docker for Macにある「File Sharing」の設定と同じです。 VirtualBox設定画面にて「共有フォルダー」を選択し、追加ボタンをクリックすると共有フォルダが追加できます。 C:¥Users はデフォルトで共有になっているので、ホームディレクトリ以外にソースコードを置いた場合に設定が必要となります。 追加画面では以下のように設定します ■フォルダーのパス 共有したいフォルダーのパスです。 右側の「∨」マークをクリックすれば選択ウィンドウが開きます。 ■フォルダー名 共有フォルダに名称を付けます。 基本的にはフォルダーのパスをスラッシュ区切りにしたものにしておけば分かりやすいです。 例えば、『C:¥develop』を共有したい場合は下図のようになります。 これでVirtualBoxを再起動し、docker-compose up -d をすればコンテナが立ち上がりました"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1477/",
    title: "CircleCIでプルリク時にUnitテストが走るようにする",
    date: "2018-08-14T01:30:40Z",
    body: "CircleCIでプルリク時にUnitテストが走るようにする CircleCIを使って、GitHubにてプルリクエストを投げた際に Unitテストが走るようにします。 ソースコードはPHPで、Unitテストにはphpunitを使っている前提とします。 Circle CIのアカウント登録 Circle CIのホームページにアクセスし、右上のSign Upをクリックします。 GitHubかBitBucketのアカウントでログインできるようです。 今回はGitHubのアカウントでログインしました。 連携を許可するか聞かれるのでAuthorize circleciをクリックします。 Circle CIにプロジェクト作成 アカウント作成が完了したらプロジェクト作成するためAdd Projectsをクリックします。 紐付けるリポジトリのSet Up Projectをクリックします。 対象リポジトリの環境を設定します。 該当するものをクリックします。 下にスクロールすると今後の流れが書いてあります。 まず該当リポジトリに.circleci/config.ymlを作成します。 config.ymlの内容はとりあえず内容はCopy To Clipboardでコピーしたもの(2018/8/11現在)に 以下の修正を加えたものにしています。 ①Dockerイメージ変更 Dockerイメージを自分のサーバーの環境に近いものに変更します。 どのようなイメージがあるかは詳細はDockerHubで確認してください。 https://hub.docker.com/r/circleci/ (使いたいDockerイメージが別にであるなら、circleci公式のイメージじゃなくても動くらしい) なお、サンプルのconfig.ymlに書いてあったcircleci/php:7.1.5-browsersは存在しません。 サンプルのまま実行すると以下のエラーが発生します。 Error response from daemon: manifest for circleci/php:7.1.5-browsers not found ②phpunit実行コマンドを変更 composerなどでphpunitを入れている場合はコマンドが違うと思うので変更します。 出来上がったconfig.ymlが以下です # PHP CircleCI 2.0 configuration file # # Check https://circleci.com/docs/2.0/language-php/ for more details # version: 2 jobs: build: docker: # specify the version you desire here #- image: circleci/php:7.1.5-browsers - image: circleci/php:7.1-fpm-node-browsers # Specify service dependencies here if necessary # CircleCI maintains a library of pre-built images # documented at https://circleci.com/docs/2.0/circleci-images/ # - image: circleci/mysql:9.4 working_directory: ~/repo steps: - checkout # Download and cache dependencies - restore_cache: keys: - v1-dependencies-{{ checksum \u0026#34;composer.json\u0026#34; }} # fallback to using the latest cache if no exact match is found - v1-dependencies- - run: composer install -n --prefer-dist - save_cache: paths: - ./vendor key: v1-dependencies-{{ checksum \u0026#34;composer.json\u0026#34; }} # run tests! #- run: phpunit - run: vendor/phpunit/phpunit/phpunit tests/ これで作成したconfig.ymlをmasterリポジトリにpushしておき さきほどのCircle CIの画面にて『Start building』をクリックします。 これで登録は完了です。 該当リポジトリにてプルリクを投げると自動でphpunitのテストが走るようになります。 また、CircleCIの実行結果はデフォルトではCircleCIに設定したメールアドレスに届くようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1470/",
    title: "【Linux Mint】 Fcitxの変換候補が黒く塗りつぶされる",
    date: "2018-08-13T13:01:18Z",
    body: "【Linux Mint】 Fcitxの変換候補が黒く塗りつぶされる 環境 Linux Mint 18.3 cinnamon 64bit 事象 ある日、Fcitxで日本語入力していたら 変換候補が真っ黒になってしまいました Fcitx設定にて外観タブを見てみると、Kimpanelに設定オプションはありません とのこと。 解決方法 ちょっと調べてみた感じでは「Kimpanel」ってそもそも要らないようなのでアンインストールしてしまいます。 sudo apt purge fcitx-module-kimpanel これで一度ログアウトして再度ログインすると、真っ黒状態は治ります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1439/",
    title: "AWS ELB配下でWordpressを動かす際のSSL対応",
    date: "2018-07-20T17:56:57Z",
    body: "AWS ELB配下でWordpressを動かす際のSSL対応 ■環境 AWS ELB AWS EC2 Apache 2.2.32 Wordpress 4.1.1 EC-CUBEではありませんが、内容的にはほぼ以下の伊賀もの様の記事通りです。 続カッコの付け方 WordpressでもリクエストURLを見ており、httpsでアクセスした場合のみhttpsでCSSを取得するようになっているのですが AWS ELBがポートを80に変えてしまうため以下の事象が起こります。 ① ChromeでHTTPSのWordpressページを開く ② WordpressがCSSなどはHTTPで取得するよう返す ③ Chrome様が『HTTPSのページのリソースはHTTPSで取得せぇや！』と怒る ※ELB周りの話はこちらでも書いてます AWS ELB配下でApacheのRewriteRuleが上手く動かなかった話 対応方法上記記事同様に、Wordpress（Ver4.1.1時点）でも $_SERVER[\u0026lsquo;HTTPS\u0026rsquo;] を見ているので、 Apacheの設定（.htaccessでも可）で書き換えてしまえば解決です。 # ELBがhttpsで受けている場合はHTTPSを有効にする SetEnvIf X-Forwarded-Proto ^https$ HTTPS=on # httpアクセスはhttpsにリダイレクトする(設定するかは任意) RewriteEngine on RewriteCond %{HTTP:X-Forwarded-Proto} ^http$ RewriteRule ^(.*)$ https://%{HTTP_HOST}%{REQUEST_URI} [R,L]"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1436/",
    title: "grepでアクセスログからTPSを出す",
    date: "2018-07-05T15:28:23Z",
    body: "grepでアクセスログからTPSを出す ■環境 Amazon Linux AMI release 2013.03 grep 2.6.3 WEBサーバーのアクセスログからTPSを出すコマンドです。 ApacheでもNginxでも対応可能です。 アクセスログの時間フォーマットが05/Jul/2018:12:34:56のような形である場合 以下のコマンドでTPSが出ます。 ※出力されている時刻フォーマットに合わせて修正してください。 grep -o \u0026#34;05/Jul/2018:[0-9]\\{2\\}:[0-9]\\{2\\}:[0-9]\\{2\\}\u0026#34; access_log | sort | uniq -c ただし、これを２４時間分のアクセスログに対して実行すると行数が大変な事になるので その場合は時分秒のうち「時」を絞るなどして実行した方が良いです。 以下は１２時台のログに絞る例です。 grep -o \u0026#34;05/Jul/2018:12:[0-9]\\{2\\}:[0-9]\\{2\\}\u0026#34; access_log | sort | uniq -c また、正規表現部分を変えればTPS(秒間)ではなくTPM（分間）にすることも可能です。 grep -o \u0026#34;05/Jul/2018:[0-9]\\{2\\}:[0-9]\\{2\\}\u0026#34; access_log | sort | uniq -c"
  },
  {
    url: "https://blog2.logical-dice.com/tags/nginx/",
    title: "Nginx",
    date: "2018-07-05T15:28:23Z",
    body: "Nginx"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1423/",
    title: "AWS ELB配下でApacheのRewriteRuleが上手く動かなかった話",
    date: "2018-06-30T22:29:41Z",
    body: "AWS ELB配下でApacheのRewriteRuleが上手く動かなかった話 ■環境 AWS ELB AWS EC2 Apache 2.2.32 AWSのEC2上で動いているApacheサーバーでの話。 SEO対策で/index.htmlを/にリダイレクトしたかったので .htaccess に以下を設定しました。 RewriteRule ^(.*)index.html$ $1 [R=301,L] 検証インスタンスでは上手く動いたので、本番インスタンスにも反映させたところ https://〜/index.html(443ポート) が http://〜/(80ポート)にリダイレクトされるようになってしまいました。 なんでだろう？と思いながら以下のような分岐を作ってみましたが、結果は変わらずでした。 # 一旦プロトコルはhttpとしておく RewriteRule .* - [E=X_PRTCL:http] # リクエストプロトコルがhttpsの場合、リダイレクト先もhttpsにする RewriteCond %{HTTPS} on RewriteRule .* - [E=X_PRTCL:https] RewriteRule ^(.*)index.html$ %{ENV:X_PRTCL}://%{HTTP_HOST}/$1 [R=301,L] なぜ443ポート(https)の通信が80ポート(http)扱いになってしまったかというと 本番環境のEC2はELBを通しているのが原因でした。 つまり、検証環境では PC ↓443ポート EC2 だったのに対して、本番環境では PC ↓443ポート ELB ↓80ポート EC2 となっていたのでした。 ELBを経由した場合、HTTPプロトコルを判定するにはX-Forwarded-Protoというパラメータを見る必要があるようです。 https://docs.aws.amazon.com/ja_jp/elasticloadbalancing/latest/classic/x-forwarded-headers.html 以下のようにhtaccessへ記載すればELB経由でも上手く動きました # 一旦プロトコルはhttpとしておく RewriteRule .* - [E=X_PRTCL:http] # リクエストプロトコルがhttpsの場合、リダイレクト先もhttpsにする RewriteCond %{HTTPS} on [OR] # ELBを経由しない時用 RewriteCond %{HTTP:X-Forwarded-Proto} https # ELBを経由する時用 RewriteRule .* - [E=X_PRTCL:https] RewriteRule ^(.*)index.html$ %{ENV:X_PRTCL}://%{HTTP_HOST}/$1 [R=301,L]"
  },
  {
    url: "https://blog2.logical-dice.com/tags/cakephp/",
    title: "CakePHP",
    date: "2018-06-10T09:20:20Z",
    body: "CakePHP"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1375/",
    title: "CakePHP2のローカル検証環境構築",
    date: "2018-06-10T09:20:20Z",
    body: "CakePHP2のローカル検証環境構築 環境 CakePHP 2.10.10 今更ながらCakePHP2を仕事で使う可能性があったので、 勉強するためローカルに環境構築しました。 環境構築はDockerを利用しており、以下3つのコンテナを作成しています。 nginx php-fpm MySQL 【参考】出来上がったものはこちら https://github.com/mildjester/my-training-cakephp CakePHPのDocument関連は「Book」という名称で公開されていますので こちらを参考にして進めていきます。 （CakePHPは全体的に料理っぽい命名がされているので、レシピ本ってこと？） https://book.cakephp.org/2.0/ja/index.html CakePHPのインストール BookによるとCakePHP2のダウンロードは公式ページより行なえるそうなのですが、 現在の公式ページにはダウンロードできそうなリンクはありませんでした。 なので、GitHubからダウンロードしてきます。 私が見た時はVersion.2系の最新は「2.10.10」だったので、それの圧縮ファイルダウンロードしました。 https://github.com/cakephp/cakephp/tags ダウンロードしてきた圧縮ファイルを解凍したらCakePHP2の基本構造が出てきます。 これをDockerで作った環境で動くように突っ込みます。 Docker環境についてはコチラを参照してください。 https://github.com/mildjester/my-training-cakephp/tree/master/docker この状態でDockerを起動すればCakePHPの初期画面が表示されます。 初期設定 上記の初期画面で色々とWarning等が出ているので これの対応をしていきます。 Databaseの設定 /app/Config/database.php.default というファイルがあるので、それを同じディレクトリにdatabase.phpという名前でコピーします。 コピーしたファイルにデータベースの設定について記載されていますので、構築するDBの情報を記載します。 また、ここに記載するDB情報に合わせてMySQLにもDBやユーザーを作成しておきます。 （MySQLの操作については割愛） Coreの修正 /app/Config/core.php にセキュリティ関連のSaltやCipherSeedが設定されています。 初期値のままだと宜しくないので修正しておきます。 Configure::write(\u0026#39;Security.salt\u0026#39;, \u0026#39;DYhG93b0qyJfIxfs2guVoUubWwvniR2G0FgaC9mi\u0026#39;); // 任意の半角英数にする Configure::write(\u0026#39;Security.cipherSeed\u0026#39;, \u0026#39;76859309657453542496749683645\u0026#39;); // 任意の数字にする DebugKitの設定 CakePHP用のDebugKitがあるのでインストールしておきます。 https://github.com/cakephp/debug_kit/tree/2.2 まず、Composerでインストールし、生成されるPluginを/app/Plugin配下へ移動させます。 composer require cakephp/debug_kit \u0026#34;^2.2.0\u0026#34; mv Plugin/DebugKit app/Plugin/. もしローカルにComposerが入ってない場合は、php-fpmのDockerコンテナに入って実行しても構いません。 本例のDocker環境の場合は以下コマンドでコンテナに入れます。 docker-compose -f cakedock/docker-compose.yml exec --user=www-data workspace sh # または、上記コマンドをMakefile化してあるので、以下コマンドでも入れます make workspace 次に/app/Config/bootstrap.phpの69行目あたりにプラグイン読み込み設定を記載する箇所があるので そこにDebugKitを読み込む設定を追記します。 CakePlugin::load(\u0026#39;DebugKit\u0026#39;); ここまで対応すれば初期画面の警告はAllGreenになるはずです。 あとは各々、自分の開発を進めます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1363/",
    title: "Apacheのevent MPMのチューニング",
    date: "2018-05-26T11:28:56Z",
    body: "Apacheのevent MPMのチューニング preforkからeventに乗り換えるとチューニングの考え方が少し変わるので簡単にメモ。 event MPMではまずApacheの子プロセス（以下プロセス）が複数生成され、 さらに各プロセスの中にWorker Thread（以下スレッド）が複数生成されるイメージ。 設定値の意味は以下のようになる \u0026lt;IfModule mpm_event_module\u0026gt; ServerLimit 8 #プロセス数の最大値 StartServers 3 #プロセス数の初期値 MinSpareThreads 192 #空きスレッド数の最小値(下回ったら増やす) MaxSpareThreads 384 #空きスレッド数の最大値(上回ったら減らす) ThreadsPerChild 64 #プロセスあたりのスレッド数 MaxRequestWorkers 512 #同時に処理できる最大数 MaxConnectionsPerChild 5000 #プロセスが再起動までに処理するリクエスト数 \u0026lt;/IfModule\u0026gt; MaxRequestWorkersは最大スレッド数(ServerLimit×ThreadsPerChild)以下にしておくこと。 スレッド数以上のリクエストが来ると困る。 MinSpareThreadsは初期スレッド数(StartServers×ThreadsPerChild)以下にしておくこと。 初期スレッド数以上の空きスレッドが必要だと、起動直後にスレッド追加しなければならなくて無駄。 MinSpareThreadsとMaxSpareThreadsはThreadsPerChildの倍数になっていた方が良さそう。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/centos/",
    title: "CentOS",
    date: "2018-05-22T10:14:55Z",
    body: "CentOS"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1360/",
    title: "Linux環境のタイムゾーンを日本にする",
    date: "2018-05-22T10:14:55Z",
    body: "Linux環境のタイムゾーンを日本にする このコマンドを実行するだけ sudo ln -fs /usr/share/zoneinfo/Asia/Tokyo /etc/localtime"
  },
  {
    url: "https://blog2.logical-dice.com/tags/html/",
    title: "HTML",
    date: "2018-05-18T21:53:41Z",
    body: "HTML"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1352/",
    title: "数字入力フォームに文字数制限をつける",
    date: "2018-05-18T21:53:41Z",
    body: "数字入力フォームに文字数制限をつける HTML5で追加された\u0026lt;input type=\u0026quot;number\u0026quot; /\u0026gt;ですが、maxlengthの指定ができません。 なので、Javascriptで最大桁数の制御を入れてみます。 HTML 3桁の例 \u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;bangou3keta\u0026#34; oninput=\u0026#34;sliceMaxLength(this, 3)\u0026#34;\u0026gt; 5桁の例 \u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;bangou5keta\u0026#34; oninput=\u0026#34;sliceMaxLength(this, 5)\u0026#34;\u0026gt; Javascript function sliceMaxLength(elem, maxLength) { elem.value = elem.value.slice(0, maxLength); } また、\u0026lt;input type=\u0026quot;number\u0026quot; /\u0026gt;にすると右端に数字を増減するスピンボタンが表示されます。 電話番号の入力欄などでは不要なので、以下のCSSで非表示にしてしまいます。 CSS // Chrome、Safari input[type=\u0026#34;number\u0026#34;]::-webkit-outer-spin-button, input[type=\u0026#34;number\u0026#34;]::-webkit-inner-spin-button { -webkit-appearance: none; margin: 0; } // Firefox、IE input[type=\u0026#34;number\u0026#34;] { -moz-appearance:textfield; }"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1291/",
    title: "Wordpressを多言語対応させる",
    date: "2018-04-24T00:56:56Z",
    body: "Wordpressを多言語対応させる ■環境 Wordpress 4.9.5 MultilingualPress 2.11.1 ■手順 まずMultilingualPressというプラグインをインストールします。 インストール後、プラグインを有効化しようとするとマルチサイトを有効にしないといけない旨のメッセージが表示されます。 上記のリンク先にマルチサイト化をするための手順が記載されています。 https://multilingualpress.org/docs/install-wordpress-multisite/ まず、Wordpressがインストールされているディレクトリにあるwp-config.phpを編集し、マルチサイトを有効化します。 define(\u0026#39;WP_DEBUG\u0026#39;, false); define(\u0026#39;WP_ALLOW_MULTISITE\u0026#39;, TRUE); \u0026lt;=これを追記 /* 編集が必要なのはここまでです ! WordPress でブログをお楽しみください。 */ /** Absolute path to the WordPress directory. */ if ( !defined(\u0026#39;ABSPATH\u0026#39;) ) define(\u0026#39;ABSPATH\u0026#39;, dirname(__FILE__) . \u0026#39;/\u0026#39;); 次にプラグインを全て停止させ、管理画面より「ツール ＞ サイトネットワークの設置」を開き、『インストール』をクリックします。 すると、再度wp-config.phpを修正するよう案内が表示されるので、案内通りに修正します。 define(\u0026#39;WP_DEBUG\u0026#39;, false); define(\u0026#39;WP_ALLOW_MULTISITE\u0026#39;, TRUE); define(\u0026#39;MULTISITE\u0026#39;, true); \u0026lt;=これを追記 define(\u0026#39;SUBDOMAIN_INSTALL\u0026#39;, true); \u0026lt;=これを追記 define(\u0026#39;DOMAIN_CURRENT_SITE\u0026#39;, \u0026#39;your-domain.com\u0026#39;); \u0026lt;=これを追記 define(\u0026#39;PATH_CURRENT_SITE\u0026#39;, \u0026#39;/\u0026#39;); \u0026lt;=これを追記 define(\u0026#39;SITE_ID_CURRENT_SITE\u0026#39;, 1); \u0026lt;=これを追記 define(\u0026#39;BLOG_ID_CURRENT_SITE\u0026#39;, 1); \u0026lt;=これを追記 /* 編集が必要なのはここまでです ! WordPress でブログをお楽しみください。 */ /** Absolute path to the WordPress directory. */ if ( !defined(\u0026#39;ABSPATH\u0026#39;) ) define(\u0026#39;ABSPATH\u0026#39;, dirname(__FILE__) . \u0026#39;/\u0026#39;); また、.htaccessに次の記載をするよう指示がありますので、それも従います。 RewriteEngine On RewriteBase / RewriteRule ^index\\.php$ - [L] # add a trailing slash to /wp-admin RewriteRule ^wp-admin$ wp-admin/ [R=301,L] RewriteCond %{REQUEST_FILENAME} -f [OR] RewriteCond %{REQUEST_FILENAME} -d RewriteRule ^ - [L] RewriteRule ^(wp-(content|admin|includes).*) $1 [L] RewriteRule ^(.*\\.php)$ $1 [L] RewriteRule . index.php [L] nginxを使っている場合は.htaccessが効かないので、nginx.confに以下の記載を追加します。 rewrite ^/wp-admin$ /wp-admin/ redirect; if (!-e $request_filename){ rewrite ^/(wp-(content|admin|includes).*) /$1 break; rewrite ^/(.*\\.php)$ /$1 break; rewrite ^(.*)$ /index.php break; } nginxの設定ファイルを記載する際、こちらのコンバーターの出力結果を参考にしました。 https://winginx.com/ja/htaccess"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1245/",
    title: "【Linux Mint】gemでcompassをインストールする",
    date: "2018-04-20T16:23:01Z",
    body: "【Linux Mint】gemでcompassをインストールする 以下のコマンドでインストールしようとするとエラーが発生しました。 $ sudo gem install compass Fetching: sassa - 3.4.25.gem (100%) Successfully installed sassa - 3.4.25 Fetching: multi_json - 1.13.1.gem (100%) Successfully installed multi_json - 1.13.1 Fetching: compass-core-1.0.3.gem (100%) Successfully installed compass-core-1.0.3 Fetching: compass-import-once-1.0.5.gem (100%) Successfully installed compass-import-once-1.0.5 Fetching: chunky_png - 1.3.10 gem (100%) Successfully installed chunky_png-1.3.10 Fetching: rb - fsevent - 0.10.3.gem (100%) Successfully installed rb - fsevent - 0.10.3 Fetching: ffi - 1.9.23.gem (100%) Building native extensions. This could take a while ... ERROR: Error installing compass: ERROR: Failed to build gem native extension. Current directory: /var/lib/gems/2.3.0/gems/ffi-1.9.23/ext/ffi_c / usr / bin / ruby ​​2.3 -r ./siteconf20180420-24270-1pjlvkx.rb extconf.rb mkmf.rb can not find header files for ruby ​​at /usr/lib/ruby/include/ruby.h extconf failed, exit code 1 Gem files will remain installed in /var/lib/gems/2.3.0/gems/ffi-1.9.23 for inspection. Results logged to /var/lib/gems/2.3.0/extensions/x86_64-linux/2.3.0/ffi-1.9.23/gem_make.out 調べてみると、どうやらgemのアップデートを先にやらなければならないようでした。 https://ericdouglas.github.io/2016/08/21/Installing-Compass-on-Linux-Mint/ 以下のコマンドを実行するとインストールに成功しました。 $ sudo gem update - system $ apt install ruby-ffi $ sudo gem install compass"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1236/",
    title: "【Linux Mint】cpuminer + wpoolでBitZenyをマイニング",
    date: "2018-04-20T10:25:55Z",
    body: "【Linux Mint】cpuminer + wpoolでBitZenyをマイニング ■環境 Linux Mint 18.3 cinnamon 64bit cpuminerのインストール 基本的な流れはcpuminerのgithubページに記載されている通りです。 https://github.com/macchky/cpuminer 事前に必要な物をインストールします。 ※libjansson4は公式ページには記載がありませんが、ないとminerd実行時にエラーとなります。 apt install build-essential libcurl4-openssl-dev libjansson4 cpuminerバイナリをダウンロードして実行ファイルを取り出します。 wget https://github.com/macchky/cpuminer/releases/download/v2.6.0/ZNYminer260.zip unzip unzip ZNYminer260.zip chmod +x ZNYminer260/linux/minerd sudo mv ZNYminer260/linux/minerd /usr/local/bin/. rm -rf unzip ZNYminer260* BitZeny Walletの登録 BitZenyの受け取り用にWalletを作っておきます。 以下サイトにてWalletが作成できます。 https://bitzeny.jp アカウントを作成し、ログイン後の画面でWalletのアドレスが確認できます。 マイニングの開始 minerdを使ってマイニングを開始します。 wpoolに記載されている下記コマンドを実行すればマイニングが開始されます。 マイニング停止はCtrl+Cです。 https://wpool.work/ minerd -a yescrypt -o stratum+tcp://wpool.work:15022 -u {BitZeny Walletで取得したアドレス} マイニング中は以下のように結果が流れていきます。 「yay!!!」となっているところがマイニング成功です。 [2018-04-20 15:42:35] Stratum requested work restart [2018-04-20 15:42:35] thread 1: 32638 hashes, 0.76 khash/s [2018-04-20 15:42:35] thread 0: 1957 hashes, 0.77 khash/s [2018-04-20 15:42:35] thread 3: 4647 hashes, 0.77 khash/s [2018-04-20 15:42:35] thread 2: 33076 hashes, 0.77 khash/s [2018-04-20 15:42:45] thread 1: 6887 hashes, 0.74 khash/s [2018-04-20 15:42:45] accepted: 1364/1364 (100.00%), 3.05 khash/s (yay!!!) [2018-04-20 15:43:11] thread 3: 27706 hashes, 0.77 khash/s [2018-04-20 15:43:11] accepted: 1365/1365 (100.00%), 3.05 khash/s (yay!!!) [2018-04-20 15:43:14] thread 3: 1946 hashes, 0.77 khash/s [2018-04-20 15:43:14] accepted: 1366/1366 (100.00%), 3.05 khash/s (yay!!!) [2018-04-20 15:43:26] thread 3: 9160 hashes, 0.77 khash/s [2018-04-20 15:43:26] accepted: 1367/1367 (100.00%), 3.05 khash/s (yay!!!) [2018-04-20 15:43:30] thread 2: 42358 hashes, 0.77 khash/s [2018-04-20 15:43:30] accepted: 1368/1368 (100.00%), 3.05 khash/s (yay!!!) [2018-04-20 15:43:33] Stratum requested work restart [2018-04-20 15:43:33] thread 2: 1673 hashes, 0.77 khash/s [2018-04-20 15:43:33] thread 1: 36537 hashes, 0.76 khash/s [2018-04-20 15:43:33] thread 0: 44107 hashes, 0.77 khash/s [2018-04-20 15:43:33] thread 3: 5300 hashes, 0.77 khash/s"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1226/",
    title: "【Linux Mint】キーマップを変更する",
    date: "2018-04-13T21:33:50Z",
    body: "【Linux Mint】キーマップを変更する ■環境 Linux Mint 18.3 cinnamon 64bit Xmodmap設定 ホームディレクトリ直下に.Xmodmapというファイルを作成し、設定を記載していきます。 【.Xmodmapへの記載内容】 マウス中央クリック貼り付けを無効化 ThinkPadキーボードだと中央ボタンをクリックしながらポインタを動かす事が多いですが、 その時にちょいちょいペーストが走って邪魔なので無効化します。 .Xmodmapに以下を記載します。 pointer = 1 9 3 4 5 6 7 8 2 10 PageUp \u0026amp; PageDownをShiftにする PageUp \u0026amp; PageDownは使わないし指が当たると画面が動いて邪魔なので Shiftに変えてしてしまいます。 .Xmodmapに以下を記載します。 keycode 112 = Shift_R keycode 117 = Shift_R add Shift = Shift_R Caps LockをCtrlにする Caps Lockも使わないし指が当たると大文字入力になって邪魔なので こちらはCtrlに変えてしてしまいます。 .Xmodmapに以下を記載します。 remove Lock = Caps_Lock keysym Caps_Lock = Control_L add Control = Control_L 【.Xmodmapの反映】 .Xmodmap記載後、以下コマンドで適用されます。 $ xmodmap ~/.Xmodmap ただし、ログインのたびに上記コマンドを打つのは面倒なので 自動起動するよう設定します。 ①まず、任意のシェルスクリプトを作成し、xmodmap ~/.Xmodmapを記載する。実行権付与も忘れず。 ②『システム設定＞Startup Applications』を開く。 （「Startup Applications」は「自動起動するアプリ」と表記されている可能性もあり） ③『Custom Command』を追加し、①で作成したシェルスクリプトを選択する。 半角/全角切り替えをMac風にする 通常のキーボードについている『半角/全角』ボタンはトグル式で切り替わるので 現在「半角入力」なのか「全角入力」なのかが分からないと使いづらいです。 も そこで、普段使わない以下のキーをMac風に割り当てます。 無変換キー：半角入力にする 変換キー：全角入力にする カタカナ/ひらがなキー：誤タッチがあるので、これも全角入力にしてしまいます まず、メニューより『Mozcの設定』を開きます。 次に、一般タブのキー設定の『編集』をクリックします。 キー設定の一覧が表示されるので以下のように変更してOKをクリックします。 Hiragana：IMEを有効化 Henkan：IMEを有効化 Katakana：IMEを有効化 Muhenkan：IMEを無効化 ※モード別に複数箇所あります これでMac風に変更できました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1222/",
    title: "【Linux Mint】日本語入力できるようにする",
    date: "2018-04-13T21:28:47Z",
    body: "【Linux Mint】日本語入力できるようにする ■環境 Linux Mint 18.3 cinnamon 64bit Linux MintをEnglishでインストールした際の作業です。 日本語でインストールしてあれば、初めから日本語入力が可能なはずです。 設定手順 Fcitx + Mozcで日本語入力できるようにします。 まず「SystemSettins \u0026gt; Input Method」を開きます。 『Language support』にJapaneseがあるので、そこのInstallボタンをクリックします。 インストールが終わると『Input method』にてFcitxを選択できるようになるので選択して、 一旦マシンを再起動します。 再起動後、MenuよりFcitx　Configurationを開きます。 検索バーに『Fc』あたりまで入れると出てきます。 Input Methodの追加「＋」をクリックします。 input methodの一覧が出てくるので、Mozcを選択してOKをクリックします。 Input MethodにMozcが追加されるので、元々あったKeybord - Japaneseは削除(×)ボタンで消してしまいます。 これで日本語入力できるようになりました。 半角/全角キーで日本語/英語を入れ替えられます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1218/",
    title: "プログラミング向けフォントのインストール",
    date: "2018-04-13T21:21:39Z",
    body: "プログラミング向けフォントのインストール ■環境 Linux Mint 18.3 cinnamon 64bit ubuntu 19.10 macOS High Sierra 10.13.1 (LinuxでもMacでも基本的に同じ操作でいけるはずです) ■フォントのインストール プログラミングに向いていると評判の日本語対応フォント『Ricty Diminished』をインストールします。 確かに見やすいです。文字幅や文字の見分けやすさ（オーとゼロに違いとか）の点で良いです。 まずはフォント一式をダウンロードします。 ダウンロードする圧縮ファイルはこちらのページから確認できます。 ※(2019/11/14追記) URLが変わっていたので修正しました http://www.yusa.lab.uec.ac.jp/~yusa/ricty_diminished.html $ wget http://www.yusa.lab.uec.ac.jp/~yusa/ricty_diminished/ricty_diminished-4.1.1.tar.gz $ tar xf ricty_diminished-4.1.1.tar.gz 次にダウンロードしたフォントをフォントディレクトリへ移動します。 【Macの場合】 $ mv RictyDiminished*.ttf ~/Library/Fonts/. $ brew install fontconfig 【ubuntu, Linux Mintの場合】 $ mkdir -p ~/.fonts $ mv RictyDiminished*.ttf ~/.fonts/. 最後にフォントを読み込みます $ fc-cache -vf $ rm -f ricty_diminished-4.1.1.tar.gz これでIDEなどでRicty Diminishedを指定すれば反映されます。 少しオシャレ（？）なRicty Diminished Discordというフォントも指定できるようになりますので、お好みの方を選べば良いと思います。 ■おまけ 『Cica』というフォントも評価が良いので紹介しておきます。 https://github.com/miiton/Cica Ricty同様に文字幅や文字の見分けやすさが良く、Rictyと比べると少し丸みを帯びていて可愛いです。 以下の手順でインストールできます。 $ wget https://github.com/miiton/Cica/releases/download/v2.1.0/Cica_v2.1.0.zip $ unzip Cica_v2.1.0.zip $ mkdir -p ~/.fonts $ mv Cica-*.ttf ~/.fonts/. $ sudo fc-cache -fv $ rm -f Cica_v2.1.0.zip"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1192/",
    title: "【Linux Mint】PHP7.1のインストール",
    date: "2018-04-10T09:11:57Z",
    body: "【Linux Mint】PHP7.1のインストール $ set -x LC_ALL C.UTF-8 $ sudo add-apt-repository ppa:ondrej/php $ apt update $ apt install php7.1 php7.1-mbstring php7.1-mcrypt php7.1-mysql php7.1-xml php7.1-xmlrpc composer"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1162/",
    title: "【Linux Mint】Dockerの導入",
    date: "2018-04-09T21:06:39Z",
    body: "【Linux Mint】Dockerの導入 ■環境 Linux Mint 18.3 cinnamon 64bit Dockerのインストール dockerのGPGを追加 $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - dockerリポジトリ追加 $ sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable\u0026#34; dockerインストール $ apt update $ apt install docker-ce docker-composeのインストール $ apt install docker-compose sudo無しでdockerを使えるようにする 現在のユーザーをdockerというグループに所属させれば使えるようになるらしい。 sudo gpasswd -a $USER docker service docker restart これで、一度ログアウトすればsudo無しでdockerコマンドが使えるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1098/",
    title: "【Linux Mint】OSインストールと設定",
    date: "2018-04-07T14:01:56Z",
    body: "【Linux Mint】OSインストールと設定 ■環境 Linux Mint 18.3 cinnamon 64bit OS入手 公式ページからISOイメージをダウンロードします。 https://linuxmint.com/download.php こだわりが無ければ『Cinnamon 64bit』がオススメらしいので、それにします。 ダウンロード元に日本が無いので、近そうな台湾にします。 ※韓国はなぜか404になってしまった（2018/03/09現在） OSインストール ダウンロードしたISOをDVDなどに焼いて、インストールしたいPCに入れて起動します。 するとデスクトップが表示されInstallLinuxMintというアイコンがあるのでダブルクリックします。 後は画面指示に従ってインストールを進めるだけです。 私は以下のような設定で進めました。（特筆すべき部分のみ） 言語：ここはあえてEnglish　※2018/4/18追記 アプリケーションの文字体とかが中華文字みたいになるから日本語にした方が良さそう。 インストール方法：Clean InstallしたかったのでErase disk and install Linux Mint キーボードレイアウト：レノボキーボードはJapanese-JapaneseでOKでした Windowsとデュアルブートなどにする場合は、事前にWindows側でDiskに空き容量を作っておき インストール方法で Linux Mint をWindows Boot Managerとは別にインストール を選択すればよいです。 インストール後、デュアルブートにならない場合はBIOS設定でセキュアブートをDisableにします。 Update Managerの設定 右下のタスクバーよりUpdateManagerをクリックします。（盾みたいなアイコン） とりあえずLet me review sensitive updatesを選択してOKをクリックします。 『Do you want to switch to a local mirror?』と聞かれているのでOKをクリックします。 日本のどれかを選択します。 Refreshをクリックして最新化して、Install Updatesをクリックする。 アップデートが完了すると、さらにアップデートできるパッケージが複数表示されるので 必要に応じてアップデートしておく。 システムフォントの変更 デフォルトのフォントがあまり良くなかったので変更します。 ※インストール時に言語をEnglishにしてしまった場合のみです。 最初から日本語でインストールしておけば普通のフォントになりました。 まず「SystemSettins \u0026gt; Font」を開きます。 そこのFont Selectionで好きなフォントに変更できます。 個人的にはTakao Ex Gothic Regularが好きでした。 ホームディレクトリの英語化 Linux Mintを日本語でインストールした場合、ホームディレクトリ直下のディレクトリ郡が日本語になります。 CUIを使っているとディレクトリ名が日本語だと不便なので、以下のコマンドで英語にしておきます。 env LANG=C xdg-user-dirs-gtk-update 上記コマンド実行後「ディレクトリ名を変えてもいいですか？（日本語→英語）」と聞いてくるのでYesにします。 再ログオン後、今度は「ディレクトリ名を変えてもいいですか？（英語→日本語）」と聞いてくるので それは『二度と聞かない』にチェックを入れてNoとします。 日本語入力の追加インストールと設定 Linux Mintを日本語でインストールしておけば初めから日本語入力ができますが、 一部日本語入力用のソフトが入っていないようなので、追加インストールします。 システム設定＞入力方法を開き、『日本語』をインストールします。 次にそのまま『言語』タブを開き、言語サポートの『言語のインストールと削除』をクリックします。 インストールウィンドウが開くので、日本語を選択して『言語パッケージのインストール』をクリックします。 次にメニューより『Fcitxの設定』を開きます。 入力メソッドでMozcの上に『キーボード - 日本語』がありますが、 これは邪魔なので削除してしまいます。 これで完了です。 基本ソフトインストール 開発基本セット $ apt update $ apt install vim git Slack Slackは『ソフトウェアの管理』からもインストールできますが、 そちらだと何故か日本語入力ができないので、公式サイトからDebをダウンロードしてきてインストールした方が良いです。 https://slack.com/intl/ja-jp/downloads/linux Shutter MacのSkitchのように画像へ矢印やコメントを挿入できるアプリです。 Skitchほどリッチなデザインにはなりませんが、画像へコメントを挿入するには充分です。 libgoo-canvas-perlをインストールしておかないと編集ができません。 $ apt install shutter libgoo-canvas-perl Chromium Google ChromeのOSS版です。 機能的にはChromeとほぼ変わりません。 chromium-browser-l10nをインストールすると日本語化されます。 apt install chromium-browser chromium-browser-l10n パネルの設定変更 （この手順はLinux Mint 19.1 MATEで実施した際に追記してます) 設定メニューの中にパネル（デスクトップの下にあるツールバー）を変更する項目が見つからなかったのですが どうやらパネルを右クリックして「プロパティ」を選択すれば変更できるようでした パネルのサイズを変更したり、パネルの横幅を縮めて中央寄せにする（Mac風）にすることもできるようです その他 日本語入力できるようにする プログラミング向けフォントのインストール terminator+fish+oh-my-fishの設定 PHP7.1のインストール Visual Studio CodeでPHP開発環境構築 dockerの導入 キーマップを変更する cpuminer + wpoolでBitZenyをマイニング"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/1071/",
    title: "docker-composeチートシート",
    date: "2018-01-29T13:35:42Z",
    body: "docker-composeチートシート 環境 macOS High Sierra 10.13.2 docker-compose 1.18.0 ■よく使いそうなコマンド yamlで指定したサービスを全て生成＆起動する docker-compose up -d ※-dオプションを付けるとバックグラウンドで起動する ビルドが必要なコンテナはビルドも走る 現在稼働しているコンテナを確認する docker-compose ps 現在稼働しているコンテナを全て停止＆削除する docker-compose down ■サービスを個別に操作するコマンド サービス指定でコンテナを生成する docker-compose create {サービス名} サービス指定でコンテナを起動する docker-compose start {サービス名} createとstartを１発で実施する docker-compose run -d {サービス名} サービス指定でコンテナを停止する docker-compose stop {サービス名} ※createやrunコマンド実行時は、指定したサービスにdepends_on等で紐づけられているサービスも一緒に起動する"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/979/",
    title: "Wordpressにお問合せフォームを作る",
    date: "2018-01-17T16:18:41Z",
    body: "Wordpressにお問合せフォームを作る 環境 Wordpress 4.9.1 Contact Form 7 4.9.2 WP Mail SMTP 1.2.2 Wordpressの固定ページにお問合せフォームを作成し、 問い合わせがあった場合はメールを飛ばします。 なお、利用するメールサーバは外部のメールサーバとします。 構築手順 ■プラグインのインストール 以下のプラグインをインストールし、有効化します。 Contact Form 7 WP Mail SMTP ※『Contact Form 7』は類似プラグインで『Contact Form7』(7の前にスペース無し)もあるので注意。 ■お問合せフォームの設定 インストール済みプラグイン一覧画面にて「Contact Form 7」の設定を選択します。 フォーム一覧画面が表示され、「コンタクトフォーム1」というフォームが自動で作られているので「編集」を選択します。 ここでフォーム名やフォームの内容を編集できます。 各タブの内容は以下の通りです。 フォーム WEB画面で表示するフォームの内容を設定できます アスタリスクを付けると必須項目になります メール フォームに入力された際に送信するメールに関する設定 送信元については後工程の「WP Mail SMTP」の設定にて実施するので設定不要です メッセージ フォームを入力した際に画面上で表示するメッセージ その他の設定 細かいカスタマイズもできるようだが、ここでは割愛します。 また、フォーム一覧画面に記載されている「ショートコード」は最終的に固定ページに埋め込むので控えておいてください。 ■メールサーバの設定 インストール済みプラグイン一覧画面にて「WP Mail SMTP」の設定を選択します。 ※左メニューより『設定 \u0026gt; WP Mail SMTP』と選択しても良いです。 送信元アドレス、送信者名を適宜入力し、メーラーは環境にあったものを選んでください。 メーラーがよく分からなければ「Other　SMTP」(よくあるメールサーバのホスト名とかを指定する方式)を選択すれば良いです。 あとは選んだメーラーに依って違う詳細設定項目が下に現れるので、そこを入力して『Save Settings』をクリックすれば設定完了です。 ■固定ページの作成 手順「お問合せフォームの設定」にて控えておいたショートコードを固定ページに記載するだけです。 必要に応じてdivタグで括ったりヘッダーを付けるなどの装飾をしてください。 以上で問い合わせフォームの作成は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/942/",
    title: "【CentOS】OpenVPN \u0026 LDAPでVPNサーバ構築",
    date: "2017-12-20T16:12:33Z",
    body: "【CentOS】OpenVPN \u0026 LDAPでVPNサーバ構築 環境 CentOS 7.4 OpenVPN 2.4.4 参考サイト ServerWorldさん：https://www.server-world.info/query?os=CentOS_7\u0026amp;amp;p=openvpn cloudpackさん：https://cloudpack.media/73 ※前提条件 LDAPサーバは既に構築済みとします サーバ作業手順 ■インストール 必要なものをインストールします。 yum --enablerepo=epel install openvpn openvpn-auth-ldap easy-rsa net-tools bridge-utils ■各種証明書生成 easy-rsaのディレクトリに移動し、各種証明書を生成します。 cd /usr/share/easy-rsa/2.0 CA 証明書 まず、varsファイルを修正します。 vim vars 内容は以下の通りです。(コメント行・空行は除外しています) export EASY_RSA=\u0026#34;`pwd`\u0026#34; export OPENSSL=\u0026#34;openssl\u0026#34; export PKCS11TOOL=\u0026#34;pkcs11-tool\u0026#34; export GREP=\u0026#34;grep\u0026#34; export KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA` export KEY_DIR=\u0026#34;$EASY_RSA/keys\u0026#34; export PKCS11_MODULE_PATH=\u0026#34;dummy\u0026#34; export PKCS11_PIN=\u0026#34;dummy\u0026#34; export KEY_SIZE=2048 export CA_EXPIRE=3650 export KEY_EXPIRE=3650 export KEY_COUNTRY=\u0026#34;JP\u0026#34; #国 export KEY_PROVINCE=\u0026#34;Tokyo\u0026#34; #都道府県 export KEY_CITY=\u0026#34;Shibuya-ku\u0026#34; #市区町村 export KEY_ORG=\u0026#34;AAA-Company\u0026#34; #組織名 export KEY_EMAIL=\u0026#34;fooo@aaa.com\u0026#34; #メールアドレス export KEY_OU=\u0026#34;BTeam\u0026#34; #チーム名など（任意） export KEY_NAME=\u0026#34;EasyRSA\u0026#34; CA証明書を生成します。 source ./vars ./clean-all ./build-ca 〜色々聞かれるが全てそのままエンターで良さそう〜 サーバー証明書 サーバ証明書を生成します。 ./build-key-server server 〜色々聞かれるが基本的にはそのままエンターで良さそう〜 〜途中でYes/Noを聞かれるので、そこは「y」と入力する〜 Diffie Hellman(DH) DHを生成します。 ./build-dh 各種証明書の移動 OpenVPNのディレクトリに生成した証明書達を移動します。 cp keys/ca.crt /etc/openvpn/. cp keys/server.crt /etc/openvpn/. cp keys/server.key /etc/openvpn/. cp keys/dh2048.pem /etc/openvpn/. ■OpenVPN設定 OpenVPNのサーバ設定をします。 cp /usr/share/doc/openvpn-2.4.4/sample/sample-config-files/server.conf /etc/openvpn/server.conf vim /etc/openvpn/server.conf 内容は以下の通りです。(コメント行・空行は除外しています) port 1194 proto tcp dev tun ca ca.crt cert server.crt key server.key dh dh2048.pem server 192.168.10.0 255.255.255.0 #OpenVPNサーバのサブネットマスク route 192.168.10.0 255.255.255.0 #クライアントに割り振るサブネットマスク＝serverと同じで良い push \u0026#34;redirect-gateway def1\u0026#34; #クライアントは全ての通信をVPN経由にする push \u0026#34;dhcp-option DNS 192.168.10.254\u0026#34; #DNSサーバの設定 keepalive 10 120 status /var/log/openvpn-status.log log /var/log/openvpn.log log-append /var/log/openvpn.log verb 3 plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026#34;/etc/openvpn/auth/ldap.conf\u0026#34; client-cert-not-required username-as-common-name LDAPサーバへの接続設定もします。 vim /etc/openvpn/auth/ldap.conf 内容は以下の通りです。(コメント行・空行は除外しています) \u0026lt;LDAP\u0026gt; URL ldap://ldap.sample.com #LDAPサーバへのURL Timeout 15 TLSEnable no FollowReferrals yes \u0026lt;/LDAP\u0026gt; \u0026lt;Authorization\u0026gt; BaseDN \u0026#34;ou=People,dc=sample,dc=com\u0026#34; #LDAPサーバのログインアカウント情報のDN SearchFilter \u0026#34;(\u0026amp;(uid=%u))\u0026#34; RequireGroup false \u0026lt;/Authorization\u0026gt; ■iptable設定 VPN接続してきたクライアントがVPNサーバ経由で外部連携するためにはiptableでマスカレードしてあげる必要があります。 openvpn起動時にiptable設定ができるようスクリプトを作っておきます。 起動スクリプト #!/bin/bash # 設定クリア /etc/openvpn/openvpn-shutdown # マスカレード設定(IPアドレスや利用するイーサネット等のIFは環境に合わせて変更) iptables -t nat -A POSTROUTING -s 192.168.10.0/24 -o eno1 -j MASQUERADE # VPNサーバからの送信許可設定 iptables -I OUTPUT -o tun+ -j ACCEPT iptables -I FORWARD -o tun+ -j ACCEPT 停止スクリプト #!/bin/bash # iptablesルール削除関数 delete() { rule_number=`iptables -L $target --line-numbers -n -v|grep tun.|awk \u0026#39;{print $1}\u0026#39;|sort -r` for num in $rule_number do iptables -D $target $num done } # 受信ルール削除 target=\u0026#39;INPUT\u0026#39; delete # 転送ルール削除 target=\u0026#39;FORWARD\u0026#39; delete # 送信ルール削除 target=\u0026#39;OUTPUT\u0026#39; delete ■起動 systemctl start iptables systemctl start openvpn sh /etc/openvpn/openvpn-startup クライアント作業手順 ■Tunnelblinkの取得 以下サイトよりダウンロード\u0026amp;インストールします。　https://tunnelblick.net/downloads.html　■VPN設定 サーバ側で生成したca.crtをクライアント側にダウンロードし、その同じディレクトリにsample.ovpnを作成します。 ※上記「sample」部分については任意の名前にしてください。それがTunnelblink上での接続先名になります。 sample.ovpnの中身は以下のようにしてください。 client auth-user-pass dev tun proto tcp ca ca.crt remote xxx.xxx.xxx.xxx 1194 #xxxはクライアントからVPNサーバへアクセスする際のIPアドレス この作成したovpnファイルをダブルクリックで開くとTunnelblinkが起動し、VPN設定がされる。 ■VPN接続 Tunnelblinkを起動し、上部常駐アイコンより「sampleに接続」を選択すると接続できる。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/914/",
    title: "Visual Studio CodeでPHP開発環境構築",
    date: "2017-12-18T14:02:41Z",
    body: "Visual Studio CodeでPHP開発環境構築 ■環境 Visual Studio Code 1.19.0 Linux Mint 18.3 cinnamon 64bit macOS High Sierra 10.13.1 (LinuxでもMacでも基本的に同じ操作でいけるはずです) ■Visual Studio Codeの取得 Visual Studio Code(以下、VSCode)本体をダウンロードします。 https://code.visualstudio.com/ 【Macの場合】 ダウンロードしてきたzipを回答するとアプリケーションファイル(.app)が出てくるので、アプリケーションディレクトリへ移動しておきます。 【Linux Mintの場合】 ダウンロードしてきたdebファイルを実行し、画面上の指示に従って進めるだけです。 ■フォント変更 VSCodeで使うフォントをプログラミング向けのフォント『Ricty Diminished』に変更します。 まずフォントそのものをPCにインストールします。 手順はコチラを参照してください。 PCにフォントがインストールできたら、VSCode側の設定をします。 まず、メニューの「Code　\u0026gt; 基本設定 \u0026gt; 設定」を開きます。 次に設定ファイルに以下の行を追加します。 \u0026#34;editor.fontFamily\u0026#34;: \u0026#34;\u0026#39;Ricty Diminished\u0026#39;\u0026#34;, これでフォントがRicty Diminishedになりました。 ■拡張機能インストール VSCodeを起動したら上部メニューより表示 \u0026gt; 拡張機能を選択します。 もしくは左側にあるアイコンの中の一番下にある四角いアイコンを選択します。 検索窓が表示されるので、キーワード検索をしてインストールすることが可能です。 以下、私がインストールしてみたものです。 Project Manager プロジェクト管理をするための拡張機能です。 現在開いているワークスペースをプロジェクトとして保存しておくことで プロジェクトの切り替えが簡単になります。 NetBeansやEclipseのように複数プロジェクトを同時に開いておくことはできないようです。 ■使い方 上部メニューの表示 \u0026gt; コマンドパレットを開き、検索バーにProject Managerと入力するとProject Manager用のコマンドが表示されます。 (コマンドパレットはCommand + Shft+Pでも開けます) 主に使うコマンドについては以下の通り。 コマンド 内容 Save Project 現在のワークスペースをプロジェクトとして保存 Edit Project 保存済みプロジェクトの編集 List Project to Open 保存済みプロジェクト一覧を表示し、選択したプロジェクトを開く ftp-simple ファイルやディレクトリをサーバへアップロードしたりダウンロードするための拡張機能。 FTPだけでなく、SFTPも使えます。 ■使い方 コマンドパレットに「ftp-simple」と入れれば候補がでます。 コマンドパレットについては上記「Project Manager」と同様なので割愛。 主に使うコマンドについては以下の通り。 | コマンド | 内容 | | Config | サーバ連携の設定 | | Save | サーバへのファイル・ディレクトリアップロード | | Download | サーバからのファイル・ディレクトリダウンロード | | Diff | ローカルとサーバーの差分を表示する | | Delete | サーバー上のファイル・ディレクトリを削除する | PHP IntelliSense PHPのハイライトや補完機能を追加する拡張機能。 インストールするだけで良いです。 Path Intellisense ディレクトリパスを入力する際に自動で補完してくれる拡張機能。 これもインストールするだけで良いです。 Bracket Pair Colorizer 相対する括弧が同じ色となるので、どこからどこまでが括弧で囲われているか分かりやすくなります。 若干エディタがカラフルになり過ぎるので、人によっては嫌いかも。 これもインストールするだけで良いです。 ■設定周り ファイル拡張子毎の言語設定 ファイルの拡張子を通常ではないものにしている場合、ハイライトや補完機能が使えない場合があります。 その場合は対象のファイルを開いた状態で右下の「プレーンテキスト」をクリックします。 ※既に他の言語が紐づいている場合は、その言語が表示されていると思います。 すると言語の設定画面が開きます。 同じ拡張子のファイル全てに反映させたい場合は検索バーの下にある「\u0026rsquo;.xxx\u0026rsquo;に対するファイルの関連付けの構成」をクリックします。 ここで言語を設定すれば、今後は選択した言語のハイライトなどが反映されるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/vscode/",
    title: "VSCode",
    date: "2017-12-18T14:02:41Z",
    body: "VSCode"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/894/",
    title: "【git】ローカルとリモートのブランチを削除するスクリプト",
    date: "2017-12-12T12:47:09Z",
    body: "【git】ローカルとリモートのブランチを削除するスクリプト マージ済みのブランチを削除する時に、いつもローカルブランチとリモートブランチの両方を削除するのが面倒だったのでスクリプト化しました。 リモートはorigin限定です。 git_del_br.shという名前で作った例です。 #!/bin/bash set +eu if [ $# -eq 1 ]; then # delete local branch loBrCheck=`git branch | grep \u0026#34;^[ ]*${1}\\$\u0026#34;` if [ ${#loBrCheck} -gt 0 ]; then echo \u0026#34;delete local branch.\u0026#34; git branch -D $1; fi # delete remote branch reBrCheck=`git branch -a | grep \u0026#34;^[ ]*remotes/origin/${1}\\$\u0026#34;` if [ ${#reBrCheck} -gt 0 ]; then echo \u0026#34;delete remote branch\u0026#34; git push --delete origin $1; fi else echo echo \u0026#34;##################################\u0026#34; echo \u0026#34; Usage: git_del_br.sh [branch name]\u0026#34; echo \u0026#34;##################################\u0026#34; echo fi"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/853/",
    title: "【CentOS】侵入検知したらSlack通知する",
    date: "2017-12-12T00:30:31Z",
    body: "【CentOS】侵入検知したらSlack通知する ■環境 CentOS 7.4 AIDE 0.15.1 AIDEという侵入検知ツールを導入し、検査結果をSlackに通知するようにします。 AIDEインストール yum install -y aide AIDE設定 vim /etc/aide.conf 95行目辺りより監視対象ディレクトリの設定があります。 設定値の記載方法は以下の通り。 /{DIR_PATH}/ {CHECK_METHOD} #監視対象に含める !/{DIR_PATH}/ #監視対象から除外する 上記の『DIR_PATH』の部分に対象のディレクトリパスを記載し、 『CHECK_METHOD』に監視方法を記載します。 監視方法については本設定ファイルの９５行目以前に記載があるので、そちらを参照してください。 とりあえずAIDE 0.15.1では「CONTENT_EX」にしておくのが無難そうです。 監視対象の設定が完了したら、AIDEのデータベースを初期化します。 ※以下コマンド実行後、結構待ちます。 aide -i 自動監視設定 AIDEでスキャンを実行し、結果をSlack通知するスクリプトを作成します。 仮に、/opt/aide_scan.shという名前で作るとします。 vim /opt/aide_scan.sh シェルスクリプトの中身は以下のようにします。 #!/bin/bash set +e ##### Settings ## AIDE LOGFILE=\u0026#34;/var/log/aide/aide.log\u0026#34; AIDEDIR=\u0026#34;/var/lib/aide\u0026#34; ## Slack URL=\u0026#34;https://hooks.slack.com/services/AAAAAAAA/BBBBBBBB/CCCCCCCC\u0026#34; TO=\u0026#34;#aide\u0026#34; EMOJI=\u0026#34;:cop:\u0026#34; NAME=\u0026#34;AIDE-COP\u0026#34; ##### Run AIDE Scan /usr/sbin/aide -u \u0026gt; $LOGFILE cp $AIDEDIR/aide.db.new.gz $AIDEDIR/aide.db.gz x=$(grep \u0026#34;Looks okay\u0026#34; $LOGFILE | wc -l) if [ $x -eq 1 ]; then MSG=\u0026#34;OKだよ\u0026#34; else MSG=\u0026#34;侵入検知！詳細は[${LOGFILE}]を見てね\u0026#34; fi ##### Send To Slack payload=\u0026#34;payload={\\\u0026#34;channel\\\u0026#34;: \\\u0026#34;${TO//\\\u0026#34;/\\\\\\\u0026#34;}\\\u0026#34;, \\\u0026#34;username\\\u0026#34;: \\\u0026#34;${NAME//\\\u0026#34;/\\\\\\\u0026#34;}\\\u0026#34;, \\\u0026#34;text\\\u0026#34;: \\\u0026#34;${MSG//\\\u0026#34;/\\\\\\\u0026#34;}\\\u0026#34;, \\\u0026#34;icon_emoji\\\u0026#34;: \\\u0026#34;${EMOJI}\\\u0026#34;}\u0026#34; curl -m 30 --data-urlencode \u0026#34;${payload}\u0026#34; $URL -A \u0026#39;zabbix-slack-alertscript / https://github.com/ericoc/zabbix-slack-alertscript\u0026#39; スクリプトの設定項目は以下の通りです。 項目 内容 LOGFILE AIDEの検査結果を出力するログファイルです。 AIDEDIR AIDE本体のディレクトリです。aide.confの設定に合わせてください。 URL Slackの『Incoming WebHooks』のURLです。詳細はコチラ TO メッセージを送るSlackのチャンネル EMOJI Slack上で表示される絵文字 NAME Slack上で表示される名前 作成したスクリプトを1日1回cronで回します。 以下はcronの設定例です。 0 10 * * * /bin/sh /opt/aide_scan.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 これで1日1回AIDEが侵入検知してくれるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/857/",
    title: "【Mac】PCの起動・停止時刻を出力するシェルスクリプト",
    date: "2017-12-11T23:51:12Z",
    body: "【Mac】PCの起動・停止時刻を出力するシェルスクリプト ■環境 macOS High Sierra 10.13.1 bash 4.4.12 職場の勤怠を付ける際に数日分まとめて入力するのですが、 出勤退勤時刻の目安とするため、Macの起動・停止時刻を出すようにしました。 まず、このシェルスクリプトはbash　Ver 4.x.x台以上でないと動かないので、 bashのバージョンが３の場合はHomebrewでインストールします。 ※執筆時点でMacにデフォルトで入っているbashはVer 3.x.xでした。 $ brew install bash $ which bash /usr/local/bin/bash シェルスクリプトを作成します。 仮でホームディレクトリ直下に「kintai.sh」という名前で作る事にします。 $ vim ~/kintai.sh シェルの内容は以下の通りです。 シバン（１行目のやつ）のパスは上記のwhichコマンドで確認したbashのパスとします。 #!/usr/local/bin/bash set +e declare -A rebootTimes declare -A shutdownTimes shutdownList=`last | grep \u0026#34;^shutdown\u0026#34; | cut -d \u0026#34; \u0026#34; -f 28-33` while read line; do curdate=`env LANG=eu_US.UTF-8 date -j -f \u0026#34;%a %b %d %H:%M\u0026#34; \u0026#34;${line}\u0026#34; \u0026#34;+%Y/%m/%d\u0026#34;` curtime=`env LANG=eu_US.UTF-8 date -j -f \u0026#34;%a %b %d %H:%M\u0026#34; \u0026#34;${line}\u0026#34; \u0026#34;+%H:%M\u0026#34;` if [ ${#shutdownTimes[${curdate}]} -eq 0 ]; then shutdownTimes[${curdate}]=${curtime} fi done \u0026lt;\u0026lt; END $shutdownList END rebootList=`last | grep \u0026#34;^reboot\u0026#34; | cut -d \u0026#34; \u0026#34; -f 30-35` while read line; do curdate=`env LANG=eu_US.UTF-8 date -j -f \u0026#34;%a %b %d %H:%M\u0026#34; \u0026#34;${line}\u0026#34; \u0026#34;+%Y/%m/%d\u0026#34;` curtime=`env LANG=eu_US.UTF-8 date -j -f \u0026#34;%a %b %d %H:%M\u0026#34; \u0026#34;${line}\u0026#34; \u0026#34;+%H:%M\u0026#34;` rebootTimes[${curdate}]=${curtime} done \u0026lt;\u0026lt; END $rebootList END firstDate=`date +\u0026#34;%Y/%m/01\u0026#34;` today=`date +\u0026#34;%d\u0026#34;` firstDate=\u0026#34;2017/12/01\u0026#34; for ((i=0; i \u0026lt; ${today}; i++)); do target=`date -v+${i}d -j -f \u0026#34;%Y/%m/%d\u0026#34; \u0026#34;${firstDate}\u0026#34; +\u0026#34;%Y/%m/%d\u0026#34;` echo \u0026#34;${target} : ${rebootTimes[${target}]} - ${shutdownTimes[${target}]}\u0026#34; done あとはこのシェルに実行権限をつけて実行するだけです。 $ chmod +x ~/kintai.sh $ ~/kintai.sh 2017/12/01 : 10:14 - 19:16 2017/12/02 : - 2017/12/03 : - 2017/12/04 : 10:10 - 20:01 2017/12/05 : 10:06 - 18:44 2017/12/06 : 09:48 - 18:21 2017/12/07 : 11:22 - 15:18 2017/12/08 : 09:47 - 19:07 2017/12/09 : - 2017/12/10 : - 2017/12/11 : 10:06 - 19:53 2017/12/12 : 10:12 - 起動時刻はその日に最初に起動した時刻、停止時刻はその日最後に停止した時刻を出力しています。 日を跨いで仕事をした場合は、どうしようもないです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/842/",
    title: "Dockerチートシート",
    date: "2017-11-28T16:22:31Z",
    body: "Dockerチートシート DockerHubにログイン まずこれをしておかないとDockerHubにて公開されているイメージを取ってこれない。 ※要DockerHubアカウント docker login Dockerイメージ作成 Dockerfileからイメージを作成する。 Dockerfileの作り方そのものは割愛。 docker build --no-cache=true -t {好きなイメージ名} {Dockerfileがあるディレクトリパス} 現在ローカルにあるイメージ確認 docker images イメージからコンテナ作成 docker run -itd -h {好きなホスト名} -v {ホストのディレクトリ}:{コンテナのディレクトリ} --name {好きなコンテナ名} -p {ホスト側ポート}:{コンテナ側ポート}　{イメージ名} {実行コマンド} ■補足 【vオプション】 ホストのディレクトリとコンテナのディレクトリを共有するオプション 例えば-v /home:/homeとすると、ホストのhomeディレクトリとコンテナのhomeディレクトリの中身が同期される。 【pオプション】 ホスト側にどのポートにアクセスがあった場合、コンテナのどのポートに流すかを設定する。 例えば-p 80:80とすればホストの80ポートへのアクセスはコンテナの80ポートへ流れる。　複数ポートを設定したい場合は-p 80:80 -p 443:443のように複数指定する事も可能。 また、ホスト側が複数IPアドレスを持っている場合などに-p 192.168.1.1:80:80のようにIPアドレスを指定する事も可能。 【実行コマンド】 Dockerfileにて実行コマンドを指定していた場合は省略可能。　特定のコマンドを実行したい訳でない場合は/bin/bashを指定しておくと無難。　コンテナ一覧を確認する docker ps -a -aオプションをつけないと停止しているコンテナは表示されない。 ホストからコンテナに入る docker exec -it {コンテナ名} {実行シェル} ■補足 【実行シェル】 どのシェルでログインしたいか。 分からなければbashにしておくと無難。 コンテナを停止する docker stop {コンテナ名} コンテナを起動する docker start {コンテナ名} コンテナを削除する コンテナが停止している状態で以下を実行。 docker rm {コンテナ名} ローカルのイメージを削除する 該当イメージを利用しているコンテナが存在しない状態で以下を実行。 docker rmi {イメージ名} コンテナをバックアップする docker export {コンテナ名} \u0026gt; {任意バックアップ名}.tar バックアップからコンテナをリストアする コンテナのリストアは、バックアップしたコンテナのイメージを作成するということ。 作成したイメージからコンテナを作ればリストアとなる。 cat {バックアップ名}.tar | docker import - {作成するイメージ名} docker run 〜以下略〜 ホストとコンテナでファイル転送 # ホストからコンテナ docker cp {コピー元ファイルパス} {コンテナ名}:{コピー先ファイルパス} # コンテナからホスト docker cp {コンテナ名}:{コピー元ファイルパス} {コピー先ファイルパス}"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/820/",
    title: "【AWS】新規EC2立ち上げ後の作業メモ",
    date: "2017-11-12T01:06:50Z",
    body: "【AWS】新規EC2立ち上げ後の作業メモ 環境 AWS EC2 Amazon Linux セキュリティグループ設定 SSH接続元やHTTP通信元の設定をする。 デフォルトではEC2を立ち上げた際のアクセス元に対してSSH接続が許可されているのみ。 立ち上げたEC2へのSSH接続 以下の通りSSH接続を実施する。 ログインアカウント： ec2-user 秘密鍵： EC2立ち上げ時にダウンロードしたpemファイル ※pemファイルはDL後権限を400にしておくこと。 rootになり、パスワードを変更する。 $ sudo su - # passwd 日本時刻にする。(rootのまま実施) # cp -p /usr/share/zoneinfo/Japan /etc/localtime # service rsyslog restart # service crond restart ※localtime変更後はrsyslogとcrondを再起動しないと、ログ出力時刻やcron実行時刻が日本時刻にならない。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/799/",
    title: "【Mac】圧縮解凍ソフトのインストール(Win対応)",
    date: "2017-10-13T19:48:22Z",
    body: "【Mac】圧縮解凍ソフトのインストール(Win対応) ■環境 macOS Sierra 10.12.6 Windowsで圧縮したファイルがMacで正常に開けなかったり Macで圧縮したファイルがWindowsで正常に開けなかったり そんな時はMac側で以下ソフトを使うと解決します。 Windows → Macの圧縮ファイル連携 『The Unarchiver』というソフトを使って解凍すると正常に解凍できます。 OneDriveからダウンロードした圧縮ファイルの解凍が正常にできない場合にも使えます。 MacのAppStoreで配信されていますので、検索してインストールしてください。 使い方は、解凍したい圧縮ファイルを『The Unarchiver』で開くだけです。 Mac → Windowsの圧縮ファイル連携 『WinArchiver Lite』というソフトを使って圧縮すると、Win側で正常に解凍できます。 こちらもMacのAppStoreで配信されています。 使い方は、圧縮したいファイル・ディレクトリを『WinArchiver Lite』で開く、 または『WinArchiver Lite』を起動しておいて、圧縮したいファイル・ディレクトリを『WinArchiver Lite』のウィンドウに ドラッグ＆ドロップすると、Win対応の圧縮ファイルが生成できます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/776/",
    title: "【Mac】ForiClientでVPNを繋ぐ",
    date: "2017-10-13T11:43:37Z",
    body: "【Mac】ForiClientでVPNを繋ぐ ■環境 macOS Sierra 10.12.6 FortiClient 5.6.0.703 ■本体のダウンロード \u0026amp; インストール 公式サイトよりインストーラをダウンロードする。 http://www.forticlient.com/downloads ダウンロードしてきたdmgファイルをダブルクリックするとインストーラが走るので、 あとはインストーラに従えばインストール完了 ■VPN設定 FortiClientを起動し、「リモートアクセス ＞ 歯車マーク ＞ 新規接続の追加」をクリックする。 接続先の設定をして、『追加』ボタンをクリックする。 その後、パスワードを入力して『接続』ボタンをクリックすれば接続される。 一度設定しておけば、今後の接続＆切断は右上のタスクバーのFortiClientアイコンから実施できる。 ※FortiClientアイコンは盾マークの中に、VPN接続時は鍵マーク、VPN切断時は四角いマークとなっている。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/737/",
    title: "【Mac】NetBeansのインストール",
    date: "2017-10-13T11:05:07Z",
    body: "【Mac】NetBeansのインストール ■環境 macOS Sierra 10.12.6 NetBeans 8.2 コードの補完やFTP機能を標準装備していたりと 無料のIDEの中では個人的には使いやすいと思っています。 あと、ねこびーんが可愛い。 ■JDK \u0026amp; NetBeans本体のダウンロード \u0026amp; インストール JDKとNetBeansがセットになっているパッケージがあるので、それをダウンロードします。 http://www.oracle.com/technetwork/java/javase/downloads/jdk-netbeans-jsp-142931.html 【補足】 既にJDKをインストールしている場合は、以下のサイトからNetBeans本体のみをダウンロードすれば良いです。 https://netbeans.org/downloads/ こちらのサイトからダウンロードする場合は、利用する言語に合わせたパッケージをインストールできます。 ダウンロードしてきたdmgファイルをダブルクリックしてマウントし、 出てきたpkgファイルをダブルクリックすると、インストーラーが走ります。 そのままインストーラーに従い進めるとインストール完了です。 LaunchpadにNetBeansが追加されてます。 ■NetBeansの設定 【基本設定】 上部メニューより「NetBeans ＞ Preferences」をクリックすると設定画面が開きます。 基本的に触るとしたら以下のタブくらいだと思います。 それ以外のタブをいじる人は、もはや説明も不要なレベルの人だと思うので説明も割愛します。 一般タブ NetBeansから開くブラウザの指定をしたり、プロキシの設定をしたりできます。 よく分からなければデフォルトのままでよいです。 エディタタブ ソースコードのフォーマットを指定したり、NetBeansが表示する警告を設定したりできます。 インデントをタブにするかスペースにするか。どのくらいインデントを深くすると警告を出すか。など 個人の好みやチームのコーディング規約に合わせて変更してください。 分からない項目はデフォルトのままで良いと思います。 フォントと色タブ タブ名のままですが、NetBeansで使うフォントと色を設定できます。 見づらい色などがあれば変更すれば良いです。 キーマップタブ ショートカットキーの設定です。基本的にはデフォルトのままでいいと思います。 どんなショートカットがあるのかを確認するために、一度眺めてみるのもいいと思います。 外観タブ IDEの見栄え全般の設定です。 「ルック・アンド・フィール」でテーマの変更ができるので、好みのテーマがありそうであれば変更してください。 ※ルック・アンド・フィールの反映にはIDEの再起動が必要です。 【プラグイン導入】 利用したい言語に合わせてプラグインをインストールします。 NetBeansの上部メニューより「ツール ＞ プラグイン」をクリックし、プラグイン管理画面を開きます。 「使用可能なプラグイン」というタブより新規プラグインのインストールができます。 暗いテーマが好きな場合 「Dark Look And Feel Themes」というプラグインがあります。 これをインストールすれば、基本設定にあった「ルック・アンド・フィール」の選択肢が増えるので、 設定画面から変更してください。 個人的には「Dark Nimbus」が好きです。 PHPを使う場合 PHPの開発をするなら「PHP」というプラグインは必須です。 あとはフレームワークやテストツールの補助プラグインもあるので、それらを必要に応じてインストールすれば良いです。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/netbeans/",
    title: "NetBeans",
    date: "2017-10-13T11:05:07Z",
    body: "NetBeans"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/750/",
    title: "【Mac】Homebrewインストール",
    date: "2017-10-12T16:27:26Z",
    body: "【Mac】Homebrewインストール ■環境 macOS Sierra(10.12.5) HomebrewはmacOS用のパッケージマネージャーです。 インストールするためのコマンドは公式ページにて記載されています。 http://brew.sh/index_ja.html ２０１７年１０月現在では以下のコマンドでインストールしました。 /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; これでbrewコマンドが使えるようになりました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/734/",
    title: "【Mac】PC購入後・初期化後の設定",
    date: "2017-10-12T16:23:00Z",
    body: "【Mac】PC購入後・初期化後の設定 初期状態のMacBookから行なう設定をまとめました ■設定一覧 Homebrewインストール ターミナル設定（iTerm2 ＋ fish ＋ fisherman） 秘密鍵＆公開鍵の生成 neovim \u0026amp; dein.vimの設定 ■とりあえず入れておくアプリ 圧縮解凍ソフト(Win対応) Calendar 2 Skitch"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/741/",
    title: "【Mac】ターミナル設定（iTerm2 ＋ fish ＋ fisherman）",
    date: "2017-10-12T16:01:40Z",
    body: "【Mac】ターミナル設定（iTerm2 ＋ fish ＋ fisherman） ■環境 iTerm2 3.2.5 fish 2.7.1 fisherman 3.1.1 ■参考サイト fish shell を使いたい人生だった ターミナルのシェルをiTerm2 \u0026amp; fishにする手順です。 デフォルトである程度便利機能が入っているので、zshでカスタマイズするのが面倒な人におすすめです。 iTerm2インストール 公式ページよりzipをダウンロードします。 https://www.iterm2.com/downloads.html ダウンロードしたzipを解凍すると「iTerm」というファイルが出てくるので、それをアプリケーションにドラッグ＆ドロップします。 これでLanchPadから起動できるようになりました。 iTerm2の配色変更 iTerm2のデフォルトのままの色だとディレクトリが見づらかったりするので配色を変えます。 iTerm2のメニューバーより iTerm2 \u0026gt; Preferences \u0026gt; Profiles \u0026gt; Colors とクリックしていき そこの Color Pareset より好きなテーマを選びます。 配色設定ファイルをダウンロード設定することもできます。 私の場合は『Japanesque』という配色をよく使っています。 https://github.com/aereal/dotfiles/tree/master/colors/Japanesque この『Japanesque.itermcolors』をiTerm2で開くと、上記の Color Paresetの選択肢に Japanesque が追加されます。 fish インストール fishをインストールします。 $ brew install fish fishのパスを確認する。 $ which fish /usr/local/bin/fish 上記パスをシェル一覧に追記する $ sudo vi /etc/shells 〜末尾にwhichコマンドで確認したパスを追記する〜 シェルをfishに変更する $ chsh -s /usr/local/bin/fish これで、ターミナルを再起動するとfishが立ち上がります。 fisherman インストール fishermanというfishのプラグイン管理ツールがあります。 fishermanのgithubベージにインストールコマンドが載っています。 https://github.com/fisherman/fisherman $ curl -Lo ~/.config/fish/functions/fisher.fish --create-dirs https://git.io/fisher 上記コマンド実行時にバージョン情報が出なかった場合は、一度ターミナルを再起動すると良いです。 fishのテーマ変更 fishはテーマを変更することで見栄えが変えられます。 どのようなテーマがあるかは、以下ページが参考になります。 https://github.com/oh-my-fish/oh-my-fish/blob/master/docs/Themes.md 私は「bobthefish」というテーマを利用しており、以下手順でインストールしています。 $ fisher add oh-my-fish/theme-bobthefish $ git clone https://github.com/powerline/fonts.git $ fonts/install.sh $ rm -rf fonts 上記コマンド実施後、iTerm2のプロファイルを設定します。 プロファイル設定画面にて「Text＞Font＞ChangeFont」をクリックしてください。 CollectionでAll Fontsを選択し、「○○○ for Powerline」というフォントの中から好きなものを選んでください。 個人的には「Meslo LG M for Powerline」が好きです。 フォントサイズも必要に応じて好きなサイズに変更してください。 これで、テーマの変更は完了です。 プラグイン追加 プラグインは色々ありますが 私が入れているプラグインを紹介しておきます。 z 過去の履歴からディレクトリを選択して遷移できるようになります。 【インストール方法】 $ fisher add z 【使い方】 z {文字列} まで入力してタブを押下する。 上記の{文字列}で履歴からディレクトリ検索をして遷移できる。 {文字列}を空にしておけば直近の履歴が表示される。 bd 親以上のディレクトリ(祖先ディレクトリ？)への遷移が簡単になる。 【インストール方法】 $ fisher add 0rax/fish-bd 【使い方】 $ bdと入力してタブを押すと、親ディレクトリの一覧が表示されるので 選択してエンターを押すだけで移動できる。 peco コマンド履歴の検索が容易にできるようになる。 【インストール方法】 $ brew install peco $ fisher add oh-my-fish/plugin-peco 上記コマンド後、fishのコンフィグファイルへ以下を記載。 vim ~/.config/fish/config.fish 〜以下を追記(ファイルがなければ新規作成)〜 function fish_user_key_bindings bind \\cr \u0026#39;peco_select_history (commandline -b)\u0026#39; end ※「function fish_user_key_bindings」の定義が既にされているなら、 その中にbind〜の行だけ追記すれば良い。 上記完了後、ターミナルを再起動してください。 【使い方】 「Ctrl + R」でコマンド履歴検索画面になる。 検索画面では直近のコマンドから選択することもできるし、文字列を入力すれば該当の文字列を含むコマンド履歴に絞ることもできる。 ghq gitの管理が容易になるプラグイン 【インストール方法】 $ brew install ghq $ fisher add yoshiori/fish-peco_select_ghq_repository 上記コマンド後、fishのコンフィグファイルへ以下を記載。 vim ~/.config/fish/config.fish 〜以下を追記(ファイルがなければ新規作成)〜 function fish_user_key_bindings bind \\c] peco_select_ghq_repository # 追加 end ※「function fish_user_key_bindings」の定義が既にされているなら、 その中にbind〜の行だけ追記すれば良い。 例えば、もし上記のpecoも導入しているのであれば、config.fishの記載は以下のようになる。 function fish_user_key_bindings bind \\cr \u0026#39;peco_select_history (commandline -b)\u0026#39; bind \\c] peco_select_ghq_repository # 追加 end 上記完了後、ターミナルを再起動してください。 【使い方】 「Ctrl + ]」でghqで管理しているgitリポジトリの選択ができます。 gitリポジトリを選択すると、該当リポジトリのディレクトリへ遷移します。 ghqコマンドの使い方は割愛するので、別途調べてください。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/721/",
    title: "Google Cloud Platformにインスタンス作成",
    date: "2017-10-08T00:10:11Z",
    body: "Google Cloud Platformにインスタンス作成 参考 https://www.topgate.co.jp/category/gcp インスタンス立ち上げ ① 左メニューの「Compute Engine」をクリックする。 ②1分ほど読みこみ待ちになる。 読み込み完了後、VMインスタンスの「作成」ボタンをクリックする。 ③また、少々待ちが発生する。 インスタンスの作成画面が表示されたら、名前に任意の名称、ゾーンは「asia-northeast1-a」を選択する。(東京リージョンらしい) 項目 設定値 名前 任意の名称をつける。 ゾーン 「asia-northeast1-a」を選択する。(東京リージョンらしい) マシンタイプ 必要なスペックに合わせて選択する。安さ重視ならmicro。 ブートディスク 入れたいOSを選択する。 IDとAPIへのアクセス このインスタンスへ付与する権限のよう。とりあえずデフォルトのままにする。 ファイアウォール HTTPやHTTPSを使うならチェック 管理 とりあえずデフォルトのまま ディスク とりあえずデフォルトのまま ネットワーキング とりあえずデフォルトのまま SSH認証鍵 自分のローカルPCの認証鍵を登録しておく ④ここまでできたら、一番下の「作成」ボタンをクリックする。 しばらく処理待ちとなるが、処理が完了するとインスタンス生成され、グローバルIPも振られる これでインスタンスの立ち上げは完了です。 SSH鍵登録をしたローカル環境からグローバルIPを指定してSSH接続できるようになっています。 ssh 999.999.999.999 ※グローバルIPアドレスはインスタンス一覧画面で確認可能"
  },
  {
    url: "https://blog2.logical-dice.com/tags/atom/",
    title: "Atom",
    date: "2017-09-29T19:11:15Z",
    body: "Atom"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/708/",
    title: "Atom IDEの環境構築（PHP、Go）",
    date: "2017-09-29T19:11:15Z",
    body: "Atom IDEの環境構築（PHP、Go） ■環境 macOS Sierra 10.12.6 Atom 1.21.0-beta2 ■参考にしたサイト 公式サイト https://ide.atom.io/ はくたけ氏のブログ http://tech.innovator.jp.net/entry/2017/09/14/165139 sayama0402さんのQiita https://qiita.com/sayama0402/items/3bd8d905619c514179b7 ナウいエディタ「Atom」は、前から気になっていたのですが、 テキストエディタにしては重い IDEにしては機能がすくない という理由で、少し敬遠していました。 が、そのAtomからIDEが出たそうなので、試しに使ってみました。 ■手順 ①Atomの入手 以下サイトより、Atom 1.21.0（ベータ版）を入手してインストールする。 https://atom.io/beta ※いくつかの紹介ページを見ましたが、1.21.0が推奨されているようです。 試しに1.20.0(安定板)でも試して見ましたが、確かに動きが怪しかったです。 ②必須パッケージのインストール Atomを起動したら、上部メニュー Packages \u0026gt; Settings View \u0026gt; Install Packages/Themes より、以下を検索してインストールする。 atom-ide-ui ide-php ide-go ③使って見る。 初めてgoのソースコードを開いた際には「ide-go」が依存するパッケージをインストールするか聞かれるので、 インストールします。 また、「linter」と「atom-ide-diagnostics」が競合している旨のメッセージが出るので、 そのメッセージの下にある「Disable linter」をクリックして、競合を解決します。 これで、実際に使い始められそうです。 定義ジャンプや補完機能は十分使えそうなレベルで動いてました。 また、他の無料IDE（EclipseやNetBeans）に比べると、起動が早かったです。 その他パッケージのインストール IDEとは関係なく便利なパッケージが沢山あるので入れます。 以下、実際に入れて見たパッケージです。 ■file-icons 各ファイルの拡張子に合わせてアイコンを変えてくれる。 ちなみに、Atomに新拡張子を紐付けさせるには、上部メニューより「Atom Beta＞Config」をクリックし、 config.csonを編集する必要がある。 【例：以下の記載で「.inc」という拡張子をPHPファイルとして認識してくれる】 \u0026#34;*\u0026#34;: core: customFileTypes: \u0026#34;source.php\u0026#34;: [ \u0026#34;inc\u0026#34; ] 『source.php』に該当する部分はPHPファイルを開いた状態で「cmd + opt + P」を押すとポップアップで表示される。 別拡張子のファイルでも同じ操作でできるはず。 ■pigments 色コードを記載したら自動で該当の色でハイライトしてくれる。 ■show-ideographic-space 全角スペースを見える化 ■git-plus ターミナルなしでgit操作ができる！だそうです。 ■merge-conflict gitのコンフリクト解消が楽になる。 ■highlight-selected 選択した単語と同じ単語をソースコード上でハイライトしてくれる。 選択=ダブルクリックらしい。同じ変数を探す時などに便利。 ■linter-php 構文チェックしてくれるパッケージ。 括弧の開閉数が不一致の時とか教えてくれる。 紐づくパッケージを諸々インストールしてねと出てくるので、それらもインストールする。 ※AtomIDEにも標準で構文チェックパッケージ(?)が入っているらしく、それと競合すると警告が表示される。 IDE標準のものはエラー検知をしてくれているのかよく分からなかったので、私は無効化してしまいました。 （上記警告にDisableボタンが付いているので、それをクリックするだけ） ■project-manager 複数プロジェクトを管理するためのパッケージ。 プロジェクト切り替えが容易になる。 【管理対象のプロジェクトを増やす方法】 対象としたいプロジェクトを開いた状態で上部メニューより「packages \u0026gt; Project Manager \u0026gt; Save Project」を選択 【管理対象プロジェクトの呼び出し】 「Ctrl + Command + P」で一覧が表示されるので、そこで選択する。 上部メニューより「packages \u0026gt; Project Manager \u0026gt; List Project」を選択しても同じ。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/golang/",
    title: "Golang",
    date: "2017-09-29T19:11:15Z",
    body: "Golang"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/703/",
    title: "【Mac】コマンドラインから写真の撮影日時などを確認する",
    date: "2017-09-24T15:27:02Z",
    body: "【Mac】コマンドラインから写真の撮影日時などを確認する ■環境 macOS Sierra 10.12.6 exiftool 10.55 Macのコマンドラインから写真の撮影日時を確認する方法が知りたかったため調べてみました。 最終目標はファイル名に撮影日時を付与する事です。 exiftoolインストール 写真の情報を取り出すには、exiftoolというツールを使います。 brewでインストールできます。 brew install exiftool 写真の詳細情報を確認してみる 単純に exiftool 対象ファイル というコマンドでファイルの全詳細情報は取れます。 撮影日時だけに絞りたい場合は exiftool -DateTimeOriginal 対象ファイル とすれば絞れます。 が、 どうやら使っているカメラによって撮影日時情報が画像データに含まれていない場合や、 上記で指定した「Date/Time Original」以外のタグ名で存在する場合があるようです。 その場合は exiftool 対象ファイル で全情報を表示して、それっぽいのを選ぶしか無さそうです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/698/",
    title: "【Wordpress】テーマで指定されたCSSがうまく読み込めない",
    date: "2017-08-21T01:57:10Z",
    body: "【Wordpress】テーマで指定されたCSSがうまく読み込めない Wordpressで他人が作成したテーマを使っているとCSSがうまく読み込めない時があります。 条件としては以下の両方に当てはまっている場合だと思います。 Wordpressの管理画面と公開画面でドメインが違う テーマ内にstyle.css以外のCSSファイルがあり、それをwp_enqueue_styleを使って読み込んでいる このとき、ブラウザの開発ツールでコンソールを表示すると、以下のようなエラーが発生していると思います。 Access to Font at \u0026#39;http://yourdomain1.com/wp-content/themes/sparkling/assets/fonts/glyphicons-halflings-regular.woff2\u0026#39; from origin \u0026#39;http://yourdomain2.com\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. Origin \u0026#39;http://yourdomain2.com\u0026#39; is therefore not allowed access. クロスドメインなどが原因のようなのですが、手早く直したかったので 以下の修正をして直しました。 ①wp_enqueue_styleをしている箇所をコメントアウトする。(テーマによるが、function.phpなどに記載あり) // wp_enqueue_style( \u0026#39;bs\u0026#39;, get_template_directory_uri() . \u0026#39;/assets/css/bootstrap.min.css\u0026#39; ); // wp_enqueue_style( \u0026#39;awe\u0026#39;, get_template_directory_uri() . \u0026#39;/assets/css/font-awesome.min.css\u0026#39; ); ②htmlのheadタグ内にてCSS読み込み処理を記載する。(テーマによるが、header.phpなどに記載あり) ※以下例ではCDNを指定しています。 \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.min.css\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\u0026#34;\u0026gt; これで正常にCSSが読み込まれ、ブラウザのコンソールからもエラーが消えました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/689/",
    title: "【Mac】言語パッケージ管理用にasdfを導入する(fish環境)",
    date: "2017-08-19T17:11:43Z",
    body: "【Mac】言語パッケージ管理用にasdfを導入する(fish環境) ■環境 macOS Sierra 10.12.6 fish 2.5.0 ■手順 Brewでもインストールできるらしいが、公式ページの手順に従った方が良さそう。 公式ページ：https://github.com/asdf-vm/asdf まず、asdfをgithubからクローンし、fishのコンフィグに追加する。 $ git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.3.0 $ echo \u0026#39;source ~/.asdf/asdf.fish\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish $ mkdir -p ~/.config/fish/completions; and cp ~/.asdf/completions/asdf.fish ~/.config/fish/completions asdfで必要になるアプリケーションをbrewでインストールしておく。 下記はだいたい公式サイトで記載されているもの。 $ brew install coreutils automake autoconf openssl libyaml readline libxslt libtool unixodbc また、言語別で必要になるアプリケーションもある。 詳細は言語別のgithubページを参照 https://github.com/asdf-vm/asdf-plugins 【メモ】 icu4cは2017年10月現在、最新バージョンを入れてしまうとPHPビルドできなくなってしまうので、旧バージョンをインストールした方が良いらしい 参考：https://github.com/phpbrew/phpbrew/issues/899 asdfで管理対象の言語を追加する。 $ asdf plugin-add {任意の名前} {githubリポジトリ} ===== (例） asdf plugin-add go https://github.com/kennyp/asdf-golang 上記で設定するリポジトリは以下を参照 https://github.com/asdf-vm/asdf-plugins もしここでエラーが出てしまった場合は必要なアプリケーションが入っていない可能性があるので、 エラー文言を参考にbrewインストールしておく。 インストールできるバージョン一覧を見る。 $ asdf list-all {さきほどの名前} ===== (例） asdf list-all go 上記で確認したバージョンの中から希望のバージョンをインストールし、 利用できるように設定する。 $ asdf install {さきほどの名前} {バージョン} ===== (例） asdf install go 1.8.4 $ asdf global {さきほどの名前} {バージョン} ===== (例） asdf global go 1.8.4 以上で設定完了です。 その他コマンドは公式ページを参照。チートシートがいらないくらいまとまってます。 https://github.com/asdf-vm/asdf"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/677/",
    title: "【node】foreverチートシート",
    date: "2017-07-31T01:44:06Z",
    body: "【node】foreverチートシート 起動 forever start {jsファイル} 起動中一覧表示 forever list 停止 forever stop {停止対象の番号} ※停止対象の番号は上記の一覧表示で確認できる。 0から順に配番されているはず。 ★その他★ 起動コマンド実行後にログを見ると Error: spawn ENOMEM というエラーで落ちていることがある。 これはメモリ不足が原因らしいので、メモリを確保してから再実行したら直る。 自分の環境の場合、空きが「600MB」くらいだとダメでした。 色々整理して、空きを「1.5GB」くらいまでにしたら直りました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/674/",
    title: "Vue.js + foreverでのアプリ起動",
    date: "2017-07-31T01:32:53Z",
    body: "Vue.js + foreverでのアプリ起動 ■環境 CentOS 7.3 npm 3.10.10 =====2017/12/27追記===== 久しぶりに本手順を試したところ、 vueプロジェクト内にbuild/dev-server.jsが存在しておらず foreverでの起動ができなくなっていました。 公式にならってnpm startコマンドなどで起動した方が良いかと思われます。 ===追記ここまで===== ■手順 npmとnodejsが入っていない場合は、yumでインストールします yum install npm nodejs vueアプリを簡単に導入できるツール「vue-cli」と nodeアプリをデーモン的に起動できる「forever」をインストールします ※これらはグローバル領域にインストールするので「-g」オプションをつけます npm install -g vue-cli forever プロジェクトディレクトリを作成します プロジェクトを作成したいディレクトリで以下を実行します vue init webpack samplevue ※「samplevue」は作成したいディレクトリ名 上記実行後に色々質問をされるので、作りたい環境に合わせて回答します 以下は回答例です ? Project name samplevue ? Project description This is Sample ? Author Djiro ? Vue build standalone ? Install vue-router? Yes ? Use ESLint to lint your code? No ? Set up unit tests No ? Setup e2e tests with Nightwatch? No ? Should we run `npm install` for you after the project has been created? (recommended) npm 作成したディレクトリに移動し、必要なファイルをインストールします cd samplevue npm install ここまで終わったら、foreverでvueアプリを起動できる状態になっています dev環境で起動したい場合、以下のコマンドで起動できます forever start build/dev-server.js ここで指定するjsファイルは、作成したプロジェクトディレクトリ直下にある『package.json』を見れば、７行目あたりから記載されています { \u0026#34;name\u0026#34;: \u0026#34;samplevue\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, 〜中略〜 \u0026#34;scripts\u0026#34;: { \u0026#34;dev\u0026#34;: \u0026#34;node build/dev-server.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;node build/dev-server.js\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;node build/build.js\u0026#34;, 〜以下略〜 これで、8080ポートにアクセスすればVueアプリが表示されるようになっています http://yourdomain.com:8080/ その他、foreverの利用方法についてはコチラ"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/665/",
    title: "【Mac】neovim \u0026 dein.vimの設定",
    date: "2017-07-15T00:44:50Z",
    body: "【Mac】neovim \u0026 dein.vimの設定 ■環境 macOS Sierra 10.12.6 fish 2.6.0 nvim 0.2.0 ■参考ページ NeoVim、そしてdein.vimへ：https://qiita.com/okamos/items/2259d5c770d51b88d75b Shougo氏のGiHub：https://github.com/Shougo/dein.vim 普通のvimよりも良いと噂のneovimを導入します。 プラグインの管理にはdein.vimを使います。 ■neovimのインストール brewでインストールできます。 便利ですね。 brew install neovim/neovim/neovim ■fishへの設定 コンフィグファイルのディレクトリを設定します。 以下はホームディレクトリ直下の『.config』を設定する例です。 echo \u0026#39;set -g XDG_CONFIG_HOME \u0026#34;$HOME/.config\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish ついでに、「vi」「vim」コマンド実行時にneovimが起動するようにしてしまいます。 これは好みだと思うので、やってもやらなくてもいいです。 echo \u0026#39;alias vi=\u0026#34;nvim\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish echo \u0026#39;alias vim=\u0026#34;nvim\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.config/fish/config.fish また、neovimで利用するカラースキームのデフォルト値を設定しておきます。 以下を~/.config/nvim/init.vimの先頭行に記載する。 上記ディレクトリ、ファイルは初期状態では存在しないので新規作成すること。 colorscheme {好きなカラースキーム} 設定できるカラースキームについては以下サイトにて紹介されているので参考になると思います。 https://qiita.com/ryoff/items/134b758afa8cc45a43d3 この辺でターミナルを再起動しておくと、上記で設定した内容が綺麗に読み込まれます。 ■dein.vimのインストール Shougo氏のGitHub通りに インストールを進めます。 curl https://raw.githubusercontent.com/Shougo/dein.vim/master/bin/installer.sh \u0026gt; installer.sh sh installer.sh ~/.config 上記コマンドが成功すると、.vimrc記載する内容が表示されるので、 それを少しだけ修正して~/.config/nvim/init.vimに追記します。 installer.shの結果出力された内容から変更する箇所は 下記の自動インストール処理のコメントアウトを外すくらいです。 \u0026#34;dein Scripts----------------------------- 〜中略〜 \u0026#34; If you want to install not installed plugins on startup. if dein#check_install() call dein#install() endif 〜中略〜 \u0026#34;End dein Scripts------------------------- ここまで実施すると次回nvim起動時にプラグインのインストールが行われ、 基本的な設定は完了します。 ■プラグインの追加 deinプラグインを追加してみます。 Tomlに記載して管理する方法もあるようですが、とりあえずは~/.config/nvim/init.vimに直接追加する方法について記載します。 The NERD Tree https://github.com/scrooloose/nerdtree エディタの左端にディレクトリ階層(ツリー)を表示できるようにするプラグインです。 (IDEみたいになる) ~/.config/nvim/init.vimの『\u0026quot; Add or remove your plugins here:』と記載があるあたりの下に以下の記載を追加します。 call dein#add(\u0026#39;scrooloose/nerdtree\u0026#39;) ~/.config/nvim/init.vimの末に以下の記載を追加します。 \u0026#34; The NERD Tree let g:NERDTreeDirArrowExpandable = \u0026#39;▸\u0026#39; let g:NERDTreeDirArrowCollapsible = \u0026#39;▾\u0026#39; map \u0026lt;C-n\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; nvim起動後、「:NERDTreeToggle」と打つか「Ctrl ＋ n」を押すと ファイルツリーの表示/非表示を切り替えられるようになります。 ※最後のmapコマンドで「Ctrl＋n」に「:NERDTreeToggle」をマッピングしています。 Indent Guides https://github.com/nathanaelkane/vim-indent-guides インデントを色付けして分かりやすくしてくれます。 ~/.config/nvim/init.vimの『\u0026quot; Add or remove your plugins here:』と記載があるあたりの下に以下の記載を追加します。 call dein#add(\u0026#39;nathanaelkane/vim-indent-guides\u0026#39;) ~/.config/nvim/init.vimの末に以下の記載を追加します。 \u0026#34;Indent Guide let g:indent_guides_enable_on_vim_startup = 1 ■その他オススメ設定 ~/.config/nvim/init.vimの好きな箇所に以下の設定も記載しておくと使いやすくなります。 set number \u0026#34;行番号表示 set fenc=utf-8 \u0026#34;文字コード設定 set cursorline \u0026#34;行に下線を表示 set hlsearch \u0026#34;検索語をハイライト set mouse=a \u0026#34;マウス利用可能にする set clipboard+=unnamed \u0026#34;Yankしたものをクリップボードにも反映する"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/656/",
    title: "【Let'sEncrypt】Certbotの使い方(CentOS7 + nginx)",
    date: "2017-06-12T17:17:21Z",
    body: "【Let'sEncrypt】Certbotの使い方(CentOS7 + nginx) ■環境 CentOS 7.3 nginx 1.10.3 certbot 0.14.1 ■初回SSL証明書発行 certbotをインストールする。 yum -y install yum-utils yum install certbot # もしCentOS6系やAWS　EC2にてcertbotをインストールする場合は以下手順となります。 curl https://dl.eff.org/certbot-auto -o /usr/bin/certbot-auto chmod 700 /usr/bin/certbot-auto ln -s /usr/bin/certbot-auto /usr/bin/certbot # certbot-autoのまま使っても良いですが、後述の手順と整合性を合わせるために # シンボリックリンクを作っています。 証明書を発行する certbot certonly --webroot -w /usr/share/nginx/html -d your-domain.com ※上記コマンドは環境に合わせて読み替えてください。 /usr/share/nginx/html →設定するドメインのドキュメントルート your-domain.com →設定するドメイン 【補足】 2017年11月現在、AWS　EC2では上記コマンドに\u0026ndash;debugオプションが必要でした。 調べてみるとEC2でも「\u0026ndash;debug」は不要になったと書いてあるページがあったりするので 環境による（？）のかもしれません。 これで証明書が以下ディレクトリに生成されます。 ll /etc/letsencrypt/live/your-domain.com/ lrwxrwxrwx 1 root root 40 Jun 12 16:41 cert.pem -\u0026gt; ../../archive/your-domain.com/cert1.pem lrwxrwxrwx 1 root root 41 Jun 12 16:41 chain.pem -\u0026gt; ../../archive/your-domain.com/chain1.pem lrwxrwxrwx 1 root root 45 Jun 12 16:41 fullchain.pem -\u0026gt; ../../archive/your-domain.com/fullchain1.pem lrwxrwxrwx 1 root root 43 Jun 12 16:41 privkey.pem -\u0026gt; ../../archive/your-domain.com/privkey1.pem 証明書更新の度に上記シンボリックリンクの向き先が新証明書に切り替わるので、 nginx側の設定は1度設定すれば変える必要がなくなります。 ■nginx設定 nginxの設定ファイルにて、SSL証明書と秘密鍵の設定をします。 (/etc/nginx/nginx.conf) (省略) server { listen 443 ssl http2 default_server; (省略) ssl_certificate \u0026#34;/etc/letsencrypt/live/your-domain.com/fullchain.pem\u0026#34;; ssl_certificate_key \u0026#34;/etc/letsencrypt/live/your-domain.com/privkey.pem\u0026#34;; (省略) } 設定後、nginxを再起動します。 systemctl restart nginx.service ■SSL証明書の自動再発行設定 SSL証明書の期限が残り1ヶ月を切ったら、証明書更新が可能となります。 更新は以下コマンドで実施します。 certbot renew 自動で更新できるように、cronに上記コマンドを登録しておきます。 例として、毎週月曜日の早朝に更新作業を行なうようにします。 （残り期限1ヶ月以上残っている場合は空振りして終わる） 10 0 * * 1 /bin/certbot renew \u0026amp;\u0026amp; /bin/systemctl reload nginx.service \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ※※※ 注意 ※※※ 証明書の更新だけしても、nginxにて再読込をしなければ反映されません。 ※2020年2月追記 certbot ver1.0.0現在ではrenew後に実行するコマンドを以下のようにオプションで指定できるようになりました。 /bin/certbot renew \u0026ndash;post-hook \u0026ldquo;systemctl restart nginx.service\u0026rdquo; これで設定は完了です。 ■SSL証明書の削除 不要になったドメインのSSL証明書の削除は以下のコマンドで実施します。 certbot revoke --cert-path=/etc/letsencrypt/archive/your-domain.com/cert1.pem これでSSL証明書を無効化し、自動更新の対象からも外れます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/642/",
    title: "MacにDocker環境を作る",
    date: "2017-05-08T11:01:23Z",
    body: "MacにDocker環境を作る ■環境 macOS Sierra 10.12.4 Dockerのインストール 公式ページ(こちら)の「Get Docker for Mac(Stable)」をクリックし、 Dockerのdmgをダウンロードする。 ダウンロードしたdmgを実行すると「Docker.app」を「Applications」へドラッグ＆ドロップするよう表示されるので、 その通りに実施する。 これで、LaunchpadにDockerが追加されましたので、クリックして起動します。 初回起動時に設定が走るので、Macのパスワードを聞かれます。 しばらく設定が走り、「Docker is now up and running!」が表示されれば設定完了です。 ターミナルを立ち上げ、以下コマンドを実行するとDockerのバージョンを返すようになっているはずです。 docker version 問題なければ「Got it」をクリックしてDockerのウィンドウを閉じます。 ウィンドウを閉じてもバックグラウンドで動いているので、 ステータスバーに鯨のアイコンが表示されています。 初期設定 デフォルトではMacにログオンしたら自動でDockerが起動するようになっていますが、 普段はDockerを起動しておく必要がない場合はステータスバーの鯨をクリックし、 Preferences \u0026gt; General \u0026gt; Start Docker when you log in のチェックボックスを外しておきます。 （検証時のみDockrが欲しい人など） その場合、Dockerを利用するタイミングでLaunchpadからDockerを起動してください。 その他、Dockerコンテナと共有するディレクトリなどもPreferencesで変更できるので、 必要に応じて設定を変更してください。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/cisco/",
    title: "Cisco",
    date: "2017-05-02T15:05:24Z",
    body: "Cisco"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/631/",
    title: "Cisco IOSスイッチ操作チートシート",
    date: "2017-05-02T15:05:24Z",
    body: "Cisco IOSスイッチ操作チートシート 管理者権限になる enable ※省略系はen 現在の設定を参照する # show interfaces ※全て見る場合 # show interfaces GigabitEthernet0/X ※特定のインターフェースのみ見る # show vlan ※「GigabitEthernet0/X」は「Gi0/X」と省略できる 設定モードに入る # configure terminal ※省略系はconf terminal (設定モードにて)Smart Installを無効化する (config)# no vstack (設定モードにて)管理者権限のパスワードを変更する (config)# enable password ｛変更後のパスワード｝ (設定モードにて)IF設定モードに入る (config)# interface GigabitEthernet0/X (IF設定モードにて)停止・起動 (config-if)# shutdown (config-if)# no shutdown (IF設定モードにて)vlanの切り替え (config-if)# switchport mode access (config-if)# switchport access vlan {vlan番号} 各モードを抜ける (なんらかのモード)# exit 各種設定した内容を保存する # write memory ※最後にこれを実行しておかないと、再起動後に設定内容が戻ってしまう。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/629/",
    title: "CentOS7のネットワーク周り初期設定",
    date: "2017-05-02T15:01:15Z",
    body: "CentOS7のネットワーク周り初期設定 ■環境 CentOS 7.3 bonding設定 参考URL：https://bacchi.me/linux/bonding/ 現状確認しておきます。 nmcli c ip a bonding用インターフェースを追加します。 nmcli connection add type bond autoconnect no con-name bond0 ifname bond0 mode active-backup 物理インターフェースをbondに紐付けます nmcli connection add type bond-slave autoconnect no ifname eno1 master bond0 nmcli connection add type bond-slave autoconnect no ifname eno2 master bond0 bondにIPを設定します。 nmcli c mod bond0 ipv4.method manual ipv4.address \u0026#34;192.168.999.999/32\u0026#34; ipv4.gateway \u0026#34;192.168.999.254\u0026#34; ipv6.method ignore nmcli c mod bond0 ipv4.dns 192.168.999.254 物理インターフェースを無効にし、bondインターフェースを有効にします。 nmcli c m eno1 connection.autoconnect no nmcli c m eno2 connection.autoconnect no nmcli c m bond-slave-eno1 connection.autoconnect yes nmcli c m bond-slave-eno2 connection.autoconnect yes nmcli c m bond0 connection.autoconnect yes nmcli c up bond-slave-eno1 nmcli c up bond-slave-eno2 nmcli c up bond0 ホスト名変更 hostnamectl set-hostname sample.com SELinux無効化 SELinuxが不要な場合は無効化します。 vim /etc/sysconfig/selinux SELINUX=enforcing SELINUX=disabled firewalld停止 別のファイアーウォール内のサーバである場合、 不要なのでfirewalldは停止しておきます。 systemctl stop firewalld systemctl disable firewalld"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/624/",
    title: "MacにZabbixエージェントを入れる",
    date: "2017-05-02T14:49:34Z",
    body: "MacにZabbixエージェントを入れる ■環境 Mac OS 10.9.5 Zabbix 3.2.1 Mac OSのサーバにZabbixエージェントを入れた際の手順メモです。 WEBでzabbixのソースをダウンロードします。 https://sourceforge.net/projects/zabbix/files/ZABBIX%20Latest%20Stable/3.2.1/ Macサーバに送ります。 $ scp zabbix-3.2.1.tar.gz user@192.168.999.999:~/Downloads/. ※Macサーバ上で直接ダウンロードできるなら、その方が早いです。 macサーバにて解凍〜インストールを行ないます。 tar xzf zabbix-3.2.1.tar.gz cd zabbix-3.2.1 ./configure --enable-agent make install sudo cp conf/zabbix_agentd.conf /usr/local/etc/. sudo cp src/zabbix_agent/zabbix_agentd /usr/local/sbin/ Zabbixエージェントの設定を変更します。 sudo vim /usr/local/etc/zabbix_agentd.conf # 127.0.0.1の部分をZabbixサーバのIPアドレスに変更 # Hostnameはagentサーバのホスト（サーバ側に設定したものと同じ）に変更 Zabbixエージェントを実行します。 /usr/local/sbin/zabbix_agentd 【補足】 インストール時にzabbixユーザが作成されるはずですが、 ユーザ作成に失敗している場合は別途ユーザ作成しておかないとエージェントの実行ができません。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/zabbix/",
    title: "Zabbix",
    date: "2017-05-02T14:49:34Z",
    body: "Zabbix"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/592/",
    title: "MastodonインスタンスをHerokuだけで立てる",
    date: "2017-04-28T20:05:40Z",
    body: "MastodonインスタンスをHerokuだけで立てる 最近Mastdonが流行っているらしいので お金をかけずにMastdonインスタンスをHerokuのみで立ててみます。 注意 本手順ではS３(ストレージ)を利用しないので、アイコンやヘッダーなどは保存されません。 自分１人用のインスタンスにするつもりで作ってます。 「Herokuのみ」に拘っており、他の無料ツールと組み合わせることもできますが、やってません。 ■参考にしたサイト 結城浩大先生のブログ しょっさんのQiita ■手順(インスタンス生成編) Herokuのアカウントを持っていなければアカウント作成します。 アカウント作成はコチラから。 mastodon公式のHerokuページにて「Deploy to Heroku」ボタンをクリックする。 Herokuのアプリ新規作成画面が開くので、以下を入力します。 AppName: 作成するアプリ名 LOCAL_DOMAIN: {上記のAppName}.herokuapp.com S3_ENABLED: false SINGLE_USER_MODE: true 上記入力が完了したら『Deploy』ボタンをクリックします。 私の場合はここでクレジットカード情報を入力する画面が出ましたが、既にHerokuを使っていて無料枠を使い切っていたから出たのかもしれません。 ど新規でHeroku登録していれば、何も出ないかもしれません。 Deployが完了したら、Heroku管理画面の「Resources」画面にて「worker」をONにします。 右側のエンピツマークをクリックしたらON/OFFを切り替えできるようになります。 ここまでで、マストドンのインスタンス生成は完了です。 ■手順（アカウント登録編） 普通ならアカウント登録をし、登録したメールアドレスに届いたメールのリンクをクリックすれば アカウント登録は完了なのですが、今回はメールサーバーの設定もしてないので、 無理やりDBを参照して進めます。 まず、Heroku管理画面の「Resources」画面にて、『Heroku Postgres :: Database』をクリックします。 するとPostgreSQLの管理画面へ遷移するので、『View Credentials』をクリックし、 「Heroku CLI」(heroku pg:psql〜)を確認しておきます。 ※後ほど利用するHeroku CLIのコマンドです。 ブラウザで『{AppName}.herokuapp.com』へアクセスするとアカウント作成画面が開くので 普通にアカウント登録します。 次に、Heroku CLIをインストールします。 インストール手順はコチラにまとまってます。 ローカルPCにPostgreSQLが入っていないなら、インストールしておきます。 Macなら以下コマンドでインストールできます。 brew install postgresql PostgreSQLが使える状態になったら、HerokuのDBへ接続して メールアドレス確認用のtokenを確認します。 # ローカルPCのターミナルにて、確認した「Heroku CLI」コマンドを実行 heroku pg:psql postgresql-fitted-xxxxxx --app {AppName} # DB操作モードに切り替わるので、DBよりtokenをselectする select email, confirmation_token from users; 上記コマンドでtokenが表示されるので、それを含めた以下のURLへブラウザでアクセスします。 https://{AppName}.herokuapp.com/auth/confirmation?confirmation_token={上記で確認したtoken} これでメールアドレスの確認も完了し、Mastdonを利用開始できます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/569/",
    title: "【Mac】fish + oh-my-fish + pecoの導入方法",
    date: "2017-04-20T00:31:51Z",
    body: "【Mac】fish + oh-my-fish + pecoの導入方法 環境 OS X ElCapitan 10.11.6 fishのインストール brewでfishをインストールする。 brew install fish ※brew自体のインストールについてはこちら参照。 次にシェル一覧にfishを追加する。 vim /etc/shells # 以下行を追加 /usr/local/bin/fish 次に通常利用するシェルをfishに設定する。 chsh -s /usr/local/bin/fish oh-my-fishのインストール 以下コマンドを実行するだけ。 curl -L http://get.oh-my.fish | fish pecoのインストール pecoはbrewとomfコマンドでインストールできる。 brew install peco omf install peco インストールできたら、fishの設定ファイルにpecoの設定を追記する。 fishの設定ファイルが存在しない場合は新規作成する。 vim ~/.config/fish/config.fish # 以下行を追記する function fish_user_key_bindings bind \\cr peco_select_history end 以上で設定完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/520/",
    title: "【Laravel】migrateで『Class 'CreateFoooTable' not found』が発生する",
    date: "2017-04-11T16:25:41Z",
    body: "【Laravel】migrateで『Class 'CreateFoooTable' not found』が発生する ■環境 Laravel 5.1 Laravelの「migrate:rollback」や「migrate:refresh」をしようとして 『Class \u0026lsquo;CreateFoooTable\u0026rsquo; not found』というエラーが発生した際の解決方法です。 この事象はmigrateで利用する「/database/migrations」ディレクトリに配置されているファイルが autoloadされていない事が原因のようです。 解決するにはプロジェクトのrootディレクトリにて以下コマンドを実行し、autoloadを再編成します。 composer dump-autoload ※composerインストールについてはコチラ もしサーバにcomposerを入れられない場合は作業PCにて上記コマンドを実行すると 「/vendor」ディレクトリ配下が更新されるのでサーバにアップロードしてください。 以上。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/composer/",
    title: "Composer",
    date: "2017-04-11T16:08:08Z",
    body: "Composer"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/515/",
    title: "Composerのインストール方法",
    date: "2017-04-11T16:08:08Z",
    body: "Composerのインストール方法 ■環境 macOS Sierra 10.12.4 CentOS 6.8 作業用MacとCentOSサーバにComposerをインストールしたので、その際のメモです。 【Macの場合】 brewでインストールできます。 brew install homebrew/php/composer ※※※※※※※※※※※※※※※※※※※※※※※※※※※※※ 2018年12月 追記 上記コマンドだとエラーが出るようになりました。 現在は以下のコマンドでインストールできるようです brew install composer ※※※※※※※※※※※※※※※※※※※※※※※※※※※※※ 【CentOSの場合】 CURLでインストーラを取得し、生成された実行ファイルをPATHが通っているディレクトリに移動します。 curl -sS https://getcomposer.org/installer | php mv composer.phar /usr/bin/composer 以上"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/512/",
    title: "【PHP】Laravel5でカスタムバリデーションを追加する",
    date: "2017-04-11T15:11:41Z",
    body: "【PHP】Laravel5でカスタムバリデーションを追加する ■環境 Laravel 5.1 Laravel5でカスタムバリデーションを追加した際の手順です。 まず、サービスプロバイダーを作成します。今回は『ValidatorServiceProvider』という名前で作る事にします。 以下コマンドを実行すると、『/app/providers/ValidatorServiceProvider.php』が生成されます。 php artisan make:provider ValidatorServiceProvider 作成したサービスプロバイダーを『/config/app.php』に追記します。 \u0026#34;providers\u0026#34; =\u0026gt; [ 〜〜　省略　〜〜 App\\Providers\\ValidatorServiceProvider::class, 〜〜　省略　〜〜 ], 作成したサービスプロバーダーにバリデーションルールを記載します。 例えば以下のルールの社員番号があったとします。 ・頭文字は「A」または「Z」 ・2文字目以降は数字６桁 このバリデーションルールを『shainbangou』という名前のルールで登録するとこうなります。 namespace App\\Providers; use Illuminate\\Support\\ServiceProvider; use Validator; class ValidatorServiceProvider extends ServiceProvider { public function boot() { Validator::extend(\u0026#39;shainbangou\u0026#39;, function($attr, $value, $param) { return preg_match(\u0026#39;/^[A|Z][0-9]{6}$/\u0026#39;, $value); }); } } 次にエラー文言の設定をします。 viewにて社員番号はempIdという変数名で設定されているとします。 【/resources/lang/ja/validation.php】 return [ 〜〜　省略　〜〜 \u0026#39;custom\u0026#39; =\u0026gt; [ \u0026#39;empId\u0026#39; =\u0026gt; [ \u0026#39;shainbangou\u0026#39; =\u0026gt; \u0026#39;社員番号が間違っています。（A/Z + 数字6桁）.\u0026#39;, ], ], 〜〜　省略　〜〜 \u0026#39;attributes\u0026#39; =\u0026gt; [ \u0026#39;empId\u0026#39; =\u0026gt; \u0026#39;社員番号\u0026#39;, ], ]; これで、コントローラなどのバリデーション処理で新ルール「shainbangou」が使えるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/501/",
    title: "gitの履歴から特定のコミット以降の履歴だけを残してクリアする",
    date: "2017-04-10T19:39:28Z",
    body: "gitの履歴から特定のコミット以降の履歴だけを残してクリアする 個人で開発しているアプリケーションのgitリポジトリで履歴がグチャグチャしてしまったので、 コミット履歴をちょっと弄った時のメモです。 ==================================== ※注意※ gitの履歴は過去に何があったかを調べる際の手がかりになるので、 削除する理由がないのであれば残しておきましょう。 特に、他人と共同で修正しているリポジトリであれば尚更です。 履歴を残し始める直前のバージョンの状態を作る # git clone \u0026lt;git URL\u0026gt; tmpRepo1 # cd tmpRepo1 # git checkout {残し始めたいコミットの直前のコミット番号} # rm -rf .git # cd ../ 作業用のリポジトリを作る # git clone \u0026lt;git URL\u0026gt; tmpRepo2 # cd tmpRepo2 # git checkout --orphan tmpBr 初期状態をコミットする # rm -rf \u0026lt;.git以外の全て\u0026gt; # cp -r ../tmpRepo1/* . # git add . # git commit -m \u0026#34;Initialization\u0026#34; 残したいコミット履歴に対して、次の処理を過去から順に繰り返し実施する # git cherry-pick {残したいコミット} なお、残したいコミット履歴がマージ履歴の場合は以下の手順となる # git cherry-pick -m 1 {残したいマージコミット} # git commit --allow-empty コメントの編集画面が出るが、そのまま:wqして抜ける 意図した履歴が生成されたか確認 # git log masterにプッシュする # git checkout -B master # git push --force 一時的に作ったブランチを削除 # git branch -d tmpBr"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/491/",
    title: "【golang】mapの要素順は変動する",
    date: "2017-04-08T12:22:59Z",
    body: "【golang】mapの要素順は変動する ■環境 OS X El Capitan 10.11.6 go 1.8 darwin/amd64 タイトルの件、ちょっと引っかかったのでメモです。 golangで変数にmapを格納する場合、その要素の順番は 呼び出すたびに変わります。 以下サンプルコードです。 mapをrangeで取り出しながらforで回し、標準出力する という処理を3回繰り返します。 package main import ( \u0026#34;fmt\u0026#34; ) func main() { // create new map fruitColor := make(map[string]string) fruitColor[\u0026#34;pineapple\u0026#34;] = \u0026#34;yellow\u0026#34; fruitColor[\u0026#34;apple\u0026#34;] = \u0026#34;red\u0026#34; fruitColor[\u0026#34;grape\u0026#34;] = \u0026#34;purple\u0026#34; fruitColor[\u0026#34;melon\u0026#34;] = \u0026#34;green\u0026#34; // printout 3 times for i := 1; i \u0026lt;= 3; i++ { fmt.Println(fmt.Sprintf(\u0026#34;===== %d times =====\u0026#34;, i)) for name, color := range fruitColor { fmt.Println(fmt.Sprintf(\u0026#34;%s is %s\u0026#34;, name, color)) } } } これを実行すると、このように出力されます。 要素順は変動するので、実行する度に変わると思います。 ===== 1 times ===== apple is red grape is purple melon is green pineapple is yellow ===== 2 times ===== grape is purple melon is green pineapple is yellow apple is red ===== 3 times ===== melon is green pineapple is yellow apple is red grape is purple 上記のような順不同なアプリケーションだとしても、 ユーザが画面を表示する度に順番が変わるのは気持ち悪いので sliceを使って表示順を保持しておいた方が良いです。 sliceを使う例は以下の通りです。 package main import ( \u0026#34;fmt\u0026#34; ) func main() { // create new map fruitColor := make(map[string]string) fruitColor[\u0026#34;pineapple\u0026#34;] = \u0026#34;yellow\u0026#34; fruitColor[\u0026#34;apple\u0026#34;] = \u0026#34;red\u0026#34; fruitColor[\u0026#34;grape\u0026#34;] = \u0026#34;purple\u0026#34; fruitColor[\u0026#34;melon\u0026#34;] = \u0026#34;green\u0026#34; // set display order fruitList := []string{\u0026#34;pineapple\u0026#34;,\u0026#34;apple\u0026#34;,\u0026#34;grape\u0026#34;,\u0026#34;melon\u0026#34;} // printout 3 times for i := 1; i \u0026lt;= 3; i++ { fmt.Println(fmt.Sprintf(\u0026#34;===== %d times =====\u0026#34;, i)) for _, name := range fruitList { fmt.Println(fmt.Sprintf(\u0026#34;%s is %s\u0026#34;, name, fruitColor[name])) } } } これを実行すると以下のようになります。 ===== 1 times ===== pineapple is yellow apple is red grape is purple melon is green ===== 2 times ===== pineapple is yellow apple is red grape is purple melon is green ===== 3 times ===== pineapple is yellow apple is red grape is purple melon is green"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/467/",
    title: "【法人】地方税のeLTAXでの申請手順メモ",
    date: "2017-03-10T00:04:51Z",
    body: "【法人】地方税のeLTAXでの申請手順メモ 法人（休眠中）の地方税に関する確定申告をeLTAXで行なったので、その手順をメモします。 休眠中じゃなくても、大体の手順は同じだと思います。 ※e-tax(国税）についてはこちら ■必要なもの ・Windows PC ・ICカードリーダー ・決算書など、確定申告書に添付する資料のPDF ■手順 ①eLTAX利用登録 こちらに沿って、eLTAX利用登録をします。 http://www.eltax.jp/www/contents/1398146092894/index.html 電子証明書は法人のものでなくても、代表個人のものでもよいです。 マイナンバーカードに電子証明書が付いているので、それを使うのが一番早いと思います。 なお、提出先選択画面にて選択する「地方公共団体」は市区町村ではなく都道府県です。 ②eLTAXソフトのダウンロード 上記①のサイトにてeLTAXソフト『PCdesk』のダウンロードができるので、 ダウンロード＆インストールしてください。 ③PCdeskへの暗証番号設定 PCdeskを起動したら、まず『1:利用者情報メニュー ＞ 利用者情報』にて 暗証番号の変更をしてください。 ④PCdeskでの申告書作成 『3:申告データ作成 ＞ 申告データ新規作成』にて申告書の作成を行ないます。 申告区分は『確定申告』で、作成する書類は『中間・確定申告書』のみとします。 申告書作成画面下部メニューの『様式/添付一覧』から添付資料は添付できるので、 e-taxで送付した決算書一式も添付しておく。 ④PCdeskでの申告書送信 『3:申告データ作成メニュー ＞ 申告データ署名』にて署名し、 『3:申告データ作成メニュー ＞ 申告データ送信』にて送信します。 以上でeLTAXでの申請は完了。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/444/",
    title: "Redmine2.3.2にグループ管理プラグインを入れる",
    date: "2017-02-27T15:16:05Z",
    body: "Redmine2.3.2にグループ管理プラグインを入れる redmineにグループ管理のプラグインを入れたが、ちゃんと動かなかったので つまづいた所をメモする。 ※補足 Ruby on Railsを全然知らないPHPerがちょちょっと修正しているので、 Railsに詳しい人から見たらおかしいかも。指摘してくれるとありがたいです。 ■環境 CentOS 6.5 Redmine 2.3.2 ruby 2.0.0 rails 3.2.13 https://github.com/fathomssen/redmine_group_manager 基本的なインストール方法はREADME.rdoc通り ①order by句がおかしい オーダー句がおかしいとエラーになったので、app/models/gmanager.rbの９行目を修正した 【修正前】 mem = Member.where(:project_id =\u0026gt; pid).order(user_id: :asc) 【修正後】 mem = Member.where(:project_id =\u0026gt; pid).order(\u0026#34;user_id asc\u0026#34;) ②オーナー未指定のグループがあると動かない まあ、インストール直後は全てのグループにオーナーが指定されていないのですが、 オーナーが未指定の場合はユーザテーブルのID=1のユーザがオーナーになるそうです。 （app/helpers/gmanagers_helper.rbの２８行目参照） 私の環境ではID=1のユーザが既に削除されていたので、 新しくID=１で絶対的管理者的存在のユーザを作成しました。 別IDで絶対的管理者がいるなら、上記のヘルパーを修正するのでも良いと思います。 （個人のIDにしてしまうと退職した際などに困るのでオススメしません） ③グループ編集タグがredmine管理者にしかでない 各プロジェクトページにてグループ編集タブが表示されるはずなのですが、 プラグインを入れた時点ではredmineの管理者でなければタブが表示されていませんでした。 プロジェクトの管理者にグループ編集機能を使わせたい場合は、redmine管理者メニューの 『管理　＞　ロールと権限』からグループ編集権限を予め付与しておかなければなりません。 説明をよく読めば書いてあるのかもしれませんが、英語辛いので斜め読みしてました。すみません。 ④グループメンバー一覧の項目順がおかしい 各プロジェクトのグループ編集タブよりグループメンバー一覧を表示すると、 カスタムフィールドの項目名と値の並び順が一致していませんでした。 なので、一致するようソート処理を入れます。 app/models/gmanager.rbの34行目と48行目に同じ修正を入れる。 【修正前】 keys = CustomField.where(:type =\u0026gt; \u0026#39;UserCustomField\u0026#39;) 【修正後】 keys = CustomField.where(:type =\u0026gt; \u0026#39;UserCustomField\u0026#39;).order(:id) app/models/gmanager.rbの49行目も修正する。 【修正前】 values = User.find(id).custom_values 【修正後】 values = User.find(id).custom_values.order(:custom_field_id) これで動くようになりました。 〜番外編〜 このプラグインはプロジェクトに所属している人をグループに追加することしかできません。 私はプロジェクトに所属していないユーザもグループ追加できるようにしたかったので、 プラグインを大幅に修正しました。 プロジェクトに未所属のメンバーもグループに追加できるようにしたプラグインは こちらです。 https://github.com/mildjester/redmine_group_manager"
  },
  {
    url: "https://blog2.logical-dice.com/tags/ruby/",
    title: "Ruby",
    date: "2017-02-27T15:16:05Z",
    body: "Ruby"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/440/",
    title: "CentOS7にfishを入れる",
    date: "2017-02-17T00:00:18Z",
    body: "CentOS7にfishを入れる 最近人気らしいfishを試しに入れてみました。 ■環境 CentOS 7.3 fish 2.3.1 epelリポジトリが入っていない人はインストールします。 yum -y install epel-release fishをyumでインストールします。 yum -y install fish 今だけfishを使う場合は、コマンドでそのままfishと打ちます。 fish 恒久的にfishを使いたい場合はchshしておきます。 chsh -s /usr/bin/fish　｛ユーザ名｝ これでfishが使えるようになりました。 〜参考〜 CentOS6.X系の場合は以下でfishのインストールできるようです。 wget -P /etc/yum.repos.d/ http://download.opensuse.org/repositories/shells:fish:release:2/CentOS_6/shells:fish:release:2.repo yum -y install fish 【oh-my-fishインストール】 ついでにoh-my-fishもインストールします。 CentOS7でyumインストールできるgitのバージョンは1.8系(2017年4月現在)なのですが、 oh-my-fishのインストールにはgit 1.9.5以上が必要なので ソースからコンパイルします。 ソースからコンパイルする際に必要なものをインストールします。 私の環境では以下でしたが、ここは各環境に合わせてインストールしてください。 ソースからコンパイルする方法を調べれば出てくると思います。 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-ExtUtils-MakeMaker autoconf gettext gcc ソースコードをダウンロードしてきます cd /tmp/ wget https://www.kernel.org/pub/software/scm/git/git-2.9.3.tar.gz tar xzf git-2.9.3.tar.gz ※最新のソースは以下で確認 https://www.kernel.org/pub/software/scm/git/ gitをインストールします。 configureのオプション「\u0026ndash;with-curl」を付けないと、oh-my-fishインストール時にコケます。 cd git-2.9.3 make configure ./configure --with-curl make make install 『/usr/local/bin』配下にgit系コマンドが配置されるので、 もしPATHを通していない場合は追加する。 set -U fish_user_paths $fish_user_paths /usr/local/bin git --version git version 2.9.3 最後にoh-my-fishをインストールして完了です。 curl -L https://get.oh-my.fish | fish oh-my-fishのインストール詳細についてはこちらを参照してください。 https://github.com/oh-my-fish/oh-my-fish 【pecoインストール】 ついでにpecoもインストールしておきます。 peco本体はgithubからダウンロードし、利用可能な状態にしておきます。 cd /tmp/ wget https://github.com/peco/peco/releases/download/v0.5.1/peco_linux_amd64.tar.gz tar xzf peco_linux_amd64.tar.gz mv peco_linux_amd64/peco /usr/local/bin/. oh-my-fishでpecoをfishに取り込みます。 omf install peco 最後にfishのコンフィグファイルを修正します。 vim ~/.config/fish/config.fish # 記載する内容 function fish_user_key_bindings bind \\cr peco_select_history end 以上で設定は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/424/",
    title: "zabbixでPHPの警告が出る場合の対処",
    date: "2017-02-14T00:48:55Z",
    body: "zabbixでPHPの警告が出る場合の対処 ■環境 zabbix 3.2 PHP 7.1 PHP7.1でzabbixを動かすと、以下の警告が出ました。 A non well formed numeric value encountered [zabbix.php:21 → require_once() → ZBase-\u0026gt;run() → ZBase-\u0026gt;processRequest() → CView-\u0026gt;getOutput() → include() → make_status_of_zbx() → CFrontendSetup-\u0026gt;checkRequirements() → CFrontendSetup-\u0026gt;checkPhpMemoryLimit() → str2mem() in include/func.inc.php:410] A non well formed numeric value encountered [zabbix.php:21 → require_once() → ZBase-\u0026gt;run() → ZBase-\u0026gt;processRequest() → CView-\u0026gt;getOutput() → include() → make_status_of_zbx() → CFrontendSetup-\u0026gt;checkRequirements() → CFrontendSetup-\u0026gt;checkPhpPostMaxSize() → str2mem() in include/func.inc.php:410] A non well formed numeric value encountered [zabbix.php:21 → require_once() → ZBase-\u0026gt;run() → ZBase-\u0026gt;processRequest() → CView-\u0026gt;getOutput() → include() → make_status_of_zbx() → CFrontendSetup-\u0026gt;checkRequirements() → CFrontendSetup-\u0026gt;checkPhpUploadMaxFilesize() → str2mem() in include/func.inc.php:410] 警告が出ている箇所は以下のようになっています。 [/usr/share/zabbix/include/func.inc.php] 394 /** 395 * Converts strings like 2M or 5k to bytes 396 * 397 * @param string $val 398 * 399 * @return int 400 */ 401 function str2mem($val) { 402 $val = trim($val); 403 $last = strtolower(substr($val, -1)); 404 405 switch ($last) { 406 case \u0026#39;g\u0026#39;: 407 $val *= 1024; 408 /* falls through */ 409 case \u0026#39;m\u0026#39;: 410 $val *= 1024; 411 /* falls through */ 412 case \u0026#39;k\u0026#39;: 413 $val *= 1024; 414 } 415 416 return $val; 417 } この引数の「$val」には『256M』のように単位付きの数字が入ってくるようで、 それを『$val *= 1024』と数字のように扱っているので警告が出ているようです。 （警告は出ているが、処理自体は正常に行なわれている） 解決策は色々あると思いますが、とりあえず以下のように修正したら 警告は表示されなくなりました。（404行目を追加しています） 401 function str2mem($val) { 402 $val = trim($val); 403 $last = strtolower(substr($val, -1)); 404 $val = intval($val); 405 switch ($last) { 406 case \u0026#39;g\u0026#39;: 407 $val *= 1024; 408 /* falls through */ 409 case \u0026#39;m\u0026#39;: 410 $val *= 1024; 411 /* falls through */ 412 case \u0026#39;k\u0026#39;: 413 $val *= 1024; 414 } 415 416 return $val; 417 }"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/349/",
    title: "CentOS 7.3にzabbix 3.2をインストール",
    date: "2017-02-14T00:20:50Z",
    body: "CentOS 7.3にzabbix 3.2をインストール ■環境 CentOS 7.3 zabbix 3.2 PHP 7.1 MySQL 14.14 Apache 2.4 ①zabbixをyumでインストールするためのリポジトリをインストール # yum install http://repo.zabbix.com/zabbix/3.2/rhel/7/x86_64/zabbix-release-3.2-1.el7.noarch.rpm ②zabbixの本体をインストール # yum install \\ zabbix-server-mysql \\ zabbix-web-mysql \\ zabbix-web \\ zabbix-agent \\ zabbix-get \\ zabbix-sender \\ zabbix-web-japanese ※MySQLではなくPostgreSQLを使う場合は、上記の『mysql』を『pgsql』にするだけです。 ③MySQLの設定 # mysql mysql\u0026gt; CREATE USER \u0026#39;zabbix\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;zabbixpassword\u0026#39;; mysql\u0026gt; CREATE DATABASE zabbix CHARACTER SET utf8; mysql\u0026gt; GRANT ALL ON zabbix.* to \u0026#39;zabbix\u0026#39;@\u0026#39;localhost\u0026#39;; mysql\u0026gt; quit # zcat /usr/share/doc/zabbix-server-mysql-3.2.1/create.sql.gz | mysql -uroot zabbix パスワードは任意のものを入れてください。 DB名とユーザ名も変えたければ変えても構いません。 ④Zabbixの設定 # vim /etc/zabbix/zabbix_server.conf 以下項目を設定 LogFileSize=10 DBPassword=zabbixpassword もし手順③にてDB名とユーザ名も変更していた場合は、ここで該当箇所も修正してください。 ⑤起動 # systemctl start zabbix-server # systemctl start zabbix-agent # systemctl restart httpd これでインストールは完了です。 https://ホスト名/zabbixにアクセスするとzabbixが開き、初期設定が始まります。 初期ログインのID/PASSは『Admin/zabbix』です。 ログイン後、右上の人形アイコンで言語やパスワードを変更できます。 私が実施した時はダッシュボードにて以下のPHPの警告が表示されました。 『A non well formed numeric value encountered』 解決策はコチラに記載しています。 その他 MacサーバへのZabbixエージェントインストールが難しかったので、別途記事にしました。 MacにZabbixエージェントを入れる"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/398/",
    title: "【git】gitコマンドチートシート",
    date: "2017-02-11T23:06:36Z",
    body: "【git】gitコマンドチートシート 自分が使うgitコマンドを整理しました。 ■環境 OS X El Capitan 10.11.6 git 2.10.1 【設定について】 グローバルコンフィグの設定 （全リポジトリの共通設定） $ git config --global user.name \u0026amp;quot;jester\u0026amp;quot; $ git config --global user.email \u0026amp;quot;jester@sample.co.jp\u0026amp;quot; リポジトリ別コンフィグの設定 (対象リポジトリのディレクトリにて) $ git config user.name \u0026amp;quot;master\u0026amp;quot; $ git config user.email \u0026amp;quot;master@sample.co.jp\u0026amp;quot; 【リポジトリの作成・リモート設定について】 リモートのリポジトリをクローンする $ git clone {リモートリポジトリの指定} {生成するディレクトリ} ※生成するディレクトリを省略すると、リモートリポジトリと同じ名前でディレクトリが生成される ローカルで新規にリポジトリを作成する $ mkdir SampleProject $ cd SampleProject $ git init ローカルで作成したリポジトリを後からリモートに紐付ける (対象のリポジトリディレクトリにて) $ git remote add {名前、originなど} {リモートリポジトリの指定} リモートリポジトリの変更する (対象のリポジトリディレクトリにて) $ git remote set-url {名前、originなど} {リモートリポジトリの指定} リモートとの紐付けを解除する (対象のリポジトリディレクトリにて) $ git remote rm {名前、originなど}"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/402/",
    title: "【Wordpress】SyntaxHighlighter Evolvedを高速化する",
    date: "2017-02-10T18:23:37Z",
    body: "【Wordpress】SyntaxHighlighter Evolvedを高速化する Wordperssのプラグイン『SyntaxHighlighter Evolved』を入れていたところ 画面表示がとても遅くなったので高速化しました。 〜〜〜2017/2/11 追記〜〜〜 プラグインを変えた方が早いかもしれないです 「WP Code Highlight.js」に変えてみたところ、表示は問題なく早いですし paddingなどもこちらの方が気に入ってます。 〜〜〜追記終わり〜〜〜 ■環境 CentOS 7.3 Wordpress 4.7.2 SyntaxHighlighter Evolved 3.2.1 ※前提条件として、Wordpressを置いてあるサーバを操作できる権限があることとします 『SyntaxHighlighter Evolved』は各種言語に対応しているのですが、 実際、自分は書かないなーという言語も入っているので、不要なものは プラグインで読み込まないようにします。 具体的には、プラグイン内のPHPファイルを修正し、不要な言語の設定を読み込んでいそうな行を コメントアウトします。 # cd /{wordpressが配置されているディレクトリ}/wp-content/plugins/syntaxhighlighter # vim syntaxhighlighter.php コメントアウト箇所は114行目あたりから始まる以下の部分です。 wp_register_script( \u0026#39;syntaxhighlighter-brush-○○○\u0026#39;,〜〜〜　$this-\u0026gt;agshver ); 上記の「○○○」部分に言語名が入っているので、不要な言語の行はコメントアウトします。 あと、あまり効果があるか分かりませんが、テーマも使わないものはコメントアウトしてしまいました。 該当箇所は149行目あたりから始まる以下の部分です。 wp_register_stype( \u0026#39;syntaxhighlighter-theme-○○○\u0026#39;,〜〜〜　$this-\u0026gt;agshver ); 上記の「○○○」部分にテーマ名が入ります。 上記でコメントアウトした言語、テーマについて 下の方の行で配列に格納されているので、そこもコメントアウトします。 言語についての配列は、162行目あたりにあります。 $this-\u0026gt;brushes = (array) apply_filters( \u0026#39;syntaxhighlighter_brushes\u0026#39;, array( \u0026#39;as3\u0026#39; =\u0026gt; \u0026#39;as3\u0026#39;, \u0026#39;actionscript3\u0026#39; =\u0026gt; \u0026#39;as3\u0026#39;, \u0026#39;bash\u0026#39; =\u0026gt; \u0026#39;bash\u0026#39;, \u0026#39;shell\u0026#39; =\u0026gt; \u0026#39;bash\u0026#39;, 以下略 テーマについての配列は、243行目あたりにあります。 $this-\u0026gt;themes = (array) apply_filters( \u0026#39;syntaxhighlighter_themes\u0026#39;, array( \u0026#39;default\u0026#39; =\u0026gt; __( \u0026#39;Default\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;django\u0026#39; =\u0026gt; __( \u0026#39;Django\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;eclipse\u0026#39; =\u0026gt; __( \u0026#39;Eclipse\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;emacs\u0026#39; =\u0026gt; __( \u0026#39;Emacs\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;fadetogrey\u0026#39; =\u0026gt; __( \u0026#39;Fade to Grey\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;midnight\u0026#39; =\u0026gt; __( \u0026#39;Midnight\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;rdark\u0026#39; =\u0026gt; __( \u0026#39;RDark\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), \u0026#39;none\u0026#39; =\u0026gt; __( \u0026#39;[None]\u0026#39;, \u0026#39;syntaxhighlighter\u0026#39; ), ) ); 上記箇所を適当にコメントアウトすると、画面表示がいくらか早くなりました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/384/",
    title: "PHPExcelの使い方メモ",
    date: "2017-02-07T11:42:44Z",
    body: "PHPExcelの使い方メモ PHPでExcelを生成できる『PHPExcel』の使い方メモ ※実際に私が使ったものをメモしているだけなので、機能を網羅している訳ではないです。かしこ。 インストールはcomposerを使うか、圧縮ファイルをダウンロードしてきて 適当なディレクトリに解凍します。 あとは以下をrequireするだけ。 /格納したパス/PHPExcel/Classes/PHPExcel.php /格納したパス/PHPExcel/Classes/PHPExcel/IOFactory.php 実際のソースコードは以下の通り /** * PHPExcelの新規オブジェクト作成 */ $excel = new PHPExcel(); /** * シートを作成してシート名を変える */ $sheetIndex = 0; // シート番号は０から $excel-\u0026gt;setActiveSheetIndex($sheetIndex); $sheet = $excel-\u0026gt;getActiveSheet(); $sheet-\u0026gt;setTitle(\u0026#39;SampleSheet01\u0026#39;); /** * セルにデータを入力する */ // 基本的な入力 $rowCnt = 1; // 行番号は１から $clmCnt = 0; // 列番号は0から $text = \u0026#39;セルに書き込む内容だよ\u0026#39;; $sheet-\u0026gt;setCellValueByColumnAndRow($clmCnt, $rowCnt, $val); // 文字列型で入力 $rowCnt++; // 1行下に移動 $clmCnt++; // 1列右に移動 $text = \u0026#39;000001\u0026#39;; // 文字列型にしないと、左記は「00001」ではなく「1」になってしまう $sheet-\u0026gt;setCellValueExplicitByColumnAndRow($clmCnt, $rowCnt, $val, PHPExcel_Cell_DataType::TYPE_STRING); /** * セルの書式設定 */ // シートのデフォルトフォントを設定する $sheet-\u0026gt;getDefaultStyle()-\u0026gt;getFont()-\u0026gt;setName(\u0026#39;ＭＳ Ｐゴシック\u0026#39;)-\u0026gt;setSize(11); // 背景色を設定 $bgcolor = \u0026#39;000000\u0026#39;; // 16進数で指定 $sheet-\u0026gt;getStyleByColumnAndRow($clmCnt, $rowCnt)-\u0026gt;getFill()-\u0026gt;setFillType(PHPExcel_Style_Fill::FILL_SOLID); $sheet-\u0026gt;getStyleByColumnAndRow($clmCnt, $rowCnt)-\u0026gt;getFill()-\u0026gt;getStartColor()-\u0026gt;setRGB($bgcolor); // 文字色を設定 $fontcolor = \u0026#39;FFFFFF\u0026#39;; // 16進数で指定 $sheet-\u0026gt;getStyleByColumnAndRow($clmCnt, $rowCnt)-\u0026gt;getFont()-\u0026gt;getColor()-\u0026gt;setARGB($fontcolor); // 太字にする $sheet-\u0026gt;getStyleByColumnAndRow($clmCnt, $rowCnt)-\u0026gt;getFont()-\u0026gt;setBold(true); // セル（行列番号指定）の上だけに罫線(二重線)を引く $sheet-\u0026gt;getStyleByColumnAndRow($clmCnt, $rowCnt)-\u0026gt;getBorders()-\u0026gt;getTop()-\u0026gt;setBorderStyle(PHPExcel_Style_Border::BORDER_DOUBLE); // セル（レンジ指定）の周り全てに罫線(単一線)を引く $sheet-\u0026gt;getStyle(\u0026#39;A1:B3\u0026#39;)-\u0026gt;getBorders()-\u0026gt;getAllBorders()-\u0026gt;setBorderStyle(PHPExcel_Style_Border::BORDER_THIN); // 列の幅を指定する $sheet-\u0026gt;getColumnDimension(\u0026#39;A\u0026#39;)-\u0026gt;setWidth(14.5); // 列の幅を自動調整する(※これで効くはずなんですが、効かない事が多々ある) $sheet-\u0026gt;getColumnDimension( PHPExcel_Cell::stringFromColumnIndex($clmCnt))-\u0026gt;setAutoSize(true); 以上"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/366/",
    title: "MFクラウド会計を使った休眠法人決算",
    date: "2017-02-07T00:38:00Z",
    body: "MFクラウド会計を使った休眠法人決算 休眠法人の決算を税理士を雇わずに終わらせる。 MFクラウド会計の無料プランを使う。 ①各種設定＞事業所　で必要な情報を設定する。 ②各種設定＞開始残高 で、昨年から繰り越した現金や預金の金額を設定する。 ③銀行の利子が入っていると思うので、その分だけ振替伝票を記載する。 ④以下をPDF出力する。 「決算・申告メニュー＞決算書」 「会計帳簿メニュー＞現預金出納帳」 「会計帳簿メニュー＞総勘定元帳」 ⑤e-taxにて確定申告する。 手順詳細はコチラ これで、税務署から何も言われなければ完了。 呼び出しを食らったら、大人しく従います。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/372/",
    title: "【法人】国税のe-taxでの申請手順メモ",
    date: "2017-02-07T00:25:56Z",
    body: "【法人】国税のe-taxでの申請手順メモ 法人（休眠中）の国税に関する確定申告をe-taxで行なったので、その手順をメモします。 休眠中じゃなくても、大体の手順は同じだと思います。 ※eLTAX(地方税）についてはこちら ■必要なもの Windows PC ICカードリーダー 決算書など、確定申告書に添付する資料のPDF ■手順 ①e-tax利用準備 電子証明書は法人のものでなくても、代表個人のものでもよいです。 マイナンバーカードに電子証明書が付いているので、それを使うのが一番早いと思います。 作業手順は以下に従ってください。 http://www.e-tax.nta.go.jp/hojin.html ②e-taxソフトでの申請データ作成 e-taxソフトやソフト起動時に入力する利用者識別番号も①の手順で取得できます。 ※注意 利用者識別番号は使い続けていれば過去に発行した物もずっと有効（？）らしいですが、 年に１回使う程度だと失効している可能性があります。(私は失効していました) 利用者識別番号が無効と言われた場合は、手順①を再度実施してください。 左メニューの『作成＞申告・申請等＞新規作成』にて申告書を作成します。 【１つめ：確定申告書申告書】 以下を選択して「次へ」をクリックします。 手続の種類：申告 税目：法人税・地方法人税 帳票の年：自分の事業にあった項目 もし存在しない場合はe-tax起動時のメニューにて追加インストールしてください。 また、インストール済みでも個人用の利用者識別番号を入力してしまうと法人用の申告書は表示されないようですのでご注意ください。 以下を作成します。 【普通法人の確定申告（青色）】 ・別紙1(1)　各事業年度の所得に係る申告書 ・別紙2　同族会社等の判定に関する明細書 ・別紙4　所得の金額の計算に関する明細書 ・別紙7(1)　欠損金又は災害損失金の損金参入に関する明細書 ・法人事業概要説明書 ※注意 作成する帳票を選ぶ画面がチェックボックスになっていますが複数選べません。 まずは別紙1を選んで作成し、後から他の書類を追加してください。 【2つめ：確定申告の添付書類】 以下を選択して「次へ」をクリックします。 手続の種類：申告 税目：法人税・地方法人税 帳票の年：自分の事業にあった項目 以下を作成します。 【イメージ添付書類（法人税申告）】 ・添付書類送付書 -\u0026gt;決算書、出納帳、総勘定元帳のPDFを添付する ③e-taxソフトでの署名 左メニュー『署名可能一覧へ＞電子署名』にて、上記で作成したデータに署名をします。 署名に使う電子証明書は①で取得したものです。 ④e-taxソフトでの送信 左メニュー『送信可能一覧へ＞送信』にて、まずは手順②で作成した２つの書類を紐付けます。 下側の単独送信不可の書類（作成した添付書類）を選択した状態で「紐付け」をクリックします。 すると紐付け画面が表示されるので、確定申告書を選択して「OK」をクリックします。 これで紐付けが完了したので、上側の送信可能書類（作成した確定申告書）を選択した状態で「送信」をクリックします。 その後、利用者識別番号の暗証番号を入力すれば送信完了です。 以上でe-taxでの申請は完了。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/355/",
    title: "【CentOS + Apache】Let’s Encryptで無料SSL証明書の取得＆自動更新をする",
    date: "2017-01-18T00:51:55Z",
    body: "【CentOS + Apache】Let’s Encryptで無料SSL証明書の取得＆自動更新をする ■環境 CentOS 7.3 Apache 2.4.6 Let\u0026rsquo;s Encryptで無料SSL証明書を取得し、自動更新する設定をします。 ########## 2017/04/05追記 ########## Certbotというツールが公開されており、 本手順を実施するよりも、そちらを利用した方が簡単です。 https://certbot.eff.org 設定方法はこちら 【Let\u0026rsquo;sEncrypt】Certbotの使い方(CentOS7 + nginx) ################################ ■前提 対象サーバがインターネット網に公開しているサーバであること。 →証明書発行時に認証のためLet\u0026rsquo;s Encrypt側からアクセスがあるようです。 そのため、ファイアーウォールなどで外部からのアクセスを遮断している環境では証明書発行ができません。 2017/1/19現在、アクセス時のIPアドレスも公開しておりません。 https://letsencrypt.jp/faq/#IP 設定手順 ※以下手順はroot権限で実施しています。 まず、サーバー上の任意のディレクトリにLet\u0026rsquo;s Encryptから証明書を取得するツールをダウンロードします。 サーバにgitが入っていない場合は、ローカルでダウンロードしたものをFTPなどでアップしても良いと思います。 # git clone https://github.com/letsencrypt/letsencrypt ダウンロードしたら、ツールの初期構築をします。 以下コマンドにて必要なソフトウェアのインストールなどが走り、完了したらletsencrypt-autoのヘルプが表示されます。 # cd letsencrypt # ./letsencrypt-auto --help --debug 次にツールを実行し、実際に証明書を取得します。 その際、４４３ポートを利用しているプロセスがあるとエラーとなるので、この瞬間はApacheを止める必要がありました。 # systemctl stop httpd # ./letsencrypt-auto certonly --standalone -d your-domain.com # systemctl start httpd ※上記の「your-domain.com」には証明書を生成するドメインを入力してください。 これで証明書が生成できました。 以下のディレクトリ配下に証明書や秘密鍵（のシンボリックリンク）が生成されているので、 Apacheの設定ファイルへ記載します。 ■生成されるディレクトリ /etc/letsencrypt/live/your-domain.com/ ■生成されるファイル cert.pem　：　SSL証明書本体 chain.pem　：　チェイン証明書 fullchain.pem　：　SSL証明書本体とチェイン証明書を結合したもの privkey.pem　：　秘密鍵 これらをApacheの設定ファイル（一般的に/etc/httpd/conf.d/ssl.conf）に設定する。 ・設定パターン１ SSLCertificateFile /etc/letsencrypt/live/your-domain.com/cert.pem SSLCertificateKeyFile /etc/letsencrypt/live/your-domain.com/privkey.pem SSLCertificateChainFile /etc/letsencrypt/live/your-domain.com/chain.pem ・設定パターン2 SSLCertificateFile /etc/letsencrypt/live/your-domain.com/cert.pem SSLCertificateKeyFile /etc/letsencrypt/live/your-domain.com/privkey.pem SSLCertificateChainFile /etc/letsencrypt/live/your-domain.com/chain.pem ※Apacheのバージョンが2.4.8以上の場合は「SSLCertificateChainFile」の設定はできないので、 「SSLCertificateKeyFile」にfullchain.pemを設定すること。 ここで生成した証明書は3ヶ月間しか有効期限がないので、定期的に更新が必要です。 証明書を更新するコマンドは以下です。 ./letsencrypt-auto renew ただし、更新は有効期限が残り30日を切らないと実行されないらしく、 上記コマンドを実行しても何も更新せずに終わります。 「Apache停止→証明書更新→Apache起動」を実施するシェルを作ります。 70日以内に生成した証明書が存在する場合は何もしません。 #!/bin/bash serchResult=`find /etc/letsencrypt/archive/your-domain.com/ -mtime -70 -regex \u0026#34;.*cert[0-9]?\\.pem\u0026#34;` if [ -n \u0026#34;$serchResult\u0026#34; ]; then echo \u0026#34;certs are not due for renewal yet\u0026#34; else systemctl stop httpd \u0026amp;\u0026amp; /opt/git/letsencrypt/letsencrypt-auto renew \u0026amp;\u0026amp; systemctl start httpd if [ $? != 0 ]; then # just to be sure systemctl restart httpd fi fi ※上記はツールをopt配下に配置した場合です。 このシェルをcronで毎日深夜に実行するよう設定します。 # SSL update 5 0 * * * sh /etc/letsencrypt/renew.sh ※上記はシェルを/etc/letsencrypt/renew.shに作った場合の例です。 これで設定完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/318/",
    title: "HP Proliant Microserver Gen 8 にCentOS7を入れる",
    date: "2016-12-01T13:29:48Z",
    body: "HP Proliant Microserver Gen 8 にCentOS7を入れる ■作業環境 ・OS X El Capitan 10.11.6 ■手順 CentOS7の準備 まず、CentOSのISOイメージをダウンロードします。 ここから『Everything ISO』をダウンロードしておきます。 https://www.centos.org/download/ ダウンロードしたISOをUSBメモリに起動可能USBとして焼きます。 1.USBをMacに挿す。 2.以下コマンドにてUSBメモリのdisk番号を確認する。 $ diskutil list 3.以下のコマンドにてUSBメモリを初期化する。 $ diskutil eraseDisk MS-DOS NOTITLE /dev/diskX ※「NOTITLE」作成するUSB名(任意) ※diskXは1.で確認したdisk番号 4.焼き込みをするため、USBメモリをアンマウント $ diskutil unmountDisk /dev/diskX 5.ISOを書き込む。 $ cd {CentOSのISOがある場所} $ sudo dd if=CentOS-7-x86_64-Everything-1511.iso of=/dev/diskX bs=4028 ※処理完了まで、かなり待ちます 6.USBメモリを取り外し可能状態にする。 $ diskutil eject /dev/diskX RAIDコントローラの準備 次にGen8用のRAIDコントローラをダウンロードします。 これがないとHPサーバが備えているのRAID機能が使えない。 http://downloads.linux.hpe.com/SDR/repo/spp/RedHat/7/x86_64/current/ ここから「hpvsa-1.2.14-113.rhel7u2.x86_64.dd.gz」をダウンロードします。 ※『rhel7u2』の部分はインストールするCentOSに合わせて読み替えてください。 ダウンロードしたら、CentOSを焼いたUSBメモリとは別のUSBメモリに焼きます。 MacはUSBメモリを挿すと自動でマウントするので、ディスクユーティリティからマウント解除して 以下コマンドを実施。 （先にUSBの初期化〜アンマウントはISO書き込みと同じ手順を実施） $ cd {RAIDドライバをダウンロードした場所} $ gunzip hpvsa-1.2.14-113.rhel7u2.x86_64.dd.gz $ sudo dd if=hpvsa-1.2.14-113.rhel7u2.x86_64.dd of=/dev/diskX $ diskutil eject /dev/diskX インストール実施 作成したUSBメモリ２本をサーバに刺し、サーバを起動します。 （事前にUSBブートできるようにBIOS設定はしておくこと） しばらくするとCentOSの文字が表示され、インストール方法を選択画面が表示されます。 ここでEscを押下すると「boot:」というプロンプトが現れるので、 以下を入力する。 linux modprobe.blacklist=ahci inst.dd インストールが進み途中でドライバーが入ったディスクの選択画面が表示されます。 まずrと入力してディスク一覧の再読み込みを行ない、該当するディスクの番号を入力します。 次にドライバー一覧が表示され、インストールするドライバーを選択しろと言われるので 該当するドライバの番号を入力します。 その後、cを入力して続行します。 最後にディスク一覧が表示されるので、RAIDコントローラが入っていたUSBメモリを抜き rを入力する。 するとRAIDコントローラの入ったUSBメモリが消え、ISOが入ったUSBが一番上に表示されるはずなので そのままcを入力して続行します。 あとは通常のインストール通りです。 表示される質問事項を設定していけば完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/311/",
    title: "【DNSサーバ】BINDからNSDへの移行",
    date: "2016-10-25T11:21:04Z",
    body: "【DNSサーバ】BINDからNSDへの移行 社内利用のDNSをBIND作っていたのですが、 BINDは脆弱性が頻繁に発見されるし、DNSキャッシュ機能は要らなかったので NSDに切り替えました。 ■環境 CentOS 6.5 NSD 4.1.13 (参考：切替え前)BIND 9.8.2 まず、yumにepelリポジトリが設定されていない場合はインストールします。 # yum install epel-release.noarch NSDをインストールします。 # yum install nsd --enablerepo=epel NSD設定ファイルの最下行にzoneファイルを設定します。 # vim /etc/nsd/nsd.conf 〜最下行に以下を追記〜 zone: name: sample.co.jp zonefile: sample.co.jp.zone BINDで利用していたzoneファイルをコピーします # cp {BINDのzoneファイル} /etc/nsd/sample.co.jp.zone ※補足 調べているとNSDはrebuildコマンドにてzoneファイルからdbファイルを 生成しなければいけないという記事がありましたが、　私の環境では不要でした。 （むしろ、rebuildコマンドがありませんでした） BINDを停止します。 # service named stop NSDを起動します。 # service nsd start"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/302/",
    title: "Apacheのセキュリティ設定",
    date: "2016-10-12T13:40:02Z",
    body: "Apacheのセキュリティ設定 Apacheはデフォルト設定のまま利用するとサーバーの情報が諸々見えてしまいます。 それらを隠すには設定ファイルを編集する必要があります。 修正ファイル：/etc/httpd/conf/httpd.conf ①ファイル一覧を表示しないようにする 公開ディレクトリにindex.html等の初期表示ファイルが存在しない場合 そのディレクトリのURLを指定するとファイル一覧が表示されてしまいます。 それを防ぐためには以下のIndexesにハイフンを付けます。 \u0026lt;Directory \u0026#34;/var/www/html\u0026#34;\u0026gt; 〜省略〜 # Options Indexes FollowSymLinks Options -Indexes FollowSymLinks 〜省略〜 \u0026lt;/Directory\u0026gt; ①エラー画面にてバージョン番号だけ隠す場合 ServerTokensを「OS」から「Prod」に変更するだけです # ServerTokens OS ServerTokens Prod ②エラー画面にて『Apache』という表示を隠す場合 ServerSignatureを「On」から「Off」に変更するだけです # ServerSignature On ServerSignature Off"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/295/",
    title: "brewインストールでphp-mcryptがインストールできない",
    date: "2016-09-20T00:15:18Z",
    body: "brewインストールでphp-mcryptがインストールできない ■環境 OS X El Capitan(10.11.6) Homebrew 0.9.9 brewインストールでphp-mcryptをインストールした際にエラーが出たので エラー内容と、その対応をメモしておきます。 まず、エラー内容について # brew install homebrew/php/php70-mcrypt 〜中略〜 /usr/local/opt/php70/bin/phpize: line 61: /usr/local/Library/Homebrew/shims/super/sed: No such file or directory /usr/local/opt/php70/bin/phpize: line 62: /usr/local/Library/Homebrew/shims/super/sed: No such file or directory /usr/local/opt/php70/bin/phpize: line 63: /usr/local/Library/Homebrew/shims/super/sed: No such file or directory Configuring for: PHP Api Version: Zend Module Api No: Zend Extension Api No: /usr/local/opt/php70/bin/phpize: line 155: /usr/local/Library/Homebrew/shims/super/sed: No such file or directory autoheader: error: AC_CONFIG_HEADERS not found in configure.in 〜中略〜 つまり、phpizeで利用している『/usr/local/Library/Homebrew/shims/super/sed』が無いそうです。 色々調べた結果、私の環境ではそもそもHomebrewのパスが 『/usr/local/Library/Homebrew』 ではなく 『/usr/local/Homebrew/Library/Homebrew』 となっていました。（なんだこの階層） 対応としては、phpize内にて$SEDという変数に上記パスを指定していたので、 自分の環境に合わせてパスを書き換えました。 SED=\u0026#34;/usr/local/Homebrew/Library/Homebrew/shims/super/sed\u0026#34;"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/278/",
    title: "SVNサーバ(svn+ssh)のリポジトリをgitサーバに移行する",
    date: "2016-08-10T12:04:21Z",
    body: "SVNサーバ(svn+ssh)のリポジトリをgitサーバに移行する ■前提条件 gitはサーバ＆クライアントのどちらも既に使える環境であること ①gitサーバで空のリポジトリを作成する。 作成方法は各Gitサーバでの手法に従えば良いと思います。 （コマンドだったり、gitlabで作成したり） ②ローカルにSVNリポジトリを読み込んだgitリポジトリを作成する。 $ git svn clone --prefix svn/ svn+ssh://svn.sample.com/home/svn/svnhoge githoge 以下、読み替えてください。 【svn.sample.com】　SVNサーバのホスト名(IPアドレス) 【/home/svn/svnhoge】　移行対象のリポジトリのパス ※上記２つはSVNを使っていた頃に指定していたものと同じです 【githoge】　作成するgitリポジトリのディレクトリ名 ③ローカルにリポジトリができるので、gitサーバと紐付けする $ cd githoge $ git remote add origin gituser@git.sample.com:git/githoge.git 以下、読み替えてください。 【gituser】　gitを利用する際のアカウント 【git.sample.com】　gitサーバのホスト名(IPアドレス) 【git/githoge.git】　①で作成したgitリポジトリ 以上で完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/274/",
    title: "Linuxサーバに秘密鍵方式でsshする",
    date: "2016-08-01T00:27:04Z",
    body: "Linuxサーバに秘密鍵方式でsshする ■環境 CentOS 7.2 ■手順 ①秘密鍵と公開鍵を生成 ローカルPCにて以下を実施します。 MacでもLinuxでも同じコマンドで生成できるはずです。 $ ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/Users/hoge/.ssh/id_rsa): =\u0026gt;生成するパスとファイル名を指定（空欄のままだと括弧内のファイル名で生成される) Enter passphrase (empty for no passphrase): =\u0026gt;生成する鍵のパスワード。不要なら空欄のままエンター Enter same passphrase again: =\u0026gt;鍵のパスワードの再入力 生成した秘密鍵と同ディレクトリに公開鍵(拡張子pub)も生成されているので、 catコマンドで中身を見ておく。(後ほど使うのでコピーしておくと良い) ②公開鍵をサーバに格納 ログイン先サーバにて以下を実施 $ mkdir ~/.ssh $ vim ~/.ssh/authorized_keys この「authorized_keys」に①で生成した公開鍵の中身を貼り付ける。 以上で秘密鍵認証でログインできるようになっている。 もし接続できなかった場合は以下設定も確認してみる。 ③【繋がらない場合のみ】sshd設定確認 ログイン先のサーバにて以下を実施 $ vim /etc/ssh/sshd_config 以下の設定となっているか確認 RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys ※上から２つはコメントアウトされていても良い"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/259/",
    title: "【Mac】ターミナル設定（iTerm2 + zsh + prezto + peco）",
    date: "2016-07-26T00:00:40Z",
    body: "【Mac】ターミナル設定（iTerm2 + zsh + prezto + peco） Ubuntuの場合はこちら ①iTerm2を準備する iTerm2についてはこちらの記事で記載していますので こちらをご参照ください。 【Mac】ターミナル設定（iTerm2 ＋ fish ＋ fisherman） ②zsh + preztoの導入 preztoのインストールは基本的に公式通りです。 https://github.com/sorin-ionescu/prezto まずシェルをzshに切り替えます。Macならzshは元々インストールされているはずです。 zsh 次にgithubからソースをダウンロードします。 git clone --recursive https://github.com/sorin-ionescu/prezto.git \u0026#34;${ZDOTDIR:-$HOME}/.zprezto\u0026#34; 次にpreztoを使うようにzshの設定をします。 ターミナルに以下を貼り付けて実行します。（スクリプトファイルなどを作成する必要はなく、ターミナル直貼り付けでOK） setopt EXTENDED_GLOB for rcfile in \u0026#34;${ZDOTDIR:-$HOME}\u0026#34;/.zprezto/runcoms/^README.md(.N); do ln -s \u0026#34;$rcfile\u0026#34; \u0026#34;${ZDOTDIR:-$HOME}/.${rcfile:t}\u0026#34; done 最後に、通常使うシェルをzshにします。 chsh -s /bin/zsh これでターミナルを再起動すれば、zsh + preztoになっています。 ③pecoでコマンド履歴を検索しやすくする まず、pecoをインストールします。 brew install peco あとは.zshrcに以下を記載するだけです。 function peco-history-selection() { BUFFER=`history -n 1 | tail -r | awk \u0026#39;!a[$0]++\u0026#39; | peco` CURSOR=$#BUFFER zle reset-prompt } zle -N peco-history-selection bindkey \u0026#39;^R\u0026#39; peco-history-selection ターミナルを再起動するか、以下コマンドでzshrcを再読み込みすれば適用されます。 Ctrl＋Rで履歴検索画面になります。 source .zshrc"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/249/",
    title: "【Apache】htaccessのRewriteでGETパラメータも残す",
    date: "2016-05-26T19:56:46Z",
    body: "【Apache】htaccessのRewriteでGETパラメータも残す nginxが台頭してきている今日この頃。Apacheの話です。 少しハマったのでメモします。 ■環境 CentOS 6.6 httpd 2.2.15 htaccessにてリダイレクト処理を記載する際、 以下のように書くとGETパラメータがリダイレクト時に落ちてしまいます。 RewriteEngine on} RewriteBase / RewriteRule ^api.php(.*)$ api2.php$1 例えば、このまま「https://sample.com/api.php?name=tanaka」にアクセスすると リダイレクト後は「https://sample.com/api2.php」となってしまいます。 RewriteでGETパラメータを扱いたい場合は、『RewriteCond』を使って以下のように記載します。 RewriteEngine on RewriteBase / RewriteCond %{QUERY_STRING} (.*)$ RewriteRule ^api.php$ api2.php?%1 これで「https://sample.com/api.php?name=tanaka」にアクセスすると リダイレクト後は「https://sample.com/api2.php?name=tanaka」となります。 ※RewriteRuleの右側「$1」ではなく「%1」です。間違えないように。 何が起こっているかというと、RewriteCondの『QUERY_STRING』で指定した正規表現に一致する GETパラメータが%変数に格納されているようです。 例えば以下のような記載もできます。 RewriteEngine on RewriteBase / RewriteCond %{QUERY_STRING} name=(.*)$ RewriteRule ^api.php$ api2.php?onamae=%1 これで「https://sample.com/api.php?name=tanaka」にアクセスすると リダイレクト後は「https://sample.com/api2.php?onamae=tanaka」となります。 正規表現で複数パラメータを取ることも可能なので、 RewriteCond %{QUERY_STRING} param1=([a-z]+)\u0026amp;amp;param2=([a-z]+)\u0026amp;amp;param3=([a-z]+)$ と記載すれば、param1は「%1」、param2は「%2」、param3は「%3」で取得できます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/223/",
    title: "【golang】NetBeansでgo言語を使えるようにする",
    date: "2016-05-25T23:06:28Z",
    body: "【golang】NetBeansでgo言語を使えるようにする 元々NetBeansでPHPをやっているので 同じIDEでgoも使うために設定してみました。 ■環境 Ubuntu 16.04 NetBeans 8.1 ①「http://plugins.netbeans.org/plugin/25606/go」からnbmファイルをダウンロードする。 ②NetBeansのメニューより「Tools-\u0026gt;plugin」を選択 ③Downloadedタグにて「Add Plugins」をクリックして、ダウンロードしたnbmファイルを選択する。 ④「Install」ボタンをクリックする。 ⑤Installを進めていき、最後にNetBeansの再起動をして完了。 以上でgoが使えるようになります。 新規プロジェクトでHTML5プロジェクトなどを作成し、プロジェクト内でファイルの新規作成を選択すると ファイルのカテゴリにGoが追加されています。 ※2017/1/31 追記 GOプロジェクトを作成できるようになるプラグインも見つけました http://plugins.netbeans.org/plugin/62162/go-project"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/242/",
    title: "【Ubuntu】シャットダウンが遅い",
    date: "2016-05-22T00:35:22Z",
    body: "【Ubuntu】シャットダウンが遅い ■環境 Ubuntu 16.04 Ubuntuを16.04にしたら、シャットダウンに時間がかかるようになったので対応します。 参考： http://sicklylife.at-ninja.jp/memo/ubuntu1604/settings.html システム設定ファイルを修正します。 # vim /etc/systemd/system.conf 〜以下項目のコメントアウトを外して修正〜 DefaultTimeoutStopSec=10s"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/239/",
    title: "【Ubuntu】デュアルブート環境で時計が狂うのを防ぐ",
    date: "2016-05-22T00:30:55Z",
    body: "【Ubuntu】デュアルブート環境で時計が狂うのを防ぐ ■環境 Ubuntu 16.04 Windows10 Ubuntuを16.04にしたら、デュアルブートのWindowsとUbuntuを切り替えた際に 時刻がズレるようになったので対応します。 参考：http://sicklylife.at-ninja.jp/memo/ubuntu1604/settings.html ターミナルで以下をコマンドを実行するのみです。 sudo timedatectl set-local-rtc true"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/225/",
    title: "【Ubuntu】zsh\u0026preztoのインストール",
    date: "2016-05-06T10:29:33Z",
    body: "【Ubuntu】zsh\u0026preztoのインストール Ubuntuのターミナル（端末）でzsh＆preztoを使う設定です。 Macの場合はこちら ■環境 Ubunt16.04 ①zshとgitをインストールしておきます 16.04から「apt−get」ではなく「apt」になったようです。 $ sudo apt install zsh $ sudo apt install git ②preztoを導入します。 基本的な流れは「https://github.com/sorin-ionescu/prezto」の通りです。 $ zsh ※設定画面が開いたら「q」を入力して抜ける。 $ git clone --recursive https://github.com/sorin-ionescu/prezto.git \u0026#34;${ZDOTDIR:-$HOME}/.zprezto\u0026#34; $ chsh -s /usr/bin/zsh 次にpreztoを使うようにzshの設定をします。 ターミナルに以下を貼り付けて実行します。（スクリプトファイルなどを作成する必要はなく、ターミナル直貼り付けでOK） setopt EXTENDED_GLOB for rcfile in \u0026#34;${ZDOTDIR:-$HOME}\u0026#34;/.zprezto/runcoms/^README.md(.N); do ln -s \u0026#34;$rcfile\u0026#34; \u0026#34;${ZDOTDIR:-$HOME}/.${rcfile:t}\u0026#34; done これでターミナルを開き直せばzshになるはずなのですが、なぜか端末を開き直してもzshにならなかったので ターミナルのメニューから「編集-\u0026gt;プロファイル編集」を開き、コマンドタブにてカスタムコマンド『zsh』を設定しました。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/216/",
    title: "Ubuntu16.04のインストール後にしたこと",
    date: "2016-05-04T23:54:50Z",
    body: "Ubuntu16.04のインストール後にしたこと ubuntuの16.04が出ていたので、インストールしてみました。 その際に実施した内容です。（随時更新していくと思います） ホームディレクトリを日本語から英語に変更する。 ターミナル（端末）をbashからzshに変更 FlashPlayerインストール デュアルブート時の時刻ずれ対応 シャットダウンの高速化 ■その他ソフトウェアのインストール（UbuntuSoftwareでインストール） 【WEBブラウザ】 Chromium 【PDF編集ソフト】 Inkscape 【DVD書き込みソフト】 K3b"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/206/",
    title: "【PHP】Eclipseでカーソルがある変数のハイライトを変更する",
    date: "2016-03-08T12:17:33Z",
    body: "【PHP】Eclipseでカーソルがある変数のハイライトを変更する たまにカーソルがある変数の色がおかしくなる事があります。 その際は以下の設定を変更すれば治ります。 ■環境 Mac (OS X El Capitan 10.11.3) Eclipse Luna Service Release 2 (4.4.2) 【メニューの場所】 環境設定 → General → Editors → TextEditors → Annotations 【設定箇所】 PHP elements ‘ write’ occurences：カーソルがある変数が書き込まれている箇所のハイライト色 PHP elements ‘ read’ occurences：カーソルがある変数が読まれている箇所のハイライト色 PHPについて記載しましたが、おそらく他の言語でも大体同じだと思います。 ※上記変更をしても、既に開いているソースコードの色は変わら無いようです。 変更した色を確認する場合は、設定変更後にソースファイルを開き直してください。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/eclipse/",
    title: "Eclipse",
    date: "2016-03-08T12:17:33Z",
    body: "Eclipse"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/202/",
    title: "【CentOS】cronが動かない場合のチェック",
    date: "2016-03-08T05:23:15Z",
    body: "【CentOS】cronが動かない場合のチェック cronが動かない時のチェック項目 ①cronのプロセス（crond）は起動しているか ↓起動している ②crontabの記載は正しいか ↓正しい ③cronでスクリプトを叩く場合、スクリプトの実行権限は適切か ↓適切 ④/var/log/cronに何かヒントが出力されているか ↓むしろ何も出力されていない ⑤rsyslogプロセスは起動しているか docker環境でcronが動かない時に、この⑤でハマりました。 rsyslogが起動していないとログを吐かないだけでなく、cronの実行自体されないようです。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/196/",
    title: "【Ubuntu】InkscapeでPDFを開くとズレてしまう場合",
    date: "2016-02-04T12:37:03Z",
    body: "【Ubuntu】InkscapeでPDFを開くとズレてしまう場合 知っている人にとっては当たり前の話かもしれませんが InkscapeでPDFを編集しようと開くと、一部文字がズレてしまう事象に困っていました。 対応方法はInkscapeで開く際に出てくるポップアップで 「import via Poppler」にチェックを入れてから「OK」をクリックだけです。 今まではポップアップは何も触らず「OK」をクリックしてました。 ちゃんと操作方法は調べなければいけないと思いました。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/mysql/",
    title: "MySQL",
    date: "2016-01-20T22:36:39Z",
    body: "MySQL"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/189/",
    title: "MySQLの文字コードを設定する",
    date: "2016-01-20T22:36:39Z",
    body: "MySQLの文字コードを設定する MySQLで文字コードを設定する方法です。 ■環境 MySQL 5.6.26 これからDBの作成をする場合は、設定ファイル「/etc/my.cnf」に追記することで 作成するDBの文字コードを設定できます。 [mysqld] character-set-server=utf8 [client] default-character-set=utf8 作成済みDBの文字コードを設定する場合は以下のコマンドを実行してください。 # mysql mysql\u0026gt; ALTER DATABASE sample_db CHARACTER SET utf8; mysql\u0026gt; SHOW CREATE DATABASE sample_db; 作成済みテーブルの文字コードを設定する場合は以下のコマンドを実行してください。 # mysql 【テーブルとカラム両方に設定する場合】mysql\u0026gt; ALTER TABLE sample_table CONVERT TO CHARACTER SET utf8; 【テーブルのみ設定する場合】mysql\u0026gt; ALTER TABLE sample_table charset=utf8; mysql\u0026gt; SHOW CREATE TABLE sample_table;"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/173/",
    title: "mysql系の関数がPHP7でエラーになる場合の対応",
    date: "2016-01-20T21:55:34Z",
    body: "mysql系の関数がPHP7でエラーになる場合の対応 PHP5.XからPHP7にアップデートした際にmysql系の関数がエラーになっていたので、 その際に行なった対応をメモしておきます。 関数名を「mysql_xxxx()」から「mysqli_xxxx()」に変更する。 関数の引数が「第１引数：設定値」「第２引数：mysqlオブジェクト」である場合は 順番を逆にする。（第１引数がmysqlオブジェクトとなるようにする） ※ここで言うmysqlオブジェクトとは、「mysqli_connect」の返却値の事です。 mysqli_error()には、引数にmysqlオブジェクトを設定します。 これ以外にも「mysql_xxxxx()」では引数不要であったものは「mysqli_xxxxx()」では引数に mysqlオブジェクトが必要になっているかもしれません。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/164/",
    title: "ダイジェスト認証の設定方法",
    date: "2016-01-20T21:30:56Z",
    body: "ダイジェスト認証の設定方法 Apacheで動いているWEBサーバにて、ダイジェスト認証を入れる方法です。 ■環境 CentOS Linux release 7.2.1511 (Core) Apache/2.4.6 Apacheがダイジェスト認証に対応しているか確認します。 以下のコマンドにて結果が出力されれば対応しています。 # grep digest /etc/httpd/conf.modules.d/ -r conf.modules.d/00-base.conf:LoadModule auth_digest_module modules/mod_auth_digest.so ダイジェスト認証用のIDとパスワードのセットを作成します。 下記の「Sample Auth」は認証名で「sampleuser」は設定するIDです。適宜書き換えてください。 また「/etc/httpd/conf/.hpass」が生成されるファイルですが、パスもファイル名も任意です。 # htdigest -c /etc/httpd/conf/.htpass ’Sample Auth\u0026#39; sampleuser Adding password for sampleuser in realm Logical Dice Auth. New password:　←設定するパスワードを入力する Re-type new password:　←設定するパスワードを再度入力する あとはApacheの設定ファイルにて設定をします。 以下の例では「/var/www/html/sample」配下のコンテンツにWEBブラウザにてアクセスした際に ダイジェスト認証が表示されるようになります。 下記のAuthUserFileは上記で生成したファイルを指定してください。 \u0026lt;Location \u0026#34;/sample/\u0026#34;\u0026gt; AuthType Digest AuthName “Sample Auth\u0026#34; AuthUserFile \u0026#34;/etc/httpd/conf/.htpass\u0026#34; Require valid-user \u0026lt;/Location\u0026gt; これでApacheの設定ファイル再読み込み（再起動）をすればダイジェスト認証がかかるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/161/",
    title: "【FuelPHP】PHP7対応版へのアップデート",
    date: "2015-12-27T23:50:04Z",
    body: "【FuelPHP】PHP7対応版へのアップデート FuelPHPのdev-1.7がPHP7に対応していなかったので、 既存プロジェクトをPHP7に対応しているdev-1.8にアップデートします。 ※dev-1.8が正式にmasterとして公開されていない現時点の手順です。 もし正式にPHP7対応版が出たらこの手順は不要です。 ■環境 CentOS Linux release 7.2.1511 (Core) FuelPHP　dev-1.7/master FuelPHPのプロジェクトディレクトリで以下コマンドを実施してcomposer.jsonを書き換えます。 sed -ie \u0026#34;s/dev-1.7\\/master/dev-1.8\\/develop/g\u0026#34; composer.json 以下コマンドでFuelPHPのアップデートをします。 php composer.pher update 以上でPHP7対応版にアップデートされます。"
  },
  {
    url: "https://blog2.logical-dice.com/tags/fuelphp/",
    title: "FuelPHP",
    date: "2015-12-27T23:50:04Z",
    body: "FuelPHP"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/156/",
    title: "【CentOS】php7をyumインストールする",
    date: "2015-12-07T23:31:02Z",
    body: "【CentOS】php7をyumインストールする php7がリリースされたので、CentOS7のサーバにyumでインストールした手順です。 ■環境 CentOS 7.1.1503 以下コマンドを実行するだけです。 epel導入済みであれば1行目は不要です。3行目は必要なパッケージで読み替えてください。 rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm yum install php70w php70w-mbstring php70w-mysql php70w-pdo php70w-xml"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/147/",
    title: "【CentOS】ComposerにGitHubのアクセストークンを設定する",
    date: "2015-12-07T23:11:45Z",
    body: "【CentOS】ComposerにGitHubのアクセストークンを設定する 今更ながら、ComposerにGitHubのアクセストークンを設定する方法です。 ■環境 CentOS 7.1.1503 GitHubのアカウントを持っていない人は作ります。特に解説は不要なはず。 https://github.com/ GitHubにログインし、右上のアイコンから「Settings」＞「Personal access tokens」からアクセストークンを作成します。 画面を閉じるとトークンを見れなくなるので、閉じる前にメモしてください。 Composerをインストールしていない人は以下を参照 Composerのインストール方法 以下のコマンドでComposerにGitHubのアクセストークンを設定します。 composer config --global github-oauth.github.com ｛②で取得したアクセストークン｝"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/127/",
    title: "【CentOS】PostgreSQLの導入",
    date: "2015-12-04T12:59:49Z",
    body: "【CentOS】PostgreSQLの導入 CentOSにPostgreSQLを導入した際の作業メモです。 ■環境 CentOS release 6.6 postgresql 8.4.20 まずpostgreSQLをインストールし、初期化をします。 $ su - # yum install postgresql-server # service postgresql initdb 次に外部からアクセスできるように設定をします。 「192.168.xxx.xxx/32」の部分はDBへアクセスするホストに合わせて修正してください。 # vi /var/lib/pgsql/data/pg_hba.conf local all all trust host all all 192.168.xxx.xxx/32 trust # vim postgresql.conf listen_addresses = ‘*’ 設定が完了したらpostgreSQLを起動します。 # service postgresql start 次に利用するユーザとデータベースを作成します。 # su - postgres $ psql postgres=# CREATE USER sampleUser WITH PASSWORD ’samplePass\u0026#39;; postgres=# CREATE DATABASE sampleDB WITH OWNER=sampleUser ENCODING=\u0026#39;UTF8\u0026#39; TEMPLATE template0; postgres=# ¥q これで利用準備は完了です。 コマンドラインから作成したDBへ作成したユーザでアクセスする場合は以下コマンドでアクセスできます。 $ psql -U sampleUser -d sampleDB"
  },
  {
    url: "https://blog2.logical-dice.com/tags/postgresql/",
    title: "PostgreSQL",
    date: "2015-12-04T12:59:49Z",
    body: "PostgreSQL"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/136/",
    title: "【Ubuntu】UXtermの設定",
    date: "2015-12-02T21:41:14Z",
    body: "【Ubuntu】UXtermの設定 ubuntuのターミナルは「UXterm」と普通の「端末」がありますが、 UXtermの方がカスタマイズできたりするので、UXtermを設定して使います。 ■環境 ubuntu 15.04 UXterm用の設定ファイル(~/.Xresources)を作成し、以下を貼り付けます。 これはあくまで私の設定ですので、適当に変更して良いです UXTerm*utf8 : 1 UXTerm*locale : true UXTerm*selectToClipboard : true UXTerm*faceName : Dejavu Sans Mono:style=book UXTerm*faceNameDoublesize : Takao Pゴシック,TakaoPGothic:style=Regular UXTerm*faceSize : 11 UXTerm*background : black UXTerm*foreground : white UXTerm*saveLines : 2000 UXTerm*geometry : 100x20+100+100 編集が終ったら、以下コマンドで反映させます。 xrdb ~/.Xresources あとはUXtermを再起動させれば反映されます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/139/",
    title: "【Ubuntu】atomのインストール",
    date: "2015-12-02T21:37:16Z",
    body: "【Ubuntu】atomのインストール Ubuntu環境にテキストエディタのatomをインストールする手順です。 ■環境 ubuntu 15.04 以下サイトよりインストーラ（.deb）をダウンロードしてください。 https://atom.io/ ダウンロードしたディレクトリで以下コマンドを実行し、debファイルよりインストールしてください。 sudo dpkg -i atom-amd64.deb 以上でインストール完了です。 ランチャーにて「atom」を検索すると表示されます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/130/",
    title: "【Ubuntu】exfat形式の外部メモリのマウントができない",
    date: "2015-11-15T22:05:23Z",
    body: "【Ubuntu】exfat形式の外部メモリのマウントができない ■環境 ubuntu 15.04 Ubuntuはデフォルトでは「exfat」のフォーマットに対応していないので、 外部メモリをマウントしようとすると mount: unknown filesystem type \u0026#39;exfat\u0026#39; というエラーが出ます。 ※上記はコマンドラインでmountコマンドを実施した際に出力されます。 外部メモリを挿した時にUI上では違う「exsisted」エラーが出てくる可能性があります。 その場合は、以下のコマンドを実施すると「exfat」フォーマットのメモリも扱えるようになります。 $ sudo apt-get install exfat-fuse exfat-utils"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/120/",
    title: "COMODOのSSL証明書をApacheに設定する方法",
    date: "2015-10-20T20:25:57Z",
    body: "COMODOのSSL証明書をApacheに設定する方法 COMODOの証明書を購入した後の設定が複雑だったので、まとめました。 購入手順についてはSSL購入サイトを見れば分かると思うので割愛します。 ■環境 CentOS 7.1.1503 httpd 2.4.6-31.el7.centos.1 mod_ssl 1:2.4.6-31.el7.centos.1 openssl 1:1.0.1e-42.el7.9 ■必要なもの 証明書購入時に作成した秘密鍵（yourdomain_com.keyとする） 証明書購入後にメールで送られてきた証明書たち ・yourdomain_com.crt ・COMODORSADomainValidationSecureServerCA.crt ・COMODORSAAddTrustCA.crt ・AddTrustExternalCARoot.crt まず、上記必要なもの全てを設定するサーバにアップロードしておきます。 場所はどこでも良いのですが、本例では「/etc/pki/comodo/」配下に格納したとします。 そして、メールで送られてきた証明書のうち自分のドメイン名がファイル名に入っていない３つを連結します。 連結は以下のコマンドで実施できます。連結する順番は以下順を守ってください。 なお、連結後のファイル名（以下例ではyourdomain_com.ca_bundle）は任意です。 # cat COMODORSADomainValidationSecureServerCA.crt COMODORSAAddTrustCA.crt AddTrustExternalCARoot.crt \u0026gt; yourdomain_com.ca_bundle ApacheのSSL設定ファイルを編集し、以下の行を修正します。 (/etc/httpd/conf.d/ssl.conf) SSLCertificateFile /etc/pki/comodo/yourdomain_com.crt SSLCertificateKeyFile /etc/pki/comodo/yourdomain_com.key SSLCertificateChainFile /etc/pki/comodo/yourdomain_com.ca_bundle Apacheの設定を再読み込みします。 # systemctl reload httpd これでSSLの設定が反映されるはずです。 もし駄目だったら、Apacheの再起動をしてみてください。 # systemctl restart httpd"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/106/",
    title: "CentOS7のsshd設定（セキュリティ）",
    date: "2015-10-19T21:52:24Z",
    body: "CentOS7のsshd設定（セキュリティ） サーバへのsshログインについて、以下の設定をします ・sshポートの変更 ・ルート権限でのログインを無効化 ・ルート権限にsuできるアカウントの制限 ■環境 CentOS 7.1.1503 Openssh 6.6.1 以下の設定でポート番号変更とルート権限ログインの無効化を実施します 以下の行を修正（なければ追加）してください (/etc/ssh/sshd_config) Port 1234　※任意の番号 PermitRootLogin no 「wheel」グループのアカウントを作成します # useradd testuser　※任意のアカウント（既存のアカウントを使うなら不要） # usermod -g wheel -G wheel testuser 以下の設定でルート権限にsuできるグループを「wheel」に限定します 以下の行を修正（なければ追加）してください (/etc/pam.d/su) auth required pam_wheel.so use_uid 以上で設定は完了ですが、設定をしたターミナルを閉じる前に新規ターミナルを立ち上げて 上記wheelグループに設定したアカウントにてsshログイン＆suが出来ることを確認してください。 確認前に設定したターミナルを閉じてしまうと、設定に失敗していた場合に最悪だとルート権限になれなくなります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/83/",
    title: "PHPで文字化けが直らない場合の対処",
    date: "2015-10-06T21:38:07Z",
    body: "PHPで文字化けが直らない場合の対処 ■環境 CentOS 6.6 Apache2.2 日本語が文字化けするのでphpのソースで以下の記載をしていました。 setlocale(LC_ALL, \u0026#39;ja_JP.UTF-8\u0026#39;); これでも文字化けが直っていたので良いのですが、 この対応方法だと同サーバにてアプリケーションを作成する度に記載が必要です。 なので、デフォルトで「ja_JP.UTF-8」を設定しました。 ①sysconfigで設定する方法 /etc/sysconfig/httpdで以下の設定をすれば直りました。 #HTTPD_LANG=C HTTPD_LANG=ja_JP.UTF-8 ②php.iniで設定する方法 /etc/php.iniで以下の設定をすれば良いという記事を見ました。 私の環境では以下の設定をしても文字化けは解消しなかったのですが、念のため載せておきます。 ;intl.default_locale = intl.default_locale =ja_JP.UTF-8"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/69/",
    title: "【Ubuntu】JDKとNetBeans IDEのインストール",
    date: "2015-09-16T23:18:43Z",
    body: "【Ubuntu】JDKとNetBeans IDEのインストール 開発環境としてUbuntuにJDKとNetBeansをインストールします。 ■環境 Ubuntu 15.04 openjdk 8 NetBeans 8.0.2 JDKがインストールされているか確認し、なければインストールします。 $ javac -version プログラム \u0026#39;javac\u0026#39; は以下のパッケージで見つかりました: * default-jdk * ecj * gcj-4.9-jdk * openjdk-7-jdk * gcj-4.8-jdk * openjdk-6-jdk * openjdk-8-jdk 次の操作を試してください: sudo apt-get install \u0026lt;選択したパッケージ\u0026gt; $ sudo apt-get install openjdk-8-jdk $ javac -version Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar javac 1.8.0_45-internal https://netbeans.org/downloads/index.htmlよりNetBeansをダウンロードします。 各開発言語毎に用意されていますが、迷う様なら「すべて」を選択すれば良いと思います。 私は使用言語が特定の言語に限らなかったので「すべて」を選択しました。 ダウンロードしたNetBeansのインストールスクリプトを実行します。 $ cd ~/Downloads $ sh netbeans-8.0.2-linux.sh インストーラが立ち上がるので、必要に応じてインストール場所を変えたりしながら進めていきます。 インストーラでの処理が終わったら、インストール完了です。 ランチャーにて「NetBeans」で検索すれば出てくるようになります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/57/",
    title: "【Ubuntu】FlashPlayerのインストール",
    date: "2015-08-27T21:56:00Z",
    body: "【Ubuntu】FlashPlayerのインストール UbuntuのブラウザでFlashPlayerをインストールする手順です。 ■Firefoxの場合 AdobeよりFlashPlayerをダウンロードします。 この記事を書いた時は「install_flash_player_11_linux.x86_64.tar.gz」をダウンロードしました。 ダウンロードしたファイルを解凍してできたディレクトリに入り、 必要なファイルをコピーしていきます。 sudo cp -a libflashplayer.so /usr/lib/chromium-browser/libs/. sudo cp -a usr/* /usr/. ■Cromiumの場合 以下のコマンドだけでインストールできます。 \u0026lt;del datetime=\u0026#34;2016-06-21T14:36:14+00:00\u0026#34;\u0026gt;sudo apt-get install pepperflashplugin-nonfree\u0026lt;/del\u0026gt; \u0026lt;del datetime=\u0026#34;2016-05-04T14:36:14+00:00\u0026#34;\u0026gt;sudo update-pepperflashplugin-nonfree --install\u0026lt;/del\u0026gt; ※（2016/6/21編集）上記コマンドでのインストールでは最新版のFlashプレイヤーが取得できなくなています。 Adobeの公式サイトでダウンロードできるので、そちらから入手しましょう。 https://get.adobe.com/jp/flashplayer/?no_redirect"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/48/",
    title: "【Ubuntu】ホームディレクトリを日本語から英語にする",
    date: "2015-08-27T21:36:09Z",
    body: "【Ubuntu】ホームディレクトリを日本語から英語にする 日本語版Ubuntuをインストールすると、ホームディレクトリ内のディレクトリ名が日本語になっています。 コマンドラインで操作する時に日本語があると不都合があるので、英語表記に変更します。 例）音楽⇛Music ターミナルを開いて、以下コマンドを実行します。 LANG=C xdg-user-dirs-gtk-update 確認ダイアログが表示されるので『Update Names』ボタンを押下します。 これでディレクトリ名が英語になります。 PCを再起動後、再度確認ダイアログが表示されるので 『次回から表示しない』をチェックして『古い名前のままにする』を押下します。 これで今後は再起動後もディレクトリ名が英語のままとなります。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/40/",
    title: "メールドメインに「localhost」と付いてしまう",
    date: "2015-08-08T16:52:42Z",
    body: "メールドメインに「localhost」と付いてしまう ■環境 CentOS 7 postfix 2.10.1 postfixで構築したメールサーバからメールを送信した場合に 設定は間違えていないはずなのに、送信メールのドメイン部分（@の後ろ）に「localhost」が付いてしまう時の対処法です。 例えば、『@sample.com』で送りたいのに『@localhost.sample.com』となってしまう場合です。 ※注意 「設定は間違えていないはず」と思っているだけで本当は何かが間違っているのだと思います。 postfixの設定以外にも環境ファイルの設定値が悪い場合などもありますが、その調査をしていられない時の 暫定対処だと思ってください。 postfixの設定ファイル(/etc/postfix/main.cf)に以下の行を追加してください。 masquerade_domains = sample.com これで『sample.com』より前の部分はカットされます。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/18/",
    title: "メールサーバ構築（②dovecot編）",
    date: "2015-08-08T15:35:56Z",
    body: "メールサーバ構築（②dovecot編） ■環境 CentOS 7 dovecot 2.2.10 事前に①postfix編が終わっている前提とします。 必要な物をインストール yum -y install dovecot メイン設定ファイルの修正(vi /etc/dovecot/dovecot.conf) protocols = imap pop3 listen = * 認証設定ファイルの修正(vi /etc/dovecot/conf.d/10-auth.conf) 今回はパスワードファイルでの認証方式としてます #disable_plaintext_auth = yes auth_mechanisms = plain login digest-md5 cram-md5 #!include auth-system.conf.ext !include auth-passwdfile.conf.ext メール設定ファイルの修正(vi /etc/dovecot/conf.d/10-mail.conf) mail_location = maildir:~/Maildir マスター設定ファイルの修正(vi /etc/dovecot/conf.d/10-master.conf) unix_listener /var/spool/postfix/private/auth { mode = 0666 user = postfix group = postfix } SSL設定ファイルの修正(vi /etc/dovecot/conf.d/10-ssl.conf) ssl_cert = \u0026lt;/etc/pki/sample/sample.crt ssl_key = \u0026lt;/etc/pki/sample/sample.key ssl_ca = \u0026lt;/etc/pki/sample/sampleCA.crt 認証パスワードのCRAM-MD5で暗号化した文字列を生成します。 doveadm pw -s CRAM-MD5 Enter new password: 設定したいパスワードを入力 Retype new password: 設定したいパスワードを再入力 {CRAM-MD5}256f41a2b646bf66ae494370947623fd7a1f496442c81a0fdd9c2b1d994eb816 対象アカウントの情報を確認します。 このアカウント名〜ホームディレクトリまでの部分を後で使います。 grep demouser1 /etc/passwd demouser1:x:1000:1000::/home/demouser1:/sbin/nologin パスワードファイルを作成します(vi /etc/dovecot/users) 先ほど確認したpasswdファイルのアカウント名〜ホームディレクトリ部分をコピーし、『x』の部分に生成したCRAM-MD5の文字列をはめ込んだ行を記載すればOKです。 demouser1:{CRAM-MD5}256f〜省略〜816:1000:1000::/home/demouser1/ サービス起動 systemctl start dovecot.service サービスの自動起動設定 systemctl enable dovecot.service これでdovecotの設定は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/posts/wp/9/",
    title: "メールサーバ構築（①postfix編）",
    date: "2015-08-06T23:28:32Z",
    body: "メールサーバ構築（①postfix編） ■環境 CentOS 7 postfix 2.10.1 メールアカウントはpasswdに登録されているアカウントを利用します。 メール専用のアカウントは以下コマンドで作成してください。（SSHログイン不可） useradd -s /sbin/nologin demouser1 必要な物をインストール yum -y install postfix cyrus-sasl cyrus-sasl-plain cyrus-sasl-md5 メイン設定ファイルの修正(vi /etc/postfix/main.cf) 以下、設定変更部分 myhostname = mail.sample.com mydomain = sample.com myorigin = $mydomain inet_interfaces = all mynetworks = 192.168.0.0/23, 127.0.0.0/8 relay_domains = $mydestination home_mailbox = Maildir/ smtpd_banner = $myhostname ESMTP unknown 以下、設定追加部分 ※SSL証明書の秘密鍵はパスワード無しとします smtpd_sasl_auth_enable = yes smtpd_sasl_security_options = noanonymous smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination smtpd_tls_cert_file = /etc/pki/sample/sample.crt smtpd_tls_key_file = /etc/pki/sample/sample.key smtpd_tls_CAfile = /etc/pki/sample/sampleCA.crt smtpd_tls_loglevel = 1 smtpd_use_tls = yes マスター設定ファイルの修正(vi /etc/postfix/master.cf) 以下、設定変更部分 submission inet n - n - - smtpd -o syslog_name=postfix/submission -o smtpd_sasl_auth_enable=yes -o smtpd_client_restrictions=$mua_client_restrictions -o milter_macro_daemon_name=ORIGINATING smtps inet n - n - - smtpd -o smtpd_tls_wrappermode=yes SASL認証設定ファイルの修正(vi /etc/sasl2/smtpd.conf) 以下、設定変更部分 mech_list: plain login cram-md5 digest-md5 SASL認証ファイルへのアカウント追加 以下コマンド実行時に設定するパスワードを聞かれるので入力する saslpasswd2 -c -u sample.com demouser1 ##################### 参考 ##################### SASL認証ファイルに登録したユーザの確認は以下コマンド sasldblistusers2 SASL認証ファイルに登録したユーザの削除は以下コマンド saslpasswd2 -d -u sample.com demouser1 SASL認証ファイルの権限グループをpostfixに変更 chgrp postfix /etc/sasldb2 サービス起動 systemctl start postfix.service systemctl start saslauthd.service サービスの自動起動設定 systemctl enable postfix.service systemctl enable saslauthd.service これでpostfixの設定は完了です。"
  },
  {
    url: "https://blog2.logical-dice.com/search/data.js",
    title: "",
    date: "0001-01-01T00:00:00Z",
    body: ""
  },
  {
    url: "https://blog2.logical-dice.com/search/",
    title: "Searches",
    date: "0001-01-01T00:00:00Z",
    body: "Searches"
  },
];
